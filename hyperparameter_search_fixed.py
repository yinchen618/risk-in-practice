#!/usr/bin/env python3
"""
ERM Baseline è¶…åƒæ•¸æœç´¢è…³æœ¬ (ä¿®æ­£ç‰ˆ)
è‡ªå‹•æ¸¬è©¦ä¸åŒçš„è¶…åƒæ•¸çµ„åˆä¾†æ‰¾å‡ºæœ€ä½³æ¨¡å‹é…ç½®
"""

import requests
import json
import time
import sys
from datetime import datetime
from typing import Dict, List, Optional, Any

# é…ç½®
API_BASE = "https://weakrisk.yinchen.tw"
EXPERIMENT_RUN_ID = "bc38412a-ead9-4aec-b5fe-b2526d4ca478"  # å¾ç”¨æˆ¶æä¾›çš„ ID
MAX_WAIT_TIME = 1200  # æœ€é•·ç­‰å¾… 20 åˆ†é˜
POLL_INTERVAL = 15  # æ¯ 15 ç§’æª¢æŸ¥ä¸€æ¬¡

class HyperparameterSearcher:
    def __init__(self, experiment_run_id: str):
        self.experiment_run_id = experiment_run_id
        self.results = []
        self.exp_info = None

    def get_experiment_run_info(self):
        """ç²å–å¯¦é©—é‹è¡Œçš„åŸºæœ¬ä¿¡æ¯å’Œæ•¸æ“šæºé…ç½®"""
        try:
            response = requests.get(f"{API_BASE}/api/v2/experiment-runs/{self.experiment_run_id}")
            if response.status_code == 200:
                exp_data = response.json()

                # è§£æ filteringParameters ä»¥ç²å–æ•¸æ“šé›† ID
                filtering_params = exp_data.get('filtering_parameters')
                if filtering_params:
                    try:
                        # å¦‚æœ filtering_parameters å·²ç¶“æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                        if isinstance(filtering_params, dict):
                            params = filtering_params
                        else:
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                            params = json.loads(filtering_params)

                        selected_dataset_ids = params.get('selectedDatasetIds', [])
                        print(f"ğŸ” æ‰¾åˆ°é¸ä¸­çš„æ•¸æ“šé›†: {selected_dataset_ids}")
                        exp_data['selectedDatasetIds'] = selected_dataset_ids
                    except Exception as e:
                        print(f"âš ï¸ è§£æ filteringParameters å¤±æ•—: {e}")
                        exp_data['selectedDatasetIds'] = []
                else:
                    exp_data['selectedDatasetIds'] = []

                self.exp_info = exp_data
                return exp_data
            else:
                print(f"âŒ ç„¡æ³•ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯æ™‚å‡ºéŒ¯: {e}")
            return None

    def generate_hyperparameter_configs(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆ 10 çµ„ä¸åŒçš„è¶…åƒæ•¸é…ç½®é€²è¡Œæ¸¬è©¦"""

        configs = [
            {
                "name": "Config_1_Small_Fast",
                "modelType": "LSTM",
                "hiddenSize": 32,
                "numLayers": 1,
                "dropout": 0.2,
                "windowSize": 10,
                "learningRate": 0.001,
                "batchSize": 32,
                "epochs": 50,
                "classPrior": 0.1,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.001,
                "earlyStopping": True,
                "patience": 8,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_2_Medium_Balanced",
                "modelType": "LSTM",
                "hiddenSize": 64,
                "numLayers": 2,
                "dropout": 0.3,
                "windowSize": 15,
                "learningRate": 0.0005,
                "batchSize": 64,
                "epochs": 80,
                "classPrior": 0.15,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0005,
                "earlyStopping": True,
                "patience": 12,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_3_Large_Deep",
                "modelType": "LSTM",
                "hiddenSize": 128,
                "numLayers": 3,
                "dropout": 0.4,
                "windowSize": 20,
                "learningRate": 0.0001,
                "batchSize": 32,
                "epochs": 100,
                "classPrior": 0.2,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0001,
                "earlyStopping": True,
                "patience": 15,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_4_High_LR",
                "modelType": "LSTM",
                "hiddenSize": 64,
                "numLayers": 2,
                "dropout": 0.25,
                "windowSize": 12,
                "learningRate": 0.005,
                "batchSize": 64,
                "epochs": 60,
                "classPrior": 0.12,
                "activationFunction": "tanh",
                "optimizer": "adam",
                "l2Regularization": 0.001,
                "earlyStopping": True,
                "patience": 10,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_5_SGD_Optimizer",
                "modelType": "LSTM",
                "hiddenSize": 64,
                "numLayers": 2,
                "dropout": 0.3,
                "windowSize": 15,
                "learningRate": 0.01,
                "batchSize": 32,
                "epochs": 80,
                "classPrior": 0.15,
                "activationFunction": "relu",
                "optimizer": "sgd",
                "l2Regularization": 0.0005,
                "earlyStopping": True,
                "patience": 12,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_6_Low_Dropout",
                "modelType": "LSTM",
                "hiddenSize": 96,
                "numLayers": 2,
                "dropout": 0.1,
                "windowSize": 18,
                "learningRate": 0.0008,
                "batchSize": 48,
                "epochs": 70,
                "classPrior": 0.18,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0003,
                "earlyStopping": True,
                "patience": 10,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_7_Large_Batch",
                "modelType": "LSTM",
                "hiddenSize": 64,
                "numLayers": 2,
                "dropout": 0.35,
                "windowSize": 15,
                "learningRate": 0.0003,
                "batchSize": 128,
                "epochs": 90,
                "classPrior": 0.15,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0008,
                "earlyStopping": True,
                "patience": 15,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_8_Small_Window",
                "modelType": "LSTM",
                "hiddenSize": 64,
                "numLayers": 2,
                "dropout": 0.3,
                "windowSize": 5,
                "learningRate": 0.001,
                "batchSize": 64,
                "epochs": 80,
                "classPrior": 0.15,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0005,
                "earlyStopping": True,
                "patience": 15,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_9_Large_Window",
                "modelType": "LSTM",
                "hiddenSize": 64,
                "numLayers": 2,
                "dropout": 0.3,
                "windowSize": 30,
                "learningRate": 0.0005,
                "batchSize": 32,
                "epochs": 80,
                "classPrior": 0.15,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0005,
                "earlyStopping": True,
                "patience": 15,
                "learningRateScheduler": "none"
            },
            {
                "name": "Config_10_Conservative",
                "modelType": "LSTM",
                "hiddenSize": 48,
                "numLayers": 1,
                "dropout": 0.4,
                "windowSize": 12,
                "learningRate": 0.0003,
                "batchSize": 64,
                "epochs": 60,
                "classPrior": 0.1,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.001,
                "earlyStopping": True,
                "patience": 8,
                "learningRateScheduler": "none"
            }
        ]

        return configs

    def create_training_job(self, config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å‰µå»ºè¨“ç·´ä½œæ¥­"""
        taiwan_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        model_name = f"ERM_BASELINE_{config['name']}_{taiwan_time}"

        # ğŸ”¥ é—œéµä¿®æ­£ï¼šæ­£ç¢ºè¨­ç½® P å’Œ U æ•¸æ“šæº
        if not self.exp_info or not self.exp_info.get('selectedDatasetIds'):
            print("âŒ æ²’æœ‰æ‰¾åˆ°æ•¸æ“šé›† IDï¼Œç„¡æ³•å‰µå»ºè¨“ç·´ä½œæ¥­")
            return None

        selected_dataset_ids = self.exp_info['selectedDatasetIds']

        # ç‚º PU Learning è¨­ç½®æ•¸æ“šæºé…ç½®
        data_source_config = {
            "trainRatio": 70,
            "validationRatio": 20,
            "testRatio": 10,
            # ğŸ”¥ é—œéµï¼šè¨­ç½® P å’Œ U æ•¸æ“šæº
            "positiveDataSourceIds": selected_dataset_ids,  # P æ•¸æ“šä¾†æº
            "unlabeledDataSourceIds": selected_dataset_ids  # U æ•¸æ“šä¾†æº
        }

        payload = {
            "name": model_name,
            "scenario_type": "ERM_BASELINE",
            "experimentRunId": self.experiment_run_id,
            "modelConfig": json.dumps(config),
            "dataSourceConfig": json.dumps(data_source_config)
        }

        try:
            response = requests.post(
                f"{API_BASE}/api/v2/trained-models",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                print(f"   âœ… è¨“ç·´ä½œæ¥­å·²é–‹å§‹: {result.get('jobId', 'N/A')}")
                return result
            else:
                print(f"   âŒ å‰µå»ºè¨“ç·´ä½œæ¥­å¤±æ•—: {response.status_code}")
                try:
                    error_detail = response.json()
                    print(f"   ğŸš¨ éŒ¯èª¤è©³æƒ…: {error_detail}")
                except:
                    print(f"   ğŸš¨ éŒ¯èª¤è©³æƒ…: {response.text}")
                return None

        except Exception as e:
            print(f"   âŒ å‰µå»ºè¨“ç·´ä½œæ¥­æ™‚å‡ºéŒ¯: {e}")
            return None

    def wait_for_training_completion(self, job_id: str, model_id: str, config_name: str) -> Optional[Dict[str, Any]]:
        """ç­‰å¾…è¨“ç·´å®Œæˆä¸¦ç²å–çµæœ"""
        print(f"   â³ ç­‰å¾…è¨“ç·´å®Œæˆ (æœ€é•·ç­‰å¾… {MAX_WAIT_TIME // 60} åˆ†é˜)...")

        start_time = time.time()
        last_status = None

        while time.time() - start_time < MAX_WAIT_TIME:
            try:
                # ç²å–æ‰€æœ‰è¨“ç·´æ¨¡å‹ä¾†æ‰¾åˆ°æˆ‘å€‘çš„æ¨¡å‹
                response = requests.get(f"{API_BASE}/api/v2/trained-models")
                if response.status_code == 200:
                    models = response.json()

                    # æ‰¾åˆ°æˆ‘å€‘çš„æ¨¡å‹
                    our_model = None
                    for model in models:
                        if model.get('id') == model_id or model.get('jobId') == job_id:
                            our_model = model
                            break

                    if our_model:
                        status = our_model.get('status', 'UNKNOWN')
                        if status != last_status:
                            print(f"   ğŸ“Š ç‹€æ…‹æ›´æ–°: {status}")
                            last_status = status

                        if status == 'COMPLETED':
                            print(f"   âœ… è¨“ç·´å®Œæˆ!")
                            return our_model
                        elif status == 'FAILED':
                            print(f"   âŒ è¨“ç·´å¤±æ•—")
                            return our_model
                        elif status in ['PENDING', 'RUNNING']:
                            # ç¹¼çºŒç­‰å¾…
                            pass
                        else:
                            print(f"   âš ï¸ æœªçŸ¥ç‹€æ…‹: {status}")
                    else:
                        print(f"   âš ï¸ æ‰¾ä¸åˆ°æ¨¡å‹ ID: {model_id}")
                else:
                    print(f"   âš ï¸ ç„¡æ³•ç²å–è¨“ç·´ç‹€æ…‹: {response.status_code}")

                time.sleep(POLL_INTERVAL)

            except Exception as e:
                print(f"   âš ï¸ æª¢æŸ¥è¨“ç·´ç‹€æ…‹æ™‚å‡ºéŒ¯: {e}")
                time.sleep(POLL_INTERVAL)

        print(f"   â° è¨“ç·´è¶…æ™‚ ({MAX_WAIT_TIME // 60} åˆ†é˜)")
        return None

    def extract_model_performance(self, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¾æ¨¡å‹æ•¸æ“šä¸­æå–æ€§èƒ½æŒ‡æ¨™"""
        performance = {
            "status": model_data.get('status', 'UNKNOWN'),
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "training_time": 0.0,
            "epochs_trained": 0,
            "best_epoch": 0,
            "early_stopped": False
        }

        try:
            # è§£æè¨“ç·´æŒ‡æ¨™
            training_metrics = model_data.get('trainingMetrics')
            if training_metrics:
                metrics = json.loads(training_metrics) if isinstance(training_metrics, str) else training_metrics

                performance.update({
                    "f1_score": metrics.get('best_val_f1_score', metrics.get('final_test_f1_score', 0.0)),
                    "precision": metrics.get('final_test_precision', 0.0),
                    "recall": metrics.get('final_test_recall', 0.0),
                    "training_time": metrics.get('training_time_seconds', 0.0),
                    "epochs_trained": metrics.get('total_epochs_trained', 0),
                    "best_epoch": metrics.get('best_epoch', 0),
                    "early_stopped": metrics.get('early_stopped', False)
                })

            # è§£æé©—è­‰æŒ‡æ¨™
            validation_metrics = model_data.get('validationMetrics')
            if validation_metrics:
                val_metrics = json.loads(validation_metrics) if isinstance(validation_metrics, str) else validation_metrics

                # å¦‚æœè¨“ç·´æŒ‡æ¨™ä¸­æ²’æœ‰ F1 åˆ†æ•¸ï¼Œå˜—è©¦å¾é©—è­‰æŒ‡æ¨™ä¸­ç²å–
                if performance["f1_score"] == 0.0:
                    performance["f1_score"] = val_metrics.get('test_f1_score', val_metrics.get('validation_f1_score', 0.0))

        except Exception as e:
            print(f"   âš ï¸ è§£ææ€§èƒ½æŒ‡æ¨™æ™‚å‡ºéŒ¯: {e}")

        return performance

    def run_hyperparameter_search(self):
        """åŸ·è¡Œå®Œæ•´çš„è¶…åƒæ•¸æœç´¢"""
        print("ğŸš€ é–‹å§‹ ERM Baseline è¶…åƒæ•¸æœç´¢")
        print("=" * 60)

        # ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯
        print(f"ğŸ” ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯: {self.experiment_run_id}")
        if not self.get_experiment_run_info():
            print("âŒ ç„¡æ³•ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯ï¼Œé€€å‡º")
            return

        print(f"âœ… å¯¦é©—é‹è¡Œä¿¡æ¯ç²å–æˆåŠŸ")
        print(f"   ğŸ“Š å¯¦é©—åç¨±: {self.exp_info.get('name', 'N/A')}")
        print(f"   ğŸ“Š å¯ç”¨æ•¸æ“šé›†: {len(self.exp_info.get('selectedDatasetIds', []))}")
        print("")

        # ç”Ÿæˆè¶…åƒæ•¸é…ç½®
        configs = self.generate_hyperparameter_configs()
        print(f"ğŸ“‹ ç”Ÿæˆäº† {len(configs)} çµ„è¶…åƒæ•¸é…ç½®")
        print("")

        # é–‹å§‹æ¸¬è©¦æ¯çµ„é…ç½®
        for i, config in enumerate(configs, 1):
            print(f"ğŸ“Š é…ç½® {i}/{len(configs)}: {config['name']}")
            print("-" * 60)

            # é¡¯ç¤ºé…ç½®è©³æƒ…
            print(f"ğŸš€ é–‹å§‹è¨“ç·´: {config['name']}")
            print(f"   ğŸ“‹ æ¨¡å‹åç¨±: ERM_BASELINE_{config['name']}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}")
            print(f"   ğŸ§  Hidden Size: {config['hiddenSize']}, Layers: {config['numLayers']}")
            print(f"   ğŸ“Š Window Size: {config['windowSize']}, Batch Size: {config['batchSize']}")
            print(f"   ğŸ“ˆ Learning Rate: {config['learningRate']}, Optimizer: {config['optimizer']}")

            # å‰µå»ºè¨“ç·´ä½œæ¥­
            job_result = self.create_training_job(config)
            if not job_result:
                print(f"   âŒ è·³éæ­¤é…ç½®")
                self.results.append({
                    "config": config,
                    "performance": {"status": "FAILED_TO_CREATE", "f1_score": 0.0},
                    "error": "Failed to create training job"
                })
                print("")
                continue

            job_id = job_result.get('jobId')
            model_id = job_result.get('id')

            # ç­‰å¾…è¨“ç·´å®Œæˆ
            final_model = self.wait_for_training_completion(job_id, model_id, config['name'])

            if final_model:
                performance = self.extract_model_performance(final_model)
                print(f"   ğŸ“Š æœ€çµ‚æ€§èƒ½:")
                print(f"      ğŸ¯ F1 Score: {performance['f1_score']:.4f}")
                print(f"      ğŸ“ˆ Precision: {performance['precision']:.4f}")
                print(f"      ğŸ“‰ Recall: {performance['recall']:.4f}")
                print(f"      â±ï¸ è¨“ç·´æ™‚é–“: {performance['training_time']:.2f}s")
                print(f"      ğŸ”„ è¨“ç·´è¼ªæ•¸: {performance['epochs_trained']}")

                self.results.append({
                    "config": config,
                    "performance": performance,
                    "model_id": model_id,
                    "job_id": job_id
                })
            else:
                print(f"   âŒ è¨“ç·´è¶…æ™‚æˆ–å¤±æ•—")
                self.results.append({
                    "config": config,
                    "performance": {"status": "TIMEOUT_OR_FAILED", "f1_score": 0.0},
                    "error": "Training timeout or failed"
                })

            print("")

        # é¡¯ç¤ºæœ€çµ‚çµæœ
        self.display_final_results()

    def display_final_results(self):
        """é¡¯ç¤ºæœ€çµ‚æœç´¢çµæœ"""
        print("ğŸ† è¶…åƒæ•¸æœç´¢çµæœç¸½çµ")
        print("=" * 80)

        # æŒ‰ F1 åˆ†æ•¸æ’åº
        valid_results = [r for r in self.results if r['performance']['status'] == 'COMPLETED']
        valid_results.sort(key=lambda x: x['performance']['f1_score'], reverse=True)

        if valid_results:
            print(f"âœ… æˆåŠŸå®Œæˆ {len(valid_results)} å€‹é…ç½®çš„è¨“ç·´")
            print("")

            # é¡¯ç¤ºæ’åå‰ 5 çš„é…ç½®
            top_n = min(5, len(valid_results))
            for i, result in enumerate(valid_results[:top_n], 1):
                config = result['config']
                perf = result['performance']

                print(f"ğŸ¥‡ ç¬¬ {i} å: {config['name']}")
                print(f"   ğŸ¯ F1 Score: {perf['f1_score']:.4f}")
                print(f"   ğŸ“ˆ Precision: {perf['precision']:.4f}")
                print(f"   ğŸ“‰ Recall: {perf['recall']:.4f}")
                print(f"   â±ï¸ è¨“ç·´æ™‚é–“: {perf['training_time']:.2f}s")
                print(f"   ğŸ”„ è¨“ç·´è¼ªæ•¸: {perf['epochs_trained']}")
                print(f"   ğŸ§  æ¶æ§‹: Hidden={config['hiddenSize']}, Layers={config['numLayers']}")
                print(f"   ğŸ“Š åƒæ•¸: Window={config['windowSize']}, Batch={config['batchSize']}, LR={config['learningRate']}")
                print("")

            # ä¿å­˜è©³ç´°çµæœåˆ°æ–‡ä»¶
            self.save_results_to_file()

        else:
            print("âŒ æ²’æœ‰æˆåŠŸå®Œæˆçš„è¨“ç·´é…ç½®")

        # é¡¯ç¤ºå¤±æ•—çµ±è¨ˆ
        failed_results = [r for r in self.results if r['performance']['status'] != 'COMPLETED']
        if failed_results:
            print(f"âš ï¸ å¤±æ•—çš„é…ç½®: {len(failed_results)}")
            for result in failed_results:
                print(f"   âŒ {result['config']['name']}: {result['performance']['status']}")

    def save_results_to_file(self):
        """ä¿å­˜è©³ç´°çµæœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"hyperparameter_search_results_{timestamp}.json"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
            print(f"ğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜çµæœæ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”¬ ERM Baseline è¶…åƒæ•¸æœç´¢å·¥å…·")
    print(f"ğŸ¯ å¯¦é©—é‹è¡Œ ID: {EXPERIMENT_RUN_ID}")
    print("")

    # å‰µå»ºæœç´¢å™¨ä¸¦é‹è¡Œ
    searcher = HyperparameterSearcher(EXPERIMENT_RUN_ID)
    searcher.run_hyperparameter_search()

    print("")
    print("ğŸ è¶…åƒæ•¸æœç´¢å®Œæˆ!")

if __name__ == "__main__":
    main()
