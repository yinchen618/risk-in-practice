#!/usr/bin/env python3
"""
è©³ç´°æ¸¬è©¦å€™é¸ç”Ÿæˆé‚è¼¯
"""

import sqlite3
import json

def analyze_candidate_calculation():
    """åˆ†æå€™é¸æ•¸é‡è¨ˆç®—é‚è¼¯"""

    print("ğŸ” åˆ†æå€™é¸è¨ˆç®—é‚è¼¯...")
    print("=" * 60)

    # é€£æ¥åˆ°è³‡æ–™åº«
    db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # æ¸¬è©¦åƒæ•¸
    buildings = ["Building-A"]
    floors = ["2F", "3F"]
    occupant_types = ["STUDENT"]
    z_score_threshold = 2.5
    spike_threshold = 200

    # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
    where_conditions = ["1=1"]
    params = []

    if buildings:
        placeholders = ','.join(['?' for _ in buildings])
        where_conditions.append(f"building IN ({placeholders})")
        params.extend(buildings)

    if floors:
        placeholders = ','.join(['?' for _ in floors])
        where_conditions.append(f"floor IN ({placeholders})")
        params.extend(floors)

    if occupant_types:
        placeholders = ','.join(['?' for _ in occupant_types])
        where_conditions.append(f"occupant_type IN ({placeholders})")
        params.extend(occupant_types)

    # æŸ¥è©¢è³‡æ–™é›†
    query = f"SELECT id, name, building, floor, room, total_records FROM analysis_datasets WHERE {' AND '.join(where_conditions)}"
    cursor.execute(query, params)
    datasets = cursor.fetchall()

    print(f"ğŸ“Š æ‰¾åˆ° {len(datasets)} å€‹è³‡æ–™é›†:")

    total_candidates = 0
    base_candidates_per_dataset = 50

    for i, dataset in enumerate(datasets):
        dataset_id, name, building, floor, room, total_records = dataset

        if total_records is None:
            total_records = 1000

        # è¨ˆç®—å› å­
        threshold_factor = max(0.1, min(2.0, 3.0 / z_score_threshold))
        spike_factor = max(0.5, min(1.5, spike_threshold / 200))

        # è¨ˆç®—å€™é¸æ•¸é‡
        dataset_candidates = int(
            (total_records / 1000) * base_candidates_per_dataset *
            threshold_factor * spike_factor
        )

        total_candidates += max(0, dataset_candidates)

        print(f"  {i+1}. {name}")
        print(f"     å»ºç¯‰: {building}, æ¨“å±¤: {floor}, æˆ¿é–“: {room}")
        print(f"     ç¸½è¨˜éŒ„: {total_records:,}")
        print(f"     é–¾å€¼å› å­: {threshold_factor:.3f}")
        print(f"     å³°å€¼å› å­: {spike_factor:.3f}")
        print(f"     å€™é¸æ•¸é‡: {dataset_candidates:,}")
        print()

    print(f"ğŸ¯ ç¸½å€™é¸æ•¸é‡: {total_candidates:,}")

    # å¤šæ¬¡é‹è¡Œç›¸åŒçš„è¨ˆç®—ä¾†é©—è­‰ä¸€è‡´æ€§
    print("\nğŸ”„ é©—è­‰è¨ˆç®—ä¸€è‡´æ€§:")
    for run in range(5):
        run_total = 0
        for dataset in datasets:
            total_records = dataset[5] if dataset[5] else 1000
            threshold_factor = max(0.1, min(2.0, 3.0 / z_score_threshold))
            spike_factor = max(0.5, min(1.5, spike_threshold / 200))
            dataset_candidates = int(
                (total_records / 1000) * base_candidates_per_dataset *
                threshold_factor * spike_factor
            )
            run_total += max(0, dataset_candidates)

        print(f"  é‹è¡Œ {run+1}: {run_total:,}")

    conn.close()

if __name__ == "__main__":
    analyze_candidate_calculation()
