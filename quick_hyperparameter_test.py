#!/usr/bin/env python3
"""
ERM Baseline å¿«é€Ÿè¶…åƒæ•¸æ¸¬è©¦ (ç°¡åŒ–ç‰ˆ)
åªæ¸¬è©¦ 3 å€‹æ ¸å¿ƒé…ç½®ä¾†å¿«é€Ÿæ‰¾åˆ°æœ€ä½³è¨­ç½®
"""

import requests
import json
import time
import sys
from datetime import datetime
from typing import Dict, List, Optional, Any

# é…ç½®
API_BASE = "https://python.yinchen.tw"
EXPERIMENT_RUN_ID = "bc38412a-ead9-4aec-b5fe-b2526d4ca478"  # å¾ç”¨æˆ¶æä¾›çš„ ID
MAX_WAIT_TIME = 600  # æœ€é•·ç­‰å¾… 10 åˆ†é˜
POLL_INTERVAL = 10  # æ¯ 10 ç§’æª¢æŸ¥ä¸€æ¬¡

class QuickHyperparameterTest:
    def __init__(self, experiment_run_id: str):
        self.experiment_run_id = experiment_run_id
        self.results = []
        self.exp_info = None

    def get_experiment_run_info(self):
        """ç²å–å¯¦é©—é‹è¡Œçš„åŸºæœ¬ä¿¡æ¯å’Œæ•¸æ“šæºé…ç½®"""
        try:
            response = requests.get(f"{API_BASE}/api/v2/experiment-runs/{self.experiment_run_id}")
            if response.status_code == 200:
                exp_data = response.json()

                # è§£æ filteringParameters ä»¥ç²å–æ•¸æ“šé›† ID
                filtering_params = exp_data.get('filtering_parameters')
                if filtering_params:
                    try:
                        # å¦‚æœ filtering_parameters å·²ç¶“æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                        if isinstance(filtering_params, dict):
                            params = filtering_params
                        else:
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                            params = json.loads(filtering_params)

                        selected_dataset_ids = params.get('selectedDatasetIds', [])
                        print(f"ğŸ” æ‰¾åˆ°é¸ä¸­çš„æ•¸æ“šé›†: {selected_dataset_ids}")
                        exp_data['selectedDatasetIds'] = selected_dataset_ids
                    except Exception as e:
                        print(f"âš ï¸ è§£æ filteringParameters å¤±æ•—: {e}")
                        exp_data['selectedDatasetIds'] = []
                else:
                    exp_data['selectedDatasetIds'] = []

                self.exp_info = exp_data
                return exp_data
            else:
                print(f"âŒ ç„¡æ³•ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯æ™‚å‡ºéŒ¯: {e}")
            return None

    def generate_test_configs(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆ 3 çµ„æ ¸å¿ƒæ¸¬è©¦é…ç½®"""

        configs = [
            {
                "name": "Quick_Small_Fast",
                "modelType": "LSTM",
                "hiddenSize": 32,
                "numLayers": 1,
                "dropout": 0.2,
                "windowSize": 10,
                "learningRate": 0.001,
                "batchSize": 32,
                "epochs": 30,  # æ¸›å°‘ epochs ä»¥åŠ å¿«æ¸¬è©¦
                "classPrior": 0.1,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.001,
                "earlyStopping": True,
                "patience": 5,  # æ¸›å°‘ patience
                "learningRateScheduler": "none"
            },
            {
                "name": "Quick_Medium_Balanced",
                "modelType": "LSTM",
                "hiddenSize": 64,
                "numLayers": 2,
                "dropout": 0.3,
                "windowSize": 15,
                "learningRate": 0.0005,
                "batchSize": 64,
                "epochs": 40,  # æ¸›å°‘ epochs
                "classPrior": 0.15,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0005,
                "earlyStopping": True,
                "patience": 8,  # æ¸›å°‘ patience
                "learningRateScheduler": "none"
            },
            {
                "name": "Quick_Optimized",
                "modelType": "LSTM",
                "hiddenSize": 96,
                "numLayers": 2,
                "dropout": 0.25,
                "windowSize": 12,
                "learningRate": 0.0008,
                "batchSize": 48,
                "epochs": 35,  # æ¸›å°‘ epochs
                "classPrior": 0.18,
                "activationFunction": "relu",
                "optimizer": "adam",
                "l2Regularization": 0.0003,
                "earlyStopping": True,
                "patience": 6,  # æ¸›å°‘ patience
                "learningRateScheduler": "none"
            }
        ]

        return configs

    def create_training_job(self, config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å‰µå»ºè¨“ç·´ä½œæ¥­"""
        taiwan_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        model_name = f"ERM_BASELINE_{config['name']}_{taiwan_time}"

        # æ­£ç¢ºè¨­ç½® P å’Œ U æ•¸æ“šæº
        if not self.exp_info or not self.exp_info.get('selectedDatasetIds'):
            print("âŒ æ²’æœ‰æ‰¾åˆ°æ•¸æ“šé›† IDï¼Œç„¡æ³•å‰µå»ºè¨“ç·´ä½œæ¥­")
            return None

        selected_dataset_ids = self.exp_info['selectedDatasetIds']

        # ç‚º PU Learning è¨­ç½®æ•¸æ“šæºé…ç½®
        data_source_config = {
            "trainRatio": 70,
            "validationRatio": 20,
            "testRatio": 10,
            "positiveDataSourceIds": selected_dataset_ids,  # P æ•¸æ“šä¾†æº
            "unlabeledDataSourceIds": selected_dataset_ids  # U æ•¸æ“šä¾†æº
        }

        payload = {
            "name": model_name,
            "scenario_type": "ERM_BASELINE",
            "experimentRunId": self.experiment_run_id,
            "modelConfig": json.dumps(config),
            "dataSourceConfig": json.dumps(data_source_config)
        }

        try:
            response = requests.post(
                f"{API_BASE}/api/v2/trained-models",
                headers={"Content-Type": "application/json"},
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                print(f"   âœ… è¨“ç·´ä½œæ¥­å·²é–‹å§‹: {result.get('jobId', 'N/A')}")
                return result
            else:
                print(f"   âŒ å‰µå»ºè¨“ç·´ä½œæ¥­å¤±æ•—: {response.status_code}")
                try:
                    error_detail = response.json()
                    print(f"   ğŸš¨ éŒ¯èª¤è©³æƒ…: {error_detail}")
                except:
                    print(f"   ğŸš¨ éŒ¯èª¤è©³æƒ…: {response.text}")
                return None

        except Exception as e:
            print(f"   âŒ å‰µå»ºè¨“ç·´ä½œæ¥­æ™‚å‡ºéŒ¯: {e}")
            return None

    def wait_for_training_completion(self, job_id: str, model_id: str, config_name: str) -> Optional[Dict[str, Any]]:
        """ç­‰å¾…è¨“ç·´å®Œæˆä¸¦ç²å–çµæœ"""
        print(f"   â³ ç­‰å¾…è¨“ç·´å®Œæˆ (æœ€é•·ç­‰å¾… {MAX_WAIT_TIME // 60} åˆ†é˜)...")

        start_time = time.time()
        last_status = None

        while time.time() - start_time < MAX_WAIT_TIME:
            try:
                # ç²å–æ‰€æœ‰è¨“ç·´æ¨¡å‹ä¾†æ‰¾åˆ°æˆ‘å€‘çš„æ¨¡å‹
                response = requests.get(f"{API_BASE}/api/v2/trained-models")
                if response.status_code == 200:
                    models = response.json()

                    # æ‰¾åˆ°æˆ‘å€‘çš„æ¨¡å‹
                    our_model = None
                    for model in models:
                        if model.get('id') == model_id or model.get('jobId') == job_id:
                            our_model = model
                            break

                    if our_model:
                        status = our_model.get('status', 'UNKNOWN')
                        if status != last_status:
                            print(f"   ğŸ“Š ç‹€æ…‹æ›´æ–°: {status}")
                            last_status = status

                        if status == 'COMPLETED':
                            print(f"   âœ… è¨“ç·´å®Œæˆ!")
                            return our_model
                        elif status == 'FAILED':
                            print(f"   âŒ è¨“ç·´å¤±æ•—")
                            return our_model
                        elif status in ['PENDING', 'RUNNING']:
                            # ç¹¼çºŒç­‰å¾…
                            pass
                        else:
                            print(f"   âš ï¸ æœªçŸ¥ç‹€æ…‹: {status}")
                    else:
                        print(f"   âš ï¸ æ‰¾ä¸åˆ°æ¨¡å‹ ID: {model_id}")
                else:
                    print(f"   âš ï¸ ç„¡æ³•ç²å–è¨“ç·´ç‹€æ…‹: {response.status_code}")

                time.sleep(POLL_INTERVAL)

            except Exception as e:
                print(f"   âš ï¸ æª¢æŸ¥è¨“ç·´ç‹€æ…‹æ™‚å‡ºéŒ¯: {e}")
                time.sleep(POLL_INTERVAL)

        print(f"   â° è¨“ç·´è¶…æ™‚ ({MAX_WAIT_TIME // 60} åˆ†é˜)")
        return None

    def extract_model_performance(self, model_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¾æ¨¡å‹æ•¸æ“šä¸­æå–æ€§èƒ½æŒ‡æ¨™"""
        performance = {
            "status": model_data.get('status', 'UNKNOWN'),
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "training_time": 0.0,
            "epochs_trained": 0,
            "best_epoch": 0,
            "early_stopped": False
        }

        try:
            # ğŸ”¥ ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„å­—æ®µåç¨±ï¼ˆä¸‹åŠƒç·šæ ¼å¼ï¼‰
            training_metrics = model_data.get('training_metrics') or model_data.get('trainingMetrics')
            if training_metrics:
                metrics = json.loads(training_metrics) if isinstance(training_metrics, str) else training_metrics

                performance.update({
                    "f1_score": metrics.get('best_val_f1_score', metrics.get('final_test_f1_score', 0.0)),
                    "precision": metrics.get('final_test_precision', 0.0),
                    "recall": metrics.get('final_test_recall', 0.0),
                    "training_time": metrics.get('training_time_seconds', 0.0),
                    "epochs_trained": metrics.get('total_epochs_trained', 0),
                    "best_epoch": metrics.get('best_epoch', 0),
                    "early_stopped": metrics.get('early_stopped', False)
                })

                print(f"   ğŸ” æ‰¾åˆ°è¨“ç·´æŒ‡æ¨™: F1={performance['f1_score']:.4f}, è¨“ç·´æ™‚é–“={performance['training_time']:.2f}s")

            # ğŸ”¥ ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„å­—æ®µåç¨±ï¼ˆä¸‹åŠƒç·šæ ¼å¼ï¼‰
            validation_metrics = model_data.get('validation_metrics') or model_data.get('validationMetrics')
            if validation_metrics:
                val_metrics = json.loads(validation_metrics) if isinstance(validation_metrics, str) else validation_metrics

                # å¦‚æœè¨“ç·´æŒ‡æ¨™ä¸­æ²’æœ‰ F1 åˆ†æ•¸ï¼Œå˜—è©¦å¾é©—è­‰æŒ‡æ¨™ä¸­ç²å–
                if performance["f1_score"] == 0.0:
                    performance["f1_score"] = val_metrics.get('test_f1_score', val_metrics.get('validation_f1_score', 0.0))
                    print(f"   ğŸ” å¾é©—è­‰æŒ‡æ¨™ç²å– F1: {performance['f1_score']:.4f}")

            # å¦‚æœé‚„æ˜¯æ²’æœ‰æ‰¾åˆ°æŒ‡æ¨™ï¼Œè¼¸å‡ºèª¿è©¦ä¿¡æ¯
            if performance["f1_score"] == 0.0:
                print(f"   âš ï¸ æœªæ‰¾åˆ°æ€§èƒ½æŒ‡æ¨™ï¼Œå¯ç”¨å­—æ®µ: {list(model_data.keys())}")

        except Exception as e:
            print(f"   âš ï¸ è§£ææ€§èƒ½æŒ‡æ¨™æ™‚å‡ºéŒ¯: {e}")

        return performance    def run_quick_test(self):
        """åŸ·è¡Œå¿«é€Ÿè¶…åƒæ•¸æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹ ERM Baseline å¿«é€Ÿè¶…åƒæ•¸æ¸¬è©¦")
        print("=" * 60)

        # ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯
        print(f"ğŸ” ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯: {self.experiment_run_id}")
        if not self.get_experiment_run_info():
            print("âŒ ç„¡æ³•ç²å–å¯¦é©—é‹è¡Œä¿¡æ¯ï¼Œé€€å‡º")
            return

        print(f"âœ… å¯¦é©—é‹è¡Œä¿¡æ¯ç²å–æˆåŠŸ")
        print(f"   ğŸ“Š å¯¦é©—åç¨±: {self.exp_info.get('name', 'N/A')}")
        print(f"   ğŸ“Š å¯ç”¨æ•¸æ“šé›†: {len(self.exp_info.get('selectedDatasetIds', []))}")
        print("")

        # ç”Ÿæˆæ¸¬è©¦é…ç½®
        configs = self.generate_test_configs()
        print(f"ğŸ“‹ ç”Ÿæˆäº† {len(configs)} çµ„å¿«é€Ÿæ¸¬è©¦é…ç½®")
        print("")

        # é–‹å§‹æ¸¬è©¦æ¯çµ„é…ç½®
        for i, config in enumerate(configs, 1):
            print(f"ğŸ“Š é…ç½® {i}/{len(configs)}: {config['name']}")
            print("-" * 60)

            # é¡¯ç¤ºé…ç½®è©³æƒ…
            print(f"ğŸš€ é–‹å§‹è¨“ç·´: {config['name']}")
            print(f"   ğŸ§  Hidden Size: {config['hiddenSize']}, Layers: {config['numLayers']}")
            print(f"   ğŸ“Š Window Size: {config['windowSize']}, Batch Size: {config['batchSize']}")
            print(f"   ğŸ“ˆ Learning Rate: {config['learningRate']}, Epochs: {config['epochs']}")

            # å‰µå»ºè¨“ç·´ä½œæ¥­
            job_result = self.create_training_job(config)
            if not job_result:
                print(f"   âŒ è·³éæ­¤é…ç½®")
                self.results.append({
                    "config": config,
                    "performance": {"status": "FAILED_TO_CREATE", "f1_score": 0.0},
                    "error": "Failed to create training job"
                })
                print("")
                continue

            job_id = job_result.get('jobId')
            model_id = job_result.get('id')

            # ç­‰å¾…è¨“ç·´å®Œæˆ
            final_model = self.wait_for_training_completion(job_id, model_id, config['name'])

            if final_model:
                performance = self.extract_model_performance(final_model)
                print(f"   ğŸ“Š æœ€çµ‚æ€§èƒ½:")
                print(f"      ğŸ¯ F1 Score: {performance['f1_score']:.4f}")
                print(f"      ğŸ“ˆ Precision: {performance['precision']:.4f}")
                print(f"      ğŸ“‰ Recall: {performance['recall']:.4f}")
                print(f"      â±ï¸ è¨“ç·´æ™‚é–“: {performance['training_time']:.2f}s")
                print(f"      ğŸ”„ è¨“ç·´è¼ªæ•¸: {performance['epochs_trained']}")

                self.results.append({
                    "config": config,
                    "performance": performance,
                    "model_id": model_id,
                    "job_id": job_id
                })
            else:
                print(f"   âŒ è¨“ç·´è¶…æ™‚æˆ–å¤±æ•—")
                self.results.append({
                    "config": config,
                    "performance": {"status": "TIMEOUT_OR_FAILED", "f1_score": 0.0},
                    "error": "Training timeout or failed"
                })

            print("")

        # é¡¯ç¤ºæœ€çµ‚çµæœ
        self.display_final_results()

    def display_final_results(self):
        """é¡¯ç¤ºæœ€çµ‚æ¸¬è©¦çµæœ"""
        print("ğŸ† å¿«é€Ÿè¶…åƒæ•¸æ¸¬è©¦çµæœç¸½çµ")
        print("=" * 80)

        # æŒ‰ F1 åˆ†æ•¸æ’åº
        valid_results = [r for r in self.results if r['performance']['status'] == 'COMPLETED']
        valid_results.sort(key=lambda x: x['performance']['f1_score'], reverse=True)

        if valid_results:
            print(f"âœ… æˆåŠŸå®Œæˆ {len(valid_results)} å€‹é…ç½®çš„è¨“ç·´")
            print("")

            # é¡¯ç¤ºæ‰€æœ‰çµæœ
            for i, result in enumerate(valid_results, 1):
                config = result['config']
                perf = result['performance']

                print(f"ğŸ¥‡ ç¬¬ {i} å: {config['name']}")
                print(f"   ğŸ¯ F1 Score: {perf['f1_score']:.4f}")
                print(f"   ğŸ“ˆ Precision: {perf['precision']:.4f}")
                print(f"   ğŸ“‰ Recall: {perf['recall']:.4f}")
                print(f"   â±ï¸ è¨“ç·´æ™‚é–“: {perf['training_time']:.2f}s")
                print(f"   ğŸ”„ è¨“ç·´è¼ªæ•¸: {perf['epochs_trained']}")
                print(f"   ğŸ§  æ¶æ§‹: Hidden={config['hiddenSize']}, Layers={config['numLayers']}")
                print(f"   ğŸ“Š åƒæ•¸: Window={config['windowSize']}, Batch={config['batchSize']}, LR={config['learningRate']}")
                print("")

            # æ¨è–¦æœ€ä½³é…ç½®
            if valid_results:
                best_config = valid_results[0]['config']
                print("ğŸ¯ æ¨è–¦é…ç½® (åŸºæ–¼æœ€é«˜ F1 Score):")
                print(f"   ğŸ“‹ åç¨±: {best_config['name']}")
                print(f"   ğŸ§  Hidden Size: {best_config['hiddenSize']}")
                print(f"   ğŸ—ï¸ Layers: {best_config['numLayers']}")
                print(f"   ğŸ“Š Window Size: {best_config['windowSize']}")
                print(f"   ğŸ² Batch Size: {best_config['batchSize']}")
                print(f"   ğŸ“ˆ Learning Rate: {best_config['learningRate']}")
                print(f"   ğŸ’§ Dropout: {best_config['dropout']}")
                print("")

        else:
            print("âŒ æ²’æœ‰æˆåŠŸå®Œæˆçš„è¨“ç·´é…ç½®")

        # é¡¯ç¤ºå¤±æ•—çµ±è¨ˆ
        failed_results = [r for r in self.results if r['performance']['status'] != 'COMPLETED']
        if failed_results:
            print(f"âš ï¸ å¤±æ•—çš„é…ç½®: {len(failed_results)}")
            for result in failed_results:
                print(f"   âŒ {result['config']['name']}: {result['performance']['status']}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”¬ ERM Baseline å¿«é€Ÿè¶…åƒæ•¸æ¸¬è©¦å·¥å…·")
    print(f"ğŸ¯ å¯¦é©—é‹è¡Œ ID: {EXPERIMENT_RUN_ID}")
    print("")

    # å‰µå»ºæ¸¬è©¦å™¨ä¸¦é‹è¡Œ
    tester = QuickHyperparameterTest(EXPERIMENT_RUN_ID)
    tester.run_quick_test()

    print("")
    print("ğŸ å¿«é€Ÿè¶…åƒæ•¸æ¸¬è©¦å®Œæˆ!")

if __name__ == "__main__":
    main()
