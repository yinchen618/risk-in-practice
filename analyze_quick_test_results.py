#!/usr/bin/env python3
"""
é‡æ–°åˆ†æå·²å®Œæˆçš„å¿«é€Ÿæ¸¬è©¦çµæœ
"""
import requests
import json

class ResultAnalyzer:
    def __init__(self):
        self.api_base = "http://localhost:8000"

    def extract_model_performance(self, model_data):
        """å¾æ¨¡å‹æ•¸æ“šä¸­æå–æ€§èƒ½æŒ‡æ¨™"""
        performance = {
            "status": model_data.get('status', 'UNKNOWN'),
            "f1_score": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "training_time": 0.0,
            "epochs_trained": 0,
            "best_epoch": 0,
            "early_stopped": False
        }

        try:
            # ä½¿ç”¨æ­£ç¢ºçš„å­—æ®µåç¨±ï¼ˆä¸‹åŠƒç·šæ ¼å¼ï¼‰
            training_metrics = model_data.get('training_metrics') or model_data.get('trainingMetrics')
            if training_metrics:
                metrics = json.loads(training_metrics) if isinstance(training_metrics, str) else training_metrics

                performance.update({
                    "f1_score": metrics.get('best_val_f1_score', metrics.get('final_test_f1_score', 0.0)),
                    "precision": metrics.get('final_test_precision', 0.0),
                    "recall": metrics.get('final_test_recall', 0.0),
                    "training_time": metrics.get('training_time_seconds', 0.0),
                    "epochs_trained": metrics.get('total_epochs_trained', 0),
                    "best_epoch": metrics.get('best_epoch', 0),
                    "early_stopped": metrics.get('early_stopped', False)
                })

        except Exception as e:
            print(f"   âš ï¸ è§£ææ€§èƒ½æŒ‡æ¨™æ™‚å‡ºéŒ¯: {e}")

        return performance

    def analyze_completed_tests(self):
        """åˆ†æå·²å®Œæˆçš„å¿«é€Ÿæ¸¬è©¦çµæœ"""
        print("ğŸ“Š é‡æ–°åˆ†æå¿«é€Ÿæ¸¬è©¦çµæœ")
        print("=" * 60)

        # ç²å–æ‰€æœ‰æ¨¡å‹
        response = requests.get(f"{self.api_base}/api/v2/trained-models")
        if response.status_code != 200:
            print("âŒ ç„¡æ³•ç²å–æ¨¡å‹åˆ—è¡¨")
            return

        models = response.json()

        # æ‰¾åˆ°å¿«é€Ÿæ¸¬è©¦çš„æ¨¡å‹
        quick_models = []
        for model in models:
            if (model.get('name', '').startswith('ERM_BASELINE_Quick_') and
                model.get('status') == 'COMPLETED'):
                quick_models.append(model)

        if not quick_models:
            print("âŒ æ²’æœ‰æ‰¾åˆ°å·²å®Œæˆçš„å¿«é€Ÿæ¸¬è©¦æ¨¡å‹")
            return

        print(f"âœ… æ‰¾åˆ° {len(quick_models)} å€‹å·²å®Œæˆçš„å¿«é€Ÿæ¸¬è©¦æ¨¡å‹")
        print("")

        # åˆ†ææ¯å€‹æ¨¡å‹
        results = []
        for model in quick_models:
            config_name = model['name'].split('_')[2]  # æå–é…ç½®åç¨±
            performance = self.extract_model_performance(model)

            # å¾ model_config ä¸­æå–é…ç½®ä¿¡æ¯
            model_config = model.get('model_config', {})

            results.append({
                "config_name": config_name,
                "model_config": model_config,
                "performance": performance,
                "model": model
            })

        # æŒ‰ F1 åˆ†æ•¸æ’åº
        results.sort(key=lambda x: x['performance']['f1_score'], reverse=True)

        # é¡¯ç¤ºçµæœ
        for i, result in enumerate(results, 1):
            config = result['model_config']
            perf = result['performance']

            print(f"ğŸ¥‡ ç¬¬ {i} å: {result['config_name']}")
            print(f"   ğŸ¯ F1 Score: {perf['f1_score']:.4f}")
            print(f"   ğŸ“ˆ Precision: {perf['precision']:.4f}")
            print(f"   ğŸ“‰ Recall: {perf['recall']:.4f}")
            print(f"   â±ï¸ è¨“ç·´æ™‚é–“: {perf['training_time']:.2f}s")
            print(f"   ğŸ”„ è¨“ç·´è¼ªæ•¸: {perf['epochs_trained']}")
            print(f"   ğŸ§  æ¶æ§‹: Hidden={config.get('hiddenSize')}, Layers={config.get('numLayers')}")
            print(f"   ğŸ“Š åƒæ•¸: Window={config.get('windowSize')}, Batch={config.get('batchSize')}, LR={config.get('learningRate')}")
            print("")

        if results:
            best_result = results[0]
            print("ğŸ¯ æ¨è–¦é…ç½® (åŸºæ–¼æœ€é«˜ F1 Score):")
            print(f"   ğŸ“‹ åç¨±: {best_result['config_name']}")
            print(f"   ğŸ¯ F1 Score: {best_result['performance']['f1_score']:.4f}")
            config = best_result['model_config']
            print(f"   ğŸ§  Hidden Size: {config.get('hiddenSize')}")
            print(f"   ğŸ—ï¸ Layers: {config.get('numLayers')}")
            print(f"   ğŸ“Š Window Size: {config.get('windowSize')}")
            print(f"   ğŸ² Batch Size: {config.get('batchSize')}")
            print(f"   ğŸ“ˆ Learning Rate: {config.get('learningRate')}")
            print(f"   ğŸ’§ Dropout: {config.get('dropout')}")

def main():
    analyzer = ResultAnalyzer()
    analyzer.analyze_completed_tests()

if __name__ == "__main__":
    main()
