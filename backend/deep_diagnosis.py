#!/usr/bin/env python3
"""
æ·±åº¦è¨ºæ–·PU LearningéŒ¯èª¤ç‡éé«˜å•é¡Œ
æª¢æŸ¥æ•¸æ“šç”Ÿæˆã€æ¨¡å‹è¨“ç·´ã€éŒ¯èª¤ç‡è¨ˆç®—çš„æ¯å€‹ç’°ç¯€
"""
import sys
import os
sys.path.append('pu-learning')

import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score, classification_report
from data_generator import generate_synthetic_data
from pulearning_engine import run_pu_simulation, MLPClassifier

def diagnose_data_generation():
    """è¨ºæ–·æ•¸æ“šç”Ÿæˆæ˜¯å¦æ­£ç¢º"""
    print("ğŸ” Step 1: è¨ºæ–·æ•¸æ“šç”Ÿæˆ")
    print("="*50)
    
    # ç”Ÿæˆæ•¸æ“š
    xp, xu, xt_p, xt_n = generate_synthetic_data(
        distribution='two_moons',
        dims=2,
        n_p=50,
        n_u=300,
        prior=0.3,
        n_test=1000
    )
    
    print(f"ğŸ“Š æ•¸æ“šçµ±è¨ˆ:")
    print(f"   â€¢ æ­£æ¨£æœ¬ (xp): {xp.shape}")
    print(f"   â€¢ æœªæ¨™è¨˜æ¨£æœ¬ (xu): {xu.shape}")
    print(f"   â€¢ æ¸¬è©¦æ­£æ¨£æœ¬ (xt_p): {xt_p.shape}")
    print(f"   â€¢ æ¸¬è©¦è² æ¨£æœ¬ (xt_n): {xt_n.shape}")
    
    # æª¢æŸ¥æ¸¬è©¦é›†æ¯”ä¾‹
    total_test = len(xt_p) + len(xt_n)
    actual_prior = len(xt_p) / total_test
    print(f"   â€¢ æœŸæœ›æ¸¬è©¦é›† prior: 0.3")
    print(f"   â€¢ å¯¦éš›æ¸¬è©¦é›† prior: {actual_prior:.3f}")
    
    # æª¢æŸ¥æœªæ¨™è¨˜æ¨£æœ¬çš„çœŸå¯¦çµ„æˆ
    # é€™éœ€è¦æˆ‘å€‘çŸ¥é“xuä¸­å“ªäº›æ˜¯æ­£æ¨£æœ¬ï¼Œå“ªäº›æ˜¯è² æ¨£æœ¬
    
    return xp, xu, xt_p, xt_n

def diagnose_model_performance(xp, xu, xt_p, xt_n):
    """è¨ºæ–·æ¨¡å‹æ€§èƒ½"""
    print("\nğŸ” Step 2: è¨ºæ–·æ¨¡å‹æ€§èƒ½")
    print("="*50)
    
    # æº–å‚™æ¸¬è©¦æ•¸æ“šå’Œæ¨™ç±¤
    X_test = np.vstack([xt_p, xt_n])
    y_test = np.hstack([np.ones(len(xt_p)), np.zeros(len(xt_n))])
    
    print(f"ğŸ“Š æ¸¬è©¦é›†çµ±è¨ˆ:")
    print(f"   â€¢ æ¸¬è©¦æ¨£æœ¬ç¸½æ•¸: {len(X_test)}")
    print(f"   â€¢ æ­£æ¨£æœ¬æ•¸: {np.sum(y_test == 1)}")
    print(f"   â€¢ è² æ¨£æœ¬æ•¸: {np.sum(y_test == 0)}")
    print(f"   â€¢ æ­£æ¨£æœ¬æ¯”ä¾‹: {np.mean(y_test):.3f}")
    
    # æ¸¬è©¦ç°¡å–®åŸºç·šæ¨¡å‹
    print(f"\nğŸ—ï¸  æ¸¬è©¦åŸºç·šæ¨¡å‹:")
    
    # 1. éš¨æ©Ÿåˆ†é¡å™¨
    random_pred = np.random.binomial(1, 0.5, len(y_test))
    random_acc = accuracy_score(y_test, random_pred)
    print(f"   â€¢ éš¨æ©Ÿåˆ†é¡å™¨æº–ç¢ºç‡: {random_acc:.3f} (éŒ¯èª¤ç‡: {1-random_acc:.3f})")
    
    # 2. ç¸½æ˜¯é æ¸¬å¤šæ•¸é¡
    majority_pred = np.zeros(len(y_test))  # å‡è¨­è² æ¨£æœ¬æ˜¯å¤šæ•¸
    majority_acc = accuracy_score(y_test, majority_pred)
    print(f"   â€¢ å¤šæ•¸é¡åˆ†é¡å™¨æº–ç¢ºç‡: {majority_acc:.3f} (éŒ¯èª¤ç‡: {1-majority_acc:.3f})")
    
    return X_test, y_test

def diagnose_supervised_baseline(xp, xu, xt_p, xt_n):
    """ç”¨ç›£ç£å­¸ç¿’å»ºç«‹åŸºç·š"""
    print("\nğŸ” Step 3: ç›£ç£å­¸ç¿’åŸºç·š")
    print("="*50)
    
    # å‡è¨­æˆ‘å€‘çŸ¥é“xuä¸­çš„çœŸå¯¦æ¨™ç±¤ï¼ˆé€™åªæ˜¯ç‚ºäº†è¨ºæ–·ï¼‰
    # é‡æ–°ç”Ÿæˆæ•¸æ“šä»¥ç²å¾—çœŸå¯¦æ¨™ç±¤
    np.random.seed(42)
    from sklearn.datasets import make_moons
    
    total_samples = 50 + 300 + 2000  # n_p + n_u + 2*n_test
    X_all, y_all = make_moons(n_samples=total_samples, noise=0.1, random_state=42)
    
    # åˆ†é›¢æ¨£æœ¬
    pos_indices = np.where(y_all == 1)[0]
    neg_indices = np.where(y_all == 0)[0]
    
    # è¨“ç·´æ•¸æ“šï¼šæ­£æ¨£æœ¬ + ä¸€éƒ¨åˆ†æœªæ¨™è¨˜æ¨£æœ¬ï¼ˆæ¨¡æ“¬å®Œå…¨ç›£ç£ï¼‰
    X_train = X_all[:350]  # å‰350å€‹æ¨£æœ¬ä½œç‚ºè¨“ç·´
    y_train = y_all[:350]
    
    # æ¸¬è©¦æ•¸æ“š
    X_test = np.vstack([xt_p, xt_n])
    y_test = np.hstack([np.ones(len(xt_p)), np.zeros(len(xt_n))])
    
    print(f"ğŸ“Š ç›£ç£å­¸ç¿’è¨­ç½®:")
    print(f"   â€¢ è¨“ç·´æ¨£æœ¬: {len(X_train)}")
    print(f"   â€¢ è¨“ç·´æ­£æ¨£æœ¬: {np.sum(y_train == 1)}")
    print(f"   â€¢ è¨“ç·´è² æ¨£æœ¬: {np.sum(y_train == 0)}")
    
    # è¨“ç·´ç°¡å–®çš„ç¥ç¶“ç¶²è·¯
    device = torch.device('cpu')
    model = MLPClassifier(input_dim=2, hidden_dim=64)
    model.to(device)
    
    # è½‰æ›ç‚ºå¼µé‡
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.FloatTensor(y_train).to(device)
    X_test_tensor = torch.FloatTensor(X_test).to(device)
    
    # è¨“ç·´
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.BCEWithLogitsLoss()
    
    model.train()
    for epoch in range(100):
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs.squeeze(), y_train_tensor)
        loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0:
            print(f"   Epoch {epoch}, Loss: {loss.item():.4f}")
    
    # æ¸¬è©¦
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        test_probs = torch.sigmoid(test_outputs).squeeze()
        test_pred = (test_probs > 0.5).cpu().numpy()
    
    sup_acc = accuracy_score(y_test, test_pred)
    print(f"\nâœ… ç›£ç£å­¸ç¿’çµæœ:")
    print(f"   â€¢ æº–ç¢ºç‡: {sup_acc:.3f}")
    print(f"   â€¢ éŒ¯èª¤ç‡: {1-sup_acc:.3f}")
    
    if 1-sup_acc < 0.05:
        print("   âœ… ç›£ç£å­¸ç¿’éŒ¯èª¤ç‡æ­£å¸¸ (<5%)")
        return True
    else:
        print("   âš ï¸  ç›£ç£å­¸ç¿’éŒ¯èª¤ç‡éé«˜ï¼Œå¯èƒ½æ˜¯æ•¸æ“šæœ¬èº«çš„å•é¡Œ")
        return False

def main():
    """ä¸»è¨ºæ–·æµç¨‹"""
    print("ğŸ¥ PU Learning æ·±åº¦è¨ºæ–·")
    print("="*80)
    
    # Step 1: æª¢æŸ¥æ•¸æ“šç”Ÿæˆ
    xp, xu, xt_p, xt_n = diagnose_data_generation()
    
    # Step 2: æª¢æŸ¥æ¨¡å‹æ€§èƒ½
    X_test, y_test = diagnose_model_performance(xp, xu, xt_p, xt_n)
    
    # Step 3: ç›£ç£å­¸ç¿’åŸºç·š
    supervised_ok = diagnose_supervised_baseline(xp, xu, xt_p, xt_n)
    
    print("\n" + "="*80)
    print("ğŸ“‹ è¨ºæ–·ç¸½çµ:")
    
    if supervised_ok:
        print("âœ… æ•¸æ“šå’ŒåŸºæœ¬æ¨¡å‹éƒ½æ­£å¸¸")
        print("ğŸ’¡ å•é¡Œå¯èƒ½åœ¨PU Learningç®—æ³•å¯¦ç¾æˆ–åƒæ•¸é…ç½®")
    else:
        print("âš ï¸  åŸºç¤ç›£ç£å­¸ç¿’å°±æœ‰å•é¡Œ")
        print("ğŸ’¡ éœ€è¦æª¢æŸ¥æ•¸æ“šç”Ÿæˆæˆ–æ¨¡å‹æ¶æ§‹")
    
    # æœ€å¾Œæ¸¬è©¦ç•¶å‰çš„PU Learningé…ç½®
    print("\nğŸ” Step 4: æ¸¬è©¦ç•¶å‰PU Learningé…ç½®")
    print("="*50)
    
    try:
        # æ¨¡æ“¬APIè«‹æ±‚æ ¼å¼
        class MockRequest:
            def __init__(self):
                self.algorithm = "nnPU"
                self.data_params = MockDataParams()
                self.model_params = MockModelParams()
        
        class MockDataParams:
            def __init__(self):
                self.distribution = "two_moons"
                self.dims = 2
                self.n_p = 50
                self.n_u = 300
                self.prior = 0.3
        
        class MockModelParams:
            def __init__(self):
                self.activation = "relu"
                self.n_epochs = 50
                self.learning_rate = 0.001
                self.hidden_dim = 200  # ä½¿ç”¨ä¹‹å‰æ‰¾åˆ°çš„æœ€ä½³é…ç½®
                self.weight_decay = 0.005
        
        request = MockRequest()
        results = run_pu_simulation(request)
        
        pu_error = results['metrics']['error_rate']
        pu_prior = results['metrics']['estimated_prior']
        
        print(f"ğŸ“Š PU Learningçµæœ:")
        print(f"   â€¢ éŒ¯èª¤ç‡: {pu_error:.3f} ({pu_error*100:.1f}%)")
        print(f"   â€¢ ä¼°è¨ˆå…ˆé©—: {pu_prior:.3f}")
        print(f"   â€¢ çœŸå¯¦å…ˆé©—: 0.3")
        
        if pu_error < 0.05:
            print("âœ… PU Learningæ€§èƒ½æ­£å¸¸")
        else:
            print("âš ï¸  PU LearningéŒ¯èª¤ç‡ä»ç„¶éé«˜")
            
    except Exception as e:
        print(f"âŒ PU Learningæ¸¬è©¦å¤±æ•—: {e}")

if __name__ == "__main__":
    main()
