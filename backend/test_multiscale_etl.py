"""
å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹æ¸¬è©¦è…³æœ¬

é€™å€‹è…³æœ¬ç”¨æ–¼æ¸¬è©¦å’Œé©—è­‰å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹çš„å¯¦ç¾ã€‚
åŒ…å«å–®å…ƒæ¸¬è©¦ã€åŠŸèƒ½æ¸¬è©¦å’Œæ•ˆèƒ½æ¸¬è©¦ã€‚
"""

import asyncio
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List
import logging

# å°å…¥æˆ‘å€‘çš„æ¨¡çµ„
from data_preprocessing_etl_multiscale import (
    MultiscaleETL,
    RoomInfo,
    OccupantType
)
from multiscale_config import (
    MULTISCALE_CONFIG,
    FEATURE_PRESETS,
    get_multiscale_config,
    calculate_feature_count,
    get_feature_list,
    validate_multiscale_config
)

# è¨­å®šæ¸¬è©¦æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MultiscaleETLTester:
    """å¤šå°ºåº¦ETLæ¸¬è©¦å™¨"""

    def __init__(self):
        self.etl = None
        self.test_data = None

    def create_test_data(self, duration_hours: int = 3) -> pd.DataFrame:
        """
        å‰µå»ºæ¸¬è©¦æ•¸æ“š

        Args:
            duration_hours: æ¸¬è©¦æ•¸æ“šæŒçºŒæ™‚é–“ï¼ˆå°æ™‚ï¼‰

        Returns:
            pd.DataFrame: æ¸¬è©¦æ•¸æ“š
        """
        logger.info(f"ç”Ÿæˆ {duration_hours} å°æ™‚çš„æ¸¬è©¦æ•¸æ“š...")

        # ç”Ÿæˆæ™‚é–“åºåˆ—ï¼ˆæ¯åˆ†é˜ä¸€ç­†ï¼‰
        start_time = datetime(2024, 1, 15, 9, 0)  # é€±ä¸€ä¸Šåˆ9é»
        end_time = start_time + timedelta(hours=duration_hours)
        timestamps = pd.date_range(start_time, end_time, freq='1T')

        test_data = []

        for timestamp in timestamps:
            # æ¨¡æ“¬æ­£å¸¸ç”¨é›»æ¨¡å¼ï¼ˆå·¥ä½œæ™‚é–“è¼ƒé«˜ï¼‰
            hour = timestamp.hour
            base_power = 100 if 9 <= hour <= 18 else 30

            # æ·»åŠ éš¨æ©Ÿå™ªè²
            noise = np.random.normal(0, 10)

            # æ¨¡æ“¬ç•°å¸¸ï¼ˆéš¨æ©Ÿå‡ºç¾ï¼‰
            is_anomaly = np.random.random() < 0.05  # 5%æ©Ÿç‡ç•°å¸¸
            anomaly_multiplier = 3 if is_anomaly else 1

            l1_power = max(0, (base_power + noise) * anomaly_multiplier)
            l2_power = max(0, (base_power * 0.8 + noise) * anomaly_multiplier)

            test_data.append({
                'timestamp': timestamp,
                'rawWattageL1': l1_power,
                'rawWattageL2': l2_power,
                'room': 'TEST_ROOM_A1',
                'is_test_anomaly': is_anomaly
            })

        df = pd.DataFrame(test_data)
        logger.info(f"ç”Ÿæˆäº† {len(df)} ç­†æ¸¬è©¦æ•¸æ“š")
        logger.info(f"ç•°å¸¸æ¨£æœ¬æ•¸: {df['is_test_anomaly'].sum()}")

        return df

    async def test_feature_extraction(self, preset_name: str = "full") -> Dict:
        """
        æ¸¬è©¦ç‰¹å¾µæå–åŠŸèƒ½

        Args:
            preset_name: ç‰¹å¾µé è¨­é…ç½®

        Returns:
            Dict: æ¸¬è©¦çµæœ
        """
        logger.info(f"æ¸¬è©¦ç‰¹å¾µæå– - é…ç½®: {preset_name}")

        # ç²å–é…ç½®
        config = get_multiscale_config(preset_name)

        # å‰µå»ºETLå¯¦ä¾‹
        room_info = RoomInfo(
            room_name="TEST_ROOM_A1",
            occupant_type=OccupantType.OFFICE,
            organization_id=1
        )

        # æ¨¡æ“¬æ•¸æ“šåº«é€£æ¥ï¼ˆä¸å¯¦éš›é€£æ¥ï¼‰
        self.etl = MultiscaleETL(
            connection_string="postgresql://test:test@test:5432/test",
            config=config
        )

        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        test_data = self.create_test_data(duration_hours=3)

        # æ¸¬è©¦ç‰¹å¾µç”Ÿæˆ
        logger.info("é–‹å§‹ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾µ...")
        start_time = datetime.now()

        try:
            # ç›´æ¥èª¿ç”¨ç‰¹å¾µç”Ÿæˆæ–¹æ³•ï¼ˆç¹éæ•¸æ“šåº«ï¼‰
            features_df = self.etl.generate_multiscale_features(
                data=test_data,
                room_info=room_info
            )

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            # è¨ˆç®—ç‰¹å¾µçµ±è¨ˆ
            results = {
                "success": True,
                "processing_time_seconds": processing_time,
                "input_samples": len(test_data),
                "output_samples": len(features_df),
                "feature_count": len(features_df.columns),
                "feature_names": list(features_df.columns),
                "sample_efficiency": len(features_df) / len(test_data),
                "features_preview": features_df.head(3).to_dict(),
                "feature_statistics": {}
            }

            # è¨ˆç®—æ•¸å€¼ç‰¹å¾µçš„çµ±è¨ˆ
            numeric_columns = features_df.select_dtypes(include=[np.number]).columns
            for col in numeric_columns:
                if col in features_df.columns:
                    series = features_df[col]
                    results["feature_statistics"][col] = {
                        "mean": float(series.mean()) if not series.isna().all() else None,
                        "std": float(series.std()) if not series.isna().all() else None,
                        "min": float(series.min()) if not series.isna().all() else None,
                        "max": float(series.max()) if not series.isna().all() else None,
                        "null_count": int(series.isna().sum())
                    }

            logger.info(f"ç‰¹å¾µæå–å®Œæˆ - è€—æ™‚: {processing_time:.2f}ç§’")
            logger.info(f"è¼¸å…¥æ¨£æœ¬: {results['input_samples']}, è¼¸å‡ºæ¨£æœ¬: {results['output_samples']}")
            logger.info(f"ç‰¹å¾µæ•¸é‡: {results['feature_count']}")

            return results

        except Exception as e:
            logger.error(f"ç‰¹å¾µæå–å¤±æ•—: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "processing_time_seconds": (datetime.now() - start_time).total_seconds()
            }

    def test_window_alignment(self) -> Dict:
        """
        æ¸¬è©¦æ™‚é–“çª—å£å°é½ŠåŠŸèƒ½
        """
        logger.info("æ¸¬è©¦æ™‚é–“çª—å£å°é½Š...")

        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        test_data = self.create_test_data(duration_hours=2)

        results = {
            "success": True,
            "tests": {}
        }

        try:
            # æ¸¬è©¦ä¸åŒçª—å£å¤§å°
            for window_size in [15, 30, 60]:
                logger.info(f"æ¸¬è©¦ {window_size} åˆ†é˜çª—å£...")

                # æ¨¡æ“¬çª—å£ç‰¹å¾µæå–é‚è¼¯
                grouped_data = []
                current_time = test_data['timestamp'].min()
                end_time = test_data['timestamp'].max()

                while current_time <= end_time:
                    window_end = current_time
                    window_start = window_end - timedelta(minutes=window_size)

                    # ç¯©é¸çª—å£å…§æ•¸æ“š
                    window_data = test_data[
                        (test_data['timestamp'] > window_start) &
                        (test_data['timestamp'] <= window_end)
                    ]

                    if len(window_data) > 0:
                        window_features = {
                            'timestamp': current_time,
                            'samples_in_window': len(window_data),
                            'l1_mean': window_data['rawWattageL1'].mean(),
                            'l1_std': window_data['rawWattageL1'].std(),
                            'l2_mean': window_data['rawWattageL2'].mean(),
                            'l2_std': window_data['rawWattageL2'].std(),
                        }
                        grouped_data.append(window_features)

                    current_time += timedelta(minutes=1)  # 1åˆ†é˜æ­¥é•·

                results["tests"][f"{window_size}min_window"] = {
                    "window_count": len(grouped_data),
                    "avg_samples_per_window": np.mean([d['samples_in_window'] for d in grouped_data]),
                    "coverage_ratio": len(grouped_data) / (len(test_data) / window_size)
                }

                logger.info(f"{window_size}åˆ†é˜çª—å£: {len(grouped_data)} å€‹çª—å£")

        except Exception as e:
            logger.error(f"çª—å£å°é½Šæ¸¬è©¦å¤±æ•—: {str(e)}")
            results["success"] = False
            results["error"] = str(e)

        return results

    def test_configuration_validation(self) -> Dict:
        """
        æ¸¬è©¦é…ç½®é©—è­‰åŠŸèƒ½
        """
        logger.info("æ¸¬è©¦é…ç½®é©—è­‰...")

        results = {
            "preset_validations": {},
            "custom_validations": {}
        }

        # æ¸¬è©¦é è¨­é…ç½®
        for preset_name in FEATURE_PRESETS.keys():
            config = get_multiscale_config(preset_name)
            is_valid = validate_multiscale_config(config)
            feature_count = calculate_feature_count(config)

            results["preset_validations"][preset_name] = {
                "is_valid": is_valid,
                "feature_count": feature_count,
                "config": config
            }

        # æ¸¬è©¦éŒ¯èª¤é…ç½®
        invalid_configs = [
            {
                "name": "long_smaller_than_short",
                "config": {"long_window_size": 10, "short_window_size": 20}
            },
            {
                "name": "zero_step_size",
                "config": {"feature_step_size": 0}
            },
            {
                "name": "all_features_disabled",
                "config": {
                    "enable_long_window_features": False,
                    "enable_short_window_features": False,
                    "enable_time_features": False,
                    "enable_current_features": False
                }
            }
        ]

        for test_case in invalid_configs:
            test_config = {**get_multiscale_config("full"), **test_case["config"]}
            is_valid = validate_multiscale_config(test_config)

            results["custom_validations"][test_case["name"]] = {
                "is_valid": is_valid,
                "should_be_invalid": True,
                "test_passed": not is_valid  # æ‡‰è©²ç„¡æ•ˆ
            }

        return results

    async def run_comprehensive_test(self) -> Dict:
        """
        åŸ·è¡Œç¶œåˆæ¸¬è©¦
        """
        logger.info("é–‹å§‹ç¶œåˆæ¸¬è©¦...")

        all_results = {
            "test_start_time": datetime.now().isoformat(),
            "tests": {}
        }

        # 1. é…ç½®é©—è­‰æ¸¬è©¦
        logger.info("1/4 - é…ç½®é©—è­‰æ¸¬è©¦")
        all_results["tests"]["configuration"] = self.test_configuration_validation()

        # 2. çª—å£å°é½Šæ¸¬è©¦
        logger.info("2/4 - çª—å£å°é½Šæ¸¬è©¦")
        all_results["tests"]["window_alignment"] = self.test_window_alignment()

        # 3. ç‰¹å¾µæå–æ¸¬è©¦ï¼ˆå¤šç¨®é…ç½®ï¼‰
        logger.info("3/4 - ç‰¹å¾µæå–æ¸¬è©¦")
        all_results["tests"]["feature_extraction"] = {}

        for preset_name in ["full", "balanced", "short_only"]:
            logger.info(f"æ¸¬è©¦ {preset_name} é…ç½®...")
            all_results["tests"]["feature_extraction"][preset_name] = await self.test_feature_extraction(preset_name)

        # 4. æ•ˆèƒ½æ¸¬è©¦
        logger.info("4/4 - æ•ˆèƒ½æ¸¬è©¦")
        all_results["tests"]["performance"] = await self.test_performance()

        all_results["test_end_time"] = datetime.now().isoformat()

        # ç”Ÿæˆæ¸¬è©¦æ‘˜è¦
        all_results["summary"] = self.generate_test_summary(all_results)

        return all_results

    async def test_performance(self) -> Dict:
        """
        æ¸¬è©¦æ•ˆèƒ½è¡¨ç¾
        """
        logger.info("æ¸¬è©¦æ•ˆèƒ½è¡¨ç¾...")

        results = {}

        # æ¸¬è©¦ä¸åŒæ•¸æ“šå¤§å°
        for hours in [1, 6, 24]:
            logger.info(f"æ¸¬è©¦ {hours} å°æ™‚æ•¸æ“š...")

            test_data = self.create_test_data(duration_hours=hours)
            start_time = datetime.now()

            try:
                # æ¨¡æ“¬ç‰¹å¾µæå–ï¼ˆç°¡åŒ–ç‰ˆï¼‰
                feature_results = await self.test_feature_extraction("balanced")

                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()

                results[f"{hours}h_data"] = {
                    "input_samples": len(test_data),
                    "processing_time": processing_time,
                    "samples_per_second": len(test_data) / processing_time if processing_time > 0 else 0,
                    "success": feature_results["success"]
                }

            except Exception as e:
                results[f"{hours}h_data"] = {
                    "success": False,
                    "error": str(e)
                }

        return results

    def generate_test_summary(self, results: Dict) -> Dict:
        """
        ç”Ÿæˆæ¸¬è©¦æ‘˜è¦
        """
        summary = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "test_categories": {}
        }

        # çµ±è¨ˆå„é¡æ¸¬è©¦çµæœ
        for category, test_results in results["tests"].items():
            category_summary = {"passed": 0, "failed": 0, "total": 0}

            if category == "configuration":
                # é…ç½®æ¸¬è©¦
                for preset_name, preset_result in test_results["preset_validations"].items():
                    category_summary["total"] += 1
                    if preset_result["is_valid"]:
                        category_summary["passed"] += 1
                    else:
                        category_summary["failed"] += 1

                for test_name, test_result in test_results["custom_validations"].items():
                    category_summary["total"] += 1
                    if test_result["test_passed"]:
                        category_summary["passed"] += 1
                    else:
                        category_summary["failed"] += 1

            elif category == "feature_extraction":
                # ç‰¹å¾µæå–æ¸¬è©¦
                for preset_name, preset_result in test_results.items():
                    category_summary["total"] += 1
                    if preset_result["success"]:
                        category_summary["passed"] += 1
                    else:
                        category_summary["failed"] += 1

            elif category == "window_alignment":
                # çª—å£å°é½Šæ¸¬è©¦
                category_summary["total"] = 1
                if test_results["success"]:
                    category_summary["passed"] = 1
                else:
                    category_summary["failed"] = 1

            elif category == "performance":
                # æ•ˆèƒ½æ¸¬è©¦
                for test_name, test_result in test_results.items():
                    category_summary["total"] += 1
                    if test_result.get("success", False):
                        category_summary["passed"] += 1
                    else:
                        category_summary["failed"] += 1

            summary["test_categories"][category] = category_summary
            summary["total_tests"] += category_summary["total"]
            summary["passed_tests"] += category_summary["passed"]
            summary["failed_tests"] += category_summary["failed"]

        summary["success_rate"] = summary["passed_tests"] / summary["total_tests"] if summary["total_tests"] > 0 else 0

        return summary

def print_test_results(results: Dict):
    """
    ç¾è§€åœ°å°å‡ºæ¸¬è©¦çµæœ
    """
    print("\n" + "="*60)
    print("å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹æ¸¬è©¦å ±å‘Š")
    print("="*60)

    # æ¸¬è©¦æ‘˜è¦
    summary = results["summary"]
    print(f"\nã€æ¸¬è©¦æ‘˜è¦ã€‘")
    print(f"ç¸½æ¸¬è©¦æ•¸: {summary['total_tests']}")
    print(f"é€šéæ¸¬è©¦: {summary['passed_tests']}")
    print(f"å¤±æ•—æ¸¬è©¦: {summary['failed_tests']}")
    print(f"æˆåŠŸç‡: {summary['success_rate']:.1%}")

    # å„é¡åˆ¥æ¸¬è©¦çµæœ
    print(f"\nã€å„é¡åˆ¥çµæœã€‘")
    for category, stats in summary["test_categories"].items():
        status = "âœ…" if stats["failed"] == 0 else "âŒ"
        print(f"{status} {category}: {stats['passed']}/{stats['total']} é€šé")

    # ç‰¹å¾µæå–è©³ç´°çµæœ
    if "feature_extraction" in results["tests"]:
        print(f"\nã€ç‰¹å¾µæå–æ¸¬è©¦è©³æƒ…ã€‘")
        for preset_name, result in results["tests"]["feature_extraction"].items():
            if result["success"]:
                print(f"âœ… {preset_name}:")
                print(f"   è™•ç†æ™‚é–“: {result['processing_time_seconds']:.2f}ç§’")
                print(f"   è¼¸å…¥æ¨£æœ¬: {result['input_samples']}")
                print(f"   è¼¸å‡ºæ¨£æœ¬: {result['output_samples']}")
                print(f"   ç‰¹å¾µæ•¸é‡: {result['feature_count']}")
                print(f"   æ¨£æœ¬æ•ˆç‡: {result['sample_efficiency']:.2f}")
            else:
                print(f"âŒ {preset_name}: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

    # æ•ˆèƒ½æ¸¬è©¦çµæœ
    if "performance" in results["tests"]:
        print(f"\nã€æ•ˆèƒ½æ¸¬è©¦çµæœã€‘")
        for test_name, result in results["tests"]["performance"].items():
            if result.get("success", False):
                print(f"âœ… {test_name}:")
                print(f"   æ¨£æœ¬æ•¸: {result['input_samples']}")
                print(f"   è™•ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
                print(f"   è™•ç†é€Ÿåº¦: {result['samples_per_second']:.1f} æ¨£æœ¬/ç§’")
            else:
                print(f"âŒ {test_name}: {result.get('error', 'æ¸¬è©¦å¤±æ•—')}")

    print("\n" + "="*60)

async def main():
    """
    ä¸»æ¸¬è©¦å‡½æ•¸
    """
    print("ğŸš€ å•Ÿå‹•å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹æ¸¬è©¦...")

    tester = MultiscaleETLTester()

    try:
        # åŸ·è¡Œç¶œåˆæ¸¬è©¦
        results = await tester.run_comprehensive_test()

        # å°å‡ºçµæœ
        print_test_results(results)

        # ä¿å­˜çµæœåˆ°æ–‡ä»¶
        import json
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"multiscale_test_results_{timestamp}.json"

        # è½‰æ› datetime å°è±¡ç‚ºå­—ç¬¦ä¸²
        def json_serializer(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif pd.isna(obj):
                return None
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=json_serializer)

        print(f"\nğŸ“Š è©³ç´°æ¸¬è©¦çµæœå·²ä¿å­˜è‡³: {results_file}")

    except Exception as e:
        logger.error(f"æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")

if __name__ == "__main__":
    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(main())
