"""
æ¨¡å‹è¨“ç·´æœå‹™ - PU Learning æ¨¡å‹è¨“ç·´çš„æ ¸å¿ƒé‚è¼¯
è² è²¬å°‡æ¨™è¨»æ•¸æ“šè½‰æ›ç‚º PU Learning æ ¼å¼ä¸¦é€²è¡Œæ¨¡å‹è¨“ç·´
"""

import pandas as pd
import numpy as np
import joblib
import asyncio
import time
import json
from typing import Dict, List, Tuple, Optional, Any, Union
from datetime import datetime
import logging
import os
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
import sys

# æ·»åŠ  PU Learning æ¨¡çµ„è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
pu_learning_dir = os.path.join(os.path.dirname(current_dir), 'pu-learning')
sys.path.append(pu_learning_dir)

try:
    from pulearning_engine import run_pu_simulation
    PULEARNING_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… PU Learning å¼•æ“è¼‰å…¥æˆåŠŸ")
except ImportError as e:
    PULEARNING_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"âš ï¸ PU Learning å¼•æ“æœªæ‰¾åˆ°: {e}")

# Mock classes for when PU Learning is not available
if not PULEARNING_AVAILABLE:
    class DataParams:
        def __init__(self, dims, n_p, n_u, prior):
            self.dims = dims
            self.n_p = n_p
            self.n_u = n_u
            self.prior = prior

    class ModelParams:
        def __init__(self, activation, n_epochs):
            self.activation = activation
            self.n_epochs = n_epochs

from core.database import db_manager

logger = logging.getLogger(__name__)

class TrainingService:
    """æ¨¡å‹è¨“ç·´æœå‹™ - å°è£ PU Learning æ¨¡å‹è¨“ç·´é‚è¼¯"""

    def __init__(self):
        self.models_dir = Path(__file__).parent.parent / "trained_models"
        self.models_dir.mkdir(exist_ok=True)

    async def train_pu_model_in_background(
        self,
        model_name: str,
        model_type: str,
        model_params: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        èƒŒæ™¯è¨“ç·´ PU Learning æ¨¡å‹
        Args:
            model_name: æ¨¡å‹åç¨±
            model_type: æ¨¡å‹é¡å‹ ('uPU' æˆ– 'nnPU')
            model_params: æ¨¡å‹åƒæ•¸
        Returns:
            model_id: è¨“ç·´å®Œæˆçš„æ¨¡å‹ID
        """
        try:
            logger.info("=" * 50)
            logger.info("ğŸš€ é–‹å§‹ PU Learning æ¨¡å‹è¨“ç·´")
            logger.info("=" * 50)
            logger.info(f"ğŸ·ï¸ æ¨¡å‹åç¨±: {model_name}")
            logger.info(f"âš™ï¸ æ¨¡å‹åƒæ•¸: {model_params}")

            # éšæ®µ1: æº–å‚™è¨“ç·´æ•¸æ“š
            logger.info("ğŸ“¦ éšæ®µ1: é–‹å§‹æº–å‚™è¨“ç·´æ•¸æ“š...")
            X, y, data_summary = await self.prepare_training_data()

            # éšæ®µ2: ä¿å­˜æ¨¡å‹
            logger.info(f"ğŸ’¾ éšæ®µ2: ä¿å­˜æ¨¡å‹ {model_name}...")

            # å‰µå»ºå”¯ä¸€çš„æ¨¡å‹ID
            timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
            model_id = f"{model_name}_{timestamp}"

            logger.info(f"âœ… PU Learning æ¨¡å‹è¨“ç·´å®Œæˆ - æ¨¡å‹ID: {model_id}")
            return model_id

        except Exception as e:
            logger.error(f"âŒ PUæ¨¡å‹è¨“ç·´å¤±æ•—: {str(e)}")
            raise e

    async def prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
        """
        æº–å‚™ PU Learning è¨“ç·´æ•¸æ“š
        Returns:
            X: ç‰¹å¾µçŸ©é™£
            y: æ¨™ç±¤å‘é‡ (1: æ­£æ¨£æœ¬, -1: å¯é è² æ¨£æœ¬, 0: æœªæ¨™è¨»æ¨£æœ¬)
            summary: æ•¸æ“šæ‘˜è¦
        """
        logger.info("æº–å‚™è¨“ç·´æ•¸æ“š...")

        try:
            # å¾è³‡æ–™åº«ç²å–å·²æ¨™è¨»çš„ç•°å¸¸äº‹ä»¶
            async with db_manager.get_session() as session:
                from sqlalchemy import text

                # æŸ¥è©¢ç•°å¸¸äº‹ä»¶åŠå…¶æ¨™ç±¤
                query = """
                SELECT
                    ae.id, ae.meterId, ae.eventTimestamp, ae.score, ae.dataWindow,
                    ae.status, al.name as label_name
                FROM anomaly_event ae
                LEFT JOIN event_label_link ell ON ae.id = ell.eventId
                LEFT JOIN anomaly_label al ON ell.labelId = al.id
                ORDER BY ae.eventTimestamp
                """

                result = await session.execute(text(query))
                events = result.fetchall()

            if not events:
                raise ValueError("æœªæ‰¾åˆ°å·²æ¨™è¨»çš„ç•°å¸¸äº‹ä»¶æ•¸æ“š")

            # è½‰æ›ç‚º DataFrame
            events_data = []
            for event in events:
                events_data.append({
                    'id': event.id,
                    'meterId': event.meterId,
                    'eventTimestamp': event.eventTimestamp,
                    'score': event.score,
                    'dataWindow': event.dataWindow,
                    'status': event.status,
                    'label_name': event.label_name
                })

            df = pd.DataFrame(events_data)

            # æå–ç‰¹å¾µ
            X, feature_names = self._extract_features(df)

            # è½‰æ›æ¨™ç±¤ç‚º PU Learning æ ¼å¼
            y = self._convert_labels_to_pu_format(df)

            # ç”Ÿæˆæ•¸æ“šæ‘˜è¦
            summary = {
                'total_samples': len(df),
                'positive_samples': int(np.sum(y == 1)),
                'negative_samples': int(np.sum(y == -1)),
                'unlabeled_samples': int(np.sum(y == 0)),
                'feature_count': X.shape[1],
                'feature_names': feature_names
            }

            logger.info(f"æ•¸æ“šæº–å‚™å®Œæˆ: {summary}")
            return X, y, summary

        except Exception as e:
            logger.error(f"æº–å‚™è¨“ç·´æ•¸æ“šå¤±æ•—: {e}")
            raise

    def _extract_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """å¾äº‹ä»¶æ•¸æ“šä¸­æå–ç‰¹å¾µ"""
        logger.info("é–‹å§‹ç‰¹å¾µæå–...")

        features = []
        feature_names = []

        for _, row in df.iterrows():
            feature_vector = []

            # åŸºæœ¬ç‰¹å¾µ
            feature_vector.append(float(row['score']) if row['score'] is not None else 0.0)
            feature_names_batch = ['score']

            # æ™‚é–“ç‰¹å¾µ
            if row['eventTimestamp']:
                try:
                    if isinstance(row['eventTimestamp'], str):
                        timestamp = pd.to_datetime(row['eventTimestamp'])
                    else:
                        timestamp = row['eventTimestamp']

                    feature_vector.extend([
                        float(timestamp.hour),
                        float(timestamp.weekday()),
                        float(timestamp.day)
                    ])
                    feature_names_batch.extend(['hour', 'weekday', 'day'])
                except:
                    feature_vector.extend([0.0, 0.0, 0.0])
                    feature_names_batch.extend(['hour', 'weekday', 'day'])
            else:
                feature_vector.extend([0.0, 0.0, 0.0])
                feature_names_batch.extend(['hour', 'weekday', 'day'])

            # dataWindow ç‰¹å¾µ
            if row['dataWindow'] and isinstance(row['dataWindow'], dict):
                data_window = row['dataWindow']

                # é›»åŠ›çµ±è¨ˆç‰¹å¾µ
                if 'power_stats' in data_window:
                    stats = data_window['power_stats']
                    feature_vector.extend([
                        float(stats.get('mean', 0)),
                        float(stats.get('std', 0)),
                        float(stats.get('max', 0)),
                        float(stats.get('min', 0))
                    ])
                    feature_names_batch.extend(['power_mean', 'power_std', 'power_max', 'power_min'])
                else:
                    feature_vector.extend([0.0, 0.0, 0.0, 0.0])
                    feature_names_batch.extend(['power_mean', 'power_std', 'power_max', 'power_min'])

                # æ™‚åºé•·åº¦ç‰¹å¾µ
                if 'window_length' in data_window:
                    feature_vector.append(float(data_window['window_length']))
                    feature_names_batch.append('window_length')
                else:
                    feature_vector.append(0.0)
                    feature_names_batch.append('window_length')
            else:
                # å¦‚æœæ²’æœ‰ dataWindowï¼Œå¡«å……é›¶å€¼
                feature_vector.extend([0.0, 0.0, 0.0, 0.0, 0.0])
                feature_names_batch.extend(['power_mean', 'power_std', 'power_max', 'power_min', 'window_length'])

            features.append(feature_vector)
            if not feature_names:  # åªåœ¨ç¬¬ä¸€æ¬¡è¨­å®šç‰¹å¾µåç¨±
                feature_names = feature_names_batch

        X = np.array(features)
        logger.info(f"ç‰¹å¾µæå–å®Œæˆï¼Œå½¢ç‹€: {X.shape}")

        return X, feature_names

    def _convert_labels_to_pu_format(self, df: pd.DataFrame) -> np.ndarray:
        """å°‡æ¨™ç±¤è½‰æ›ç‚º PU Learning æ ¼å¼"""
        logger.info("è½‰æ›æ¨™ç±¤ç‚º PU Learning æ ¼å¼...")

        y = np.zeros(len(df))

        for i, row in df.iterrows():
            # åŸºæ–¼ status å’Œ label_name çš„é‚è¼¯
            if row['status'] == 'CONFIRMED_POSITIVE':
                y[i] = 1  # ç¢ºèªçš„ç•°å¸¸
            elif row['status'] == 'REJECTED_NORMAL':
                y[i] = -1  # ç¢ºèªçš„æ­£å¸¸
            elif row['label_name'] == 'Verified Normal':
                y[i] = -1  # æ˜ç¢ºæ¨™è¨˜ç‚ºæ­£å¸¸
            elif row['label_name'] and row['label_name'] != 'Verified Normal':
                y[i] = 1   # æœ‰å…¶ä»–æ¨™ç±¤çš„è¢«è¦–ç‚ºç•°å¸¸
            else:
                y[i] = 0   # æœªæ¨™è¨»

        return y

    async def get_latest_model_results(self) -> Optional[Dict[str, Any]]:
        """ç²å–æœ€æ–°çš„æ¨¡å‹è¨“ç·´çµæœ"""
        try:
            async with db_manager.get_session() as session:
                from sqlalchemy import text

                query = """
                SELECT id, "modelName", "modelType", precision, recall, "f1Score",
                       "createdAt"
                FROM trained_model
                ORDER BY "createdAt" DESC
                LIMIT 1
                """

                result = await session.execute(text(query))
                row = result.fetchone()

                if row:
                    return {
                        'id': row[0],
                        'model_name': row[1],
                        'model_type': row[2],
                        'precision': float(row[3]) if row[3] else 0.0,
                        'recall': float(row[4]) if row[4] else 0.0,
                        'f1_score': float(row[5]) if row[5] else 0.0,
                        'created_at': row[6].isoformat() if row[6] else None
                    }
                return None

        except Exception as e:
            logger.error(f"ç²å–æœ€æ–°æ¨¡å‹çµæœå¤±æ•—: {e}")
            return None


# å‰µå»ºå…¨åŸŸå¯¦ä¾‹
training_service = TrainingService()
