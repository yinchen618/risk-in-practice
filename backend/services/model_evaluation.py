"""
æ¨¡å‹è©•ä¼°æœå‹™ - å°ˆé–€è™•ç† Generalization Challenge ç­‰è·¨åŸŸè©•ä¼°
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set
import numpy as np
import pandas as pd
import joblib
import os
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# WebSocket ç›¸é—œ
from fastapi import WebSocket

logger = logging.getLogger(__name__)

# å…¨åŸŸè®Šé‡ç”¨æ–¼è¿½è¹¤è©•ä¼°ä»»å‹™å’Œ WebSocket é€£æ¥
evaluation_jobs: Dict[str, Dict[str, Any]] = {}
evaluation_websocket_connections: Set[WebSocket] = set()
evaluation_websocket_lock = asyncio.Lock()  # æ·»åŠ ç•°æ­¥é–ä¿è­· WebSocket æ“ä½œ

class ModelEvaluator:
    """æ¨¡å‹è©•ä¼°å™¨"""

    def __init__(self):
        # ç²å–é …ç›®æ ¹ç›®éŒ„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(current_dir)
        self.models_dir = os.path.join(project_root, "trained_models")  # æ¨¡å‹ä¿å­˜ç›®éŒ„
        os.makedirs(self.models_dir, exist_ok=True)

        # åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨
        self.db_manager = None

    async def start_evaluation_job(self, model_id: str, evaluation_request: dict) -> str:
        """
        å•Ÿå‹•ç•°æ­¥è©•ä¼°ä»»å‹™

        Args:
            model_id: è¦è©•ä¼°çš„æ¨¡å‹ID
            evaluation_request: è©•ä¼°è«‹æ±‚é…ç½®

        Returns:
            str: è©•ä¼°ä»»å‹™ID
        """
        job_id = str(uuid.uuid4())

        # å‰µå»ºè©•ä¼°ä»»å‹™è¨˜éŒ„
        evaluation_jobs[job_id] = {
            "id": job_id,
            "model_id": model_id,
            "status": "starting",
            "created_at": datetime.now().isoformat(),
            "progress": 0,
            "error": None,
            "results": None,
            "request": evaluation_request
        }

        # åœ¨èƒŒæ™¯åŸ·è¡Œè©•ä¼°
        asyncio.create_task(self._run_evaluation(job_id, model_id, evaluation_request))

        logger.info(f"ğŸ“Š è©•ä¼°ä»»å‹™å·²å•Ÿå‹•: {job_id} for model {model_id}")
        return job_id

    async def _run_evaluation(self, job_id: str, model_id: str, evaluation_request: dict):
        """
        åŸ·è¡Œå¯¦éš›çš„æ¨¡å‹è©•ä¼°

        Args:
            job_id: è©•ä¼°ä»»å‹™ID
            model_id: æ¨¡å‹ID
            evaluation_request: è©•ä¼°è«‹æ±‚
        """
        try:
            logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œè©•ä¼°ä»»å‹™: {job_id}")

            # æ›´æ–°ç‹€æ…‹ç‚ºé€²è¡Œä¸­
            evaluation_jobs[job_id]["status"] = "running"
            evaluation_jobs[job_id]["progress"] = 10

            # å»£æ’­é€²åº¦æ›´æ–°
            await broadcast_evaluation_progress({
                "type": "prediction_progress",
                "progress": 10,
                "stage": "starting",
                "message": "Starting evaluation...",
                "job_id": job_id
            })

            # 1. è¼‰å…¥æ¨¡å‹
            logger.info(f"ğŸ“¦ è¼‰å…¥æ¨¡å‹: {model_id}")
            # åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨
            if not self.db_manager:
                from database import db_manager
                self.db_manager = db_manager

            model_info = await self._load_model_info(model_id)
            if not model_info:
                raise Exception(f"æ‰¾ä¸åˆ°æ¨¡å‹ {model_id}")

            evaluation_jobs[job_id]["progress"] = 20

            # å»£æ’­é€²åº¦æ›´æ–°
            await broadcast_evaluation_progress({
                "type": "prediction_progress",
                "progress": 20,
                "stage": "loading_model",
                "message": f"Model {model_id} loaded successfully",
                "job_id": job_id
            })

            # 2. æº–å‚™æ¸¬è©¦æ•¸æ“š
            logger.info("ğŸ“Š æº–å‚™æ¸¬è©¦æ•¸æ“š...")
            test_data = await self._prepare_test_data(evaluation_request, job_id)
            if test_data is None or test_data.empty:
                raise Exception("ç„¡æ³•ç²å–æ¸¬è©¦æ•¸æ“šï¼šæŒ‡å®šçš„æ™‚é–“ç¯„åœå’Œå»ºç¯‰æ¨“å±¤å…§æ²’æœ‰å¯ç”¨çš„ç•°å¸¸äº‹ä»¶æ•¸æ“š")

            evaluation_jobs[job_id]["progress"] = 40

            # å»£æ’­é€²åº¦æ›´æ–°
            await broadcast_evaluation_progress({
                "type": "prediction_progress",
                "progress": 40,
                "stage": "preparing_data",
                "message": f"Test data prepared: {len(test_data)} samples",
                "job_id": job_id
            })

            # 3. è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
            logger.info("ğŸ”§ è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹...")
            trained_model = await self._load_trained_model(model_info)
            if trained_model is None:
                raise Exception("ç„¡æ³•è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹")

            evaluation_jobs[job_id]["progress"] = 60

            # å»£æ’­é€²åº¦æ›´æ–°
            await broadcast_evaluation_progress({
                "type": "prediction_progress",
                "progress": 60,
                "stage": "loading_trained_model",
                "message": "Trained model loaded successfully",
                "job_id": job_id
            })

            # 4. é€²è¡Œé æ¸¬ä¸¦å‰µå»º EvaluationRun è¨˜éŒ„
            logger.info("ğŸ¯ åŸ·è¡Œæ¨¡å‹é æ¸¬...")

            # ä½¿ç”¨çµ±ä¸€çš„é æ¸¬å·¥å…·ï¼ˆåŒ…å« EvaluationRun ç®¡ç†ï¼‰
            from services.prediction_utils import ModelPredictor

            # æº–å‚™è©•ä¼°é…ç½®
            evaluation_config = {
                "scenario_type": evaluation_request.get("scenario_type", "GENERALIZATION_CHALLENGE"),
                "name": evaluation_request.get("name", f"External Evaluation {job_id[:8]}"),
                "test_set_source": evaluation_request.get("test_set_source", {})
            }

            # æº–å‚™æ¸¬è©¦ç‰¹å¾µ
            X_test = await ModelPredictor.prepare_test_features(
                test_data,
                job_id=job_id,
                broadcast_progress_func=broadcast_evaluation_progress
            )

            # é€²è¡Œé æ¸¬ä¸¦è‡ªå‹•ç®¡ç† EvaluationRun
            prediction_result = await ModelPredictor.make_predictions_with_evaluation_run(
                model=trained_model,
                X_test=X_test,
                y_test=test_data.get('ground_truth') if 'ground_truth' in test_data.columns else None,
                model_id=model_id,
                evaluation_config=evaluation_config,
                db_manager=self.db_manager,
                job_id=job_id,
                model_info=model_info
            )

            # æå–çµæœ
            predictions = prediction_result["predictions"]
            metrics = prediction_result.get("evaluation_metrics", {})
            evaluation_run = prediction_result.get("evaluation_run")
            evaluation_run_id = prediction_result.get("evaluation_run_id")

            evaluation_jobs[job_id]["progress"] = 80

            # å»£æ’­é€²åº¦æ›´æ–°
            await broadcast_evaluation_progress({
                "type": "prediction_progress",
                "progress": 80,
                "stage": "predicting",
                "message": "Model predictions completed",
                "job_id": job_id
            })

            # 5. è™•ç†è©•ä¼°æŒ‡æ¨™
            logger.info("ğŸ“ˆ è™•ç†è©•ä¼°æŒ‡æ¨™...")

            # å¦‚æœæ²’æœ‰æŒ‡æ¨™ï¼ˆæ²’æœ‰çœŸå¯¦æ¨™ç±¤ï¼‰ï¼Œå‰‡å‰µå»ºåŸºæœ¬çš„é æ¸¬çµ±è¨ˆ
            if not metrics:
                import numpy as np
                metrics = {
                    "prediction_stats": prediction_result["prediction_stats"],
                    "total_predictions": len(predictions),
                    "positive_predictions": int(np.sum(predictions)),
                    "negative_predictions": int(len(predictions) - np.sum(predictions))
                }

            evaluation_jobs[job_id]["progress"] = 100

            # å»£æ’­é€²åº¦æ›´æ–°
            await broadcast_evaluation_progress({
                "type": "prediction_progress",
                "progress": 100,
                "stage": "calculating_metrics",
                "message": "Evaluation metrics calculated",
                "job_id": job_id
            })

            # 6. æ›´æ–°è©•ä¼°çµæœ
            logger.info("ğŸ’¾ æ›´æ–°è©•ä¼°çµæœ...")
            if evaluation_run:
                await self._update_evaluation_results(evaluation_run["id"], metrics)
            else:
                # å¦‚æœæ²’æœ‰å‰µå»º EvaluationRunï¼Œå‰‡å‰µå»ºä¸€å€‹
                await self._save_evaluation_results(job_id, model_id, evaluation_request, metrics)

            # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå®Œæˆ
            evaluation_jobs[job_id]["status"] = "completed"
            evaluation_jobs[job_id]["results"] = metrics

            # å»£æ’­å®Œæˆæ¶ˆæ¯
            await broadcast_evaluation_progress({
                "type": "evaluation_completed",
                "progress": 100,
                "stage": "completed",
                "message": "Evaluation completed successfully",
                "job_id": job_id,
                "metrics": metrics
            })

            logger.info(f"âœ… è©•ä¼°ä»»å‹™å®Œæˆ: {job_id}")
            logger.info(f"ğŸ“Š è©•ä¼°çµæœ: {metrics}")

        except Exception as e:
            logger.error(f"âŒ è©•ä¼°ä»»å‹™å¤±æ•— {job_id}: {str(e)}")
            evaluation_jobs[job_id]["status"] = "failed"
            evaluation_jobs[job_id]["error"] = str(e)
            evaluation_jobs[job_id]["progress"] = 100

            # å»£æ’­å¤±æ•—æ¶ˆæ¯
            await broadcast_evaluation_progress({
                "type": "evaluation_failed",
                "progress": 100,
                "stage": "failed",
                "message": f"Evaluation failed: {str(e)}",
                "job_id": job_id,
                "error": str(e)
            })

    async def _create_evaluation_run_record(
        self,
        job_id: str,
        model_id: str,
        evaluation_request: dict,
        model_info: dict
    ) -> Optional[dict]:
        """å‰µå»º EvaluationRun è¨˜éŒ„ï¼ŒåŒ…å«è©³ç´°çš„ P/U æºè³‡è¨Š"""
        try:
            # å¾ model_info ä¸­æå– data_source_config
            data_source_config = model_info.get("data_source_config", {})
            p_source_config = data_source_config.get("p_source", {})
            u_source_config = data_source_config.get("u_source", {})
            prediction_config = data_source_config.get("prediction_config", {})

            # æ§‹å»ºè©³ç´°çš„æ¸¬è©¦é›†ä¾†æºé…ç½®
            test_set_source = {
                "type": "external_evaluation",
                "evaluation_scenario": evaluation_request.get("scenario_type", "GENERALIZATION_CHALLENGE"),
                "job_id": job_id,
                "original_model_info": {
                    "model_id": model_id,
                    "scenario_type": model_info.get("scenario_type", "ERM_BASELINE"),
                    "experiment_run_id": model_info.get("experiment_run_id")
                },
                "p_source": {
                    "type": p_source_config.get("type", "anomaly_events"),
                    "experiment_run_id": p_source_config.get("experiment_run_id"),
                    "filter_criteria": p_source_config.get("filter_criteria"),
                    "description": p_source_config.get("description", "Positive samples from training data")
                },
                "u_source": {
                    "type": u_source_config.get("type", "anomaly_events"),
                    "time_range": u_source_config.get("time_range"),
                    "building_floors": u_source_config.get("building_floors"),
                    "sample_limit": u_source_config.get("sample_limit"),
                    "experiment_run_id": u_source_config.get("experiment_run_id"),
                    "filter_criteria": u_source_config.get("filter_criteria"),
                    "description": u_source_config.get("description", "Unlabeled samples from training data")
                },
                "prediction_period": {
                    "start_date": prediction_config.get("start_date"),
                    "end_date": prediction_config.get("end_date")
                },
                "evaluation_config": evaluation_request.get("test_set_source", {}),
                "timestamp": datetime.now().isoformat()
            }

            # å‰µå»º EvaluationRun è¨˜éŒ„
            evaluation_run = await self.db_manager.create_evaluation_run(
                name=evaluation_request.get("name", f"External Evaluation {job_id[:8]}"),
                scenario_type=evaluation_request.get("scenario_type", "GENERALIZATION_CHALLENGE"),
                trained_model_id=model_id,
                test_set_source=test_set_source
            )

            logger.info(f"âœ… å‰µå»º EvaluationRun è¨˜éŒ„: {evaluation_run['id']}")
            logger.info(f"ğŸ“Š Pæºé¡å‹: {p_source_config.get('type')}, Uæºé¡å‹: {u_source_config.get('type')}")

            return evaluation_run

        except Exception as e:
            logger.error(f"âŒ å‰µå»º EvaluationRun è¨˜éŒ„å¤±æ•—: {e}")
            return None

    async def _load_model_info(self, model_id: str) -> Dict[str, Any]:
        """è¼‰å…¥æ¨¡å‹ä¿¡æ¯"""
        try:
            from database import db_manager
            model_info = await db_manager.get_trained_model_by_id(model_id)
            return model_info
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¨¡å‹ä¿¡æ¯å¤±æ•—: {e}")
            return None

    async def _prepare_test_data(self, evaluation_request: dict, job_id: str = None) -> pd.DataFrame:
        """
        æº–å‚™æ¸¬è©¦æ•¸æ“š

        Args:
            evaluation_request: è©•ä¼°è«‹æ±‚ï¼ŒåŒ…å« test_set_source é…ç½®

        Returns:
            pd.DataFrame: æ¸¬è©¦æ•¸æ“š
        """
        try:
            from database import db_manager

            test_source = evaluation_request.get("test_set_source", {})

            # å¾ test_set_source ä¸­æå–é…ç½®
            selected_floors_by_building = test_source.get("selectedFloorsByBuilding", {})
            time_range = test_source.get("timeRange_detail", {})

            if not selected_floors_by_building or not time_range:
                logger.error("æ¸¬è©¦é›†é…ç½®ä¸å®Œæ•´")
                return None

            start_date = time_range.get("startDate")
            end_date = time_range.get("endDate")
            start_time = time_range.get("startTime", "00:00")
            end_time = time_range.get("endTime", "23:59")

            logger.info(f"ğŸ¯ æ¸¬è©¦æ•¸æ“šç¯„åœ:")
            logger.info(f"   ğŸ“… æ—¥æœŸ: {start_date} åˆ° {end_date}")
            logger.info(f"   â° æ™‚é–“: {start_time} åˆ° {end_time}")
            logger.info(f"   ğŸ¢ å»ºç¯‰æ¨“å±¤: {selected_floors_by_building}")

            # ç²å–æ¸¬è©¦æ•¸æ“š - é€™è£¡ä½¿ç”¨å¯¦éš›çš„ç•°å¸¸äº‹ä»¶æ•¸æ“šä½œç‚ºæ¸¬è©¦é›†
            test_data = await db_manager.get_anomaly_events_for_evaluation(
                selected_floors_by_building=selected_floors_by_building,
                start_date=start_date,
                end_date=end_date,
                start_time=start_time,
                end_time=end_time
            )

            if test_data is not None and not test_data.empty:
                logger.info(f"âœ… æˆåŠŸç²å–æ¸¬è©¦æ•¸æ“š: {len(test_data)} æ¢è¨˜éŒ„")

                # å»£æ’­æ¸¬è©¦æ•¸æ“šæº–å‚™å®Œæˆ
                await broadcast_evaluation_progress({
                    "type": "prediction_progress",
                    "progress": 30,
                    "stage": "test_data_ready",
                    "message": f"Test data prepared: {len(test_data)} samples found",
                    "job_id": job_id
                })

                return test_data
            else:
                logger.error("âŒ æŒ‡å®šçš„æ™‚é–“ç¯„åœå’Œå»ºç¯‰æ¨“å±¤å…§æ²’æœ‰å¯ç”¨çš„æ¸¬è©¦æ•¸æ“š")

                # å»£æ’­æ•¸æ“šä¸è¶³éŒ¯èª¤
                await broadcast_evaluation_progress({
                    "type": "evaluation_failed",
                    "progress": 30,
                    "stage": "no_test_data",
                    "message": "No test data found in the specified time range and building floors",
                    "error": "No test data available"
                })

                return None

        except Exception as e:
            logger.error(f"æº–å‚™æ¸¬è©¦æ•¸æ“šå¤±æ•—: {e}")
            return None

    async def _load_trained_model(self, model_info: dict):
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
        try:
            model_path = model_info.get("model_path")
            if not model_path or not os.path.exists(model_path):
                logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return None

            # è¼‰å…¥æ¨¡å‹æ•¸æ“š
            model_data = joblib.load(model_path)
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹æ•¸æ“š: {model_path}")

            # æª¢æŸ¥æ¨¡å‹æ•¸æ“šæ ¼å¼
            if isinstance(model_data, dict) and "model" in model_data:
                # æ–°æ ¼å¼ï¼šæ¨¡å‹ä¿å­˜ç‚ºå­—å…¸ï¼Œå¯¦éš›æ¨¡å‹åœ¨ "model" éµä¸‹
                model = model_data["model"]
                logger.info(f"ğŸ”§ æå–æ¨¡å‹å°è±¡ï¼Œé¡å‹: {type(model)}")
            else:
                # èˆŠæ ¼å¼ï¼šç›´æ¥ä¿å­˜çš„æ¨¡å‹å°è±¡
                model = model_data
                logger.info(f"ğŸ”§ ç›´æ¥è¼‰å…¥æ¨¡å‹å°è±¡ï¼Œé¡å‹: {type(model)}")

            return model

        except Exception as e:
            logger.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            return None

    async def _make_predictions(self, model, test_data: pd.DataFrame, job_id: str = None) -> np.ndarray:
        """ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬"""
        try:
            from services.feature_engineering import feature_engineering

            logger.info(f"ğŸ”§ æ¸¬è©¦æ•¸æ“šæ ¼å¼: columns={test_data.columns.tolist()}, shape={test_data.shape}")

            # å»£æ’­æ•¸æ“šæº–å‚™éšæ®µ
            await broadcast_evaluation_progress({
                "type": "prediction_progress",
                "progress": 75,
                "stage": "data_preprocessing",
                "message": f"æ­£åœ¨è™•ç† {len(test_data)} æ¢æ¸¬è©¦æ•¸æ“š",
                "job_id": job_id,
                "details": {
                    "samples_count": len(test_data),
                    "features_available": test_data.columns.tolist()
                }
            })

            # æª¢æŸ¥æ¸¬è©¦æ•¸æ“šçš„æ ¼å¼
            if 'dataWindow' in test_data.columns:
                # å¦‚æœæ˜¯ç•°å¸¸äº‹ä»¶æ ¼å¼ï¼Œç›´æ¥è½‰æ›
                test_events = test_data.to_dict('records')
                logger.info(f"ğŸ”§ ä½¿ç”¨ç•°å¸¸äº‹ä»¶æ ¼å¼ï¼Œè½‰æ›ç‚º {len(test_events)} å€‹äº‹ä»¶")

                # å»£æ’­ç‰¹å¾µå·¥ç¨‹é€²åº¦
                await broadcast_evaluation_progress({
                    "type": "prediction_detail",
                    "progress": 78,
                    "stage": "feature_engineering",
                    "message": "æ­£åœ¨å¾ç•°å¸¸äº‹ä»¶æ•¸æ“šç”Ÿæˆç‰¹å¾µ",
                    "details": {
                        "data_format": "anomaly_events",
                        "events_count": len(test_events)
                    }
                })

                # ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾µå·¥ç¨‹æ–¹æ³•ç”Ÿæˆç‰¹å¾µçŸ©é™£
                feature_matrix, _ = feature_engineering.generate_feature_matrix(test_events)
            else:
                # å¦‚æœæ˜¯æ¨¡æ“¬ç‰¹å¾µæ•¸æ“šï¼Œç›´æ¥æ§‹å»ºç‰¹å¾µçŸ©é™£
                logger.info("ğŸ”§ æª¢æ¸¬åˆ°çœŸå¯¦ç‰¹å¾µæ•¸æ“šæ ¼å¼ï¼Œç›´æ¥æ§‹å»ºç‰¹å¾µçŸ©é™£")

                # å»£æ’­ç‰¹å¾µè™•ç†é€²åº¦
                await broadcast_evaluation_progress({
                    "type": "prediction_detail",
                    "progress": 78,
                    "stage": "feature_engineering",
                    "message": "æ­£åœ¨è™•ç†çœŸå¯¦é›»è¡¨æ•¸æ“šç‰¹å¾µ",
                    "details": {
                        "data_format": "real_meter_data",
                        "samples_count": len(test_data),
                        "meters_count": test_data['meter_id'].nunique() if 'meter_id' in test_data.columns else "æœªçŸ¥"
                    }
                })

                feature_columns = [
                    'power_consumption', 'power_variance', 'power_trend', 'power_max', 'power_min',
                    'power_std', 'power_range', 'hour_of_day', 'day_of_week', 'is_weekend'
                ]

                # ç¢ºä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
                available_features = []
                for col in feature_columns:
                    if col in test_data.columns:
                        available_features.append(test_data[col].values)
                    else:
                        logger.warning(f"ç¼ºå°‘ç‰¹å¾µåˆ—: {col}ï¼Œä½¿ç”¨é»˜èªå€¼0")
                        available_features.append(np.zeros(len(test_data)))

                # æ·»åŠ é¡å¤–çš„ç‰¹å¾µä»¥åŒ¹é…è¨“ç·´æ™‚çš„17ç¶­ç‰¹å¾µå‘é‡
                # è£œé½Šåˆ°17ç¶­ç‰¹å¾µ
                while len(available_features) < 17:
                    available_features.append(np.zeros(len(test_data)))

                # æ§‹å»ºç‰¹å¾µçŸ©é™£
                feature_matrix = np.column_stack(available_features[:17])
                logger.info(f"ğŸ”§ æ§‹å»ºç‰¹å¾µçŸ©é™£: shape={feature_matrix.shape}")

            logger.info(f"ğŸ”§ ç‰¹å¾µçŸ©é™£ç”Ÿæˆå®Œæˆ: shape={feature_matrix.shape}")

            # å»£æ’­ç‰¹å¾µæ¨™æº–åŒ–é€²åº¦
            await broadcast_evaluation_progress({
                "type": "prediction_detail",
                "progress": 82,
                "stage": "feature_normalization",
                "message": "æ­£åœ¨æ¨™æº–åŒ–ç‰¹å¾µæ•¸æ“š",
                "details": {
                    "feature_matrix_shape": feature_matrix.shape,
                    "features_dimension": feature_matrix.shape[1]
                }
            })

            # æ¨™æº–åŒ–ç‰¹å¾µï¼ˆä½¿ç”¨è¨“ç·´æ™‚çš„æ¨™æº–åŒ–åƒæ•¸ï¼‰
            X_test = feature_engineering.transform_features(feature_matrix)

            # è™•ç† NaN å€¼
            if np.isnan(X_test).any():
                logger.warning("âš ï¸ Found NaN values in test feature matrix, filling with 0")
                X_test = np.nan_to_num(X_test, nan=0.0)

            # å»£æ’­æ¨¡å‹é æ¸¬é€²åº¦
            await broadcast_evaluation_progress({
                "type": "prediction_detail",
                "progress": 85,
                "stage": "model_inference",
                "message": "æ­£åœ¨åŸ·è¡Œæ¨¡å‹æ¨ç†",
                "details": {
                    "input_shape": X_test.shape,
                    "model_type": str(type(model).__name__)
                }
            })

            # é€²è¡Œé æ¸¬
            if hasattr(model, 'predict_proba'):
                # å¦‚æœæœ‰ predict_proba æ–¹æ³•ï¼Œç²å–æ­£é¡æ¦‚ç‡
                probabilities = model.predict_proba(X_test)
                predictions = (probabilities[:, 1] > 0.5).astype(int)

                # å»£æ’­é æ¸¬å®Œæˆè©³æƒ…
                await broadcast_evaluation_progress({
                    "type": "prediction_detail",
                    "progress": 88,
                    "stage": "prediction_completed",
                    "message": "æ¨¡å‹é æ¸¬å®Œæˆï¼Œæ­£åœ¨åˆ†æçµæœ",
                    "details": {
                        "prediction_method": "probability_based",
                        "positive_predictions": int(np.sum(predictions)),
                        "negative_predictions": int(len(predictions) - np.sum(predictions)),
                        "prediction_distribution": np.bincount(predictions).tolist()
                    }
                })
            else:
                # å¦å‰‡ç›´æ¥é æ¸¬
                predictions = model.predict(X_test)

                # å»£æ’­é æ¸¬å®Œæˆè©³æƒ…
                await broadcast_evaluation_progress({
                    "type": "prediction_detail",
                    "progress": 88,
                    "stage": "prediction_completed",
                    "message": "æ¨¡å‹é æ¸¬å®Œæˆï¼Œæ­£åœ¨åˆ†æçµæœ",
                    "details": {
                        "prediction_method": "direct_prediction",
                        "positive_predictions": int(np.sum(predictions)),
                        "negative_predictions": int(len(predictions) - np.sum(predictions)),
                        "prediction_distribution": np.bincount(predictions).tolist()
                    }
                })

            logger.info(f"âœ… é æ¸¬å®Œæˆï¼Œé æ¸¬çµæœåˆ†ä½ˆ: {np.bincount(predictions)}")
            return predictions

        except Exception as e:
            logger.error(f"é æ¸¬å¤±æ•—: {e}")

            # å»£æ’­éŒ¯èª¤ä¿¡æ¯
            await broadcast_evaluation_progress({
                "type": "prediction_error",
                "progress": 0,
                "stage": "prediction_failed",
                "message": f"é æ¸¬éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "details": {
                    "error_type": type(e).__name__,
                    "error_message": str(e)
                }
            })

            raise Exception(f"æ¨¡å‹é æ¸¬å¤±æ•—: {str(e)}")

    async def _calculate_metrics(self, test_data: pd.DataFrame, predictions: np.ndarray) -> Dict[str, float]:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        try:
            # å»£æ’­æŒ‡æ¨™è¨ˆç®—é–‹å§‹
            await broadcast_evaluation_progress({
                "type": "metrics_calculation",
                "progress": 90,
                "stage": "metrics_analysis",
                "message": "æ­£åœ¨åˆ†æé æ¸¬çµæœä¸¦è¨ˆç®—è©•ä¼°æŒ‡æ¨™",
                "details": {
                    "total_predictions": len(predictions),
                    "positive_predictions": int(np.sum(predictions)),
                    "negative_predictions": int(len(predictions) - np.sum(predictions))
                }
            })

            # æª¢æŸ¥æ˜¯å¦æœ‰çœŸå¯¦æ¨™ç±¤
            if 'is_anomaly' in test_data.columns:
                y_true = test_data['is_anomaly'].values

                # å»£æ’­çœŸå¯¦æ¨™ç±¤å°æ¯”
                await broadcast_evaluation_progress({
                    "type": "metrics_calculation",
                    "progress": 92,
                    "stage": "label_comparison",
                    "message": "ç™¼ç¾çœŸå¯¦æ¨™ç±¤ï¼Œæ­£åœ¨è¨ˆç®—ç²¾ç¢ºæŒ‡æ¨™",
                    "details": {
                        "has_true_labels": True,
                        "true_anomalies": int(np.sum(y_true)),
                        "true_normals": int(len(y_true) - np.sum(y_true))
                    }
                })

                # è¨ˆç®—å„ç¨®æŒ‡æ¨™
                accuracy = accuracy_score(y_true, predictions)
                precision = precision_score(y_true, predictions, zero_division=0)
                recall = recall_score(y_true, predictions, zero_division=0)
                f1 = f1_score(y_true, predictions, zero_division=0)

                # è¨ˆç®—æ··æ·†çŸ©é™£å…ƒç´ 
                tp = int(np.sum((y_true == 1) & (predictions == 1)))
                fp = int(np.sum((y_true == 0) & (predictions == 1)))
                fn = int(np.sum((y_true == 1) & (predictions == 0)))
                tn = int(np.sum((y_true == 0) & (predictions == 0)))

                metrics = {
                    "test_accuracy": float(accuracy),
                    "test_precision": float(precision),
                    "test_recall": float(recall),
                    "test_f1": float(f1),
                    "test_samples": len(test_data),
                    "positive_predictions": int(np.sum(predictions)),
                    "true_positives": tp,
                    "false_positives": fp,
                    "false_negatives": fn,
                    "true_negatives": tn
                }

                # å»£æ’­å®Œæ•´æŒ‡æ¨™çµæœ
                await broadcast_evaluation_progress({
                    "type": "metrics_calculated",
                    "progress": 95,
                    "stage": "metrics_completed",
                    "message": "è©•ä¼°æŒ‡æ¨™è¨ˆç®—å®Œæˆ",
                    "details": {
                        "has_true_labels": True,
                        "accuracy": float(accuracy),
                        "precision": float(precision),
                        "recall": float(recall),
                        "f1_score": float(f1),
                        "confusion_matrix": {
                            "true_positives": tp,
                            "false_positives": fp,
                            "false_negatives": fn,
                            "true_negatives": tn
                        }
                    }
                })

            else:
                # å°æ–¼æ¨¡æ“¬æ•¸æ“šï¼Œæ²’æœ‰çœŸå¯¦æ¨™ç±¤ï¼Œåªå ±å‘Šé æ¸¬çµ±è¨ˆ
                logger.warning("âš ï¸ æ¸¬è©¦æ•¸æ“šä¸­æ²’æœ‰çœŸå¯¦æ¨™ç±¤ 'is_anomaly'ï¼Œä½¿ç”¨æ¨¡æ“¬è©•ä¼°")

                # å»£æ’­æ¨¡æ“¬è©•ä¼°ä¿¡æ¯
                await broadcast_evaluation_progress({
                    "type": "metrics_calculation",
                    "progress": 92,
                    "stage": "simulated_evaluation",
                    "message": "ç„¡çœŸå¯¦æ¨™ç±¤ï¼Œæ­£åœ¨ç”Ÿæˆæ¨¡æ“¬è©•ä¼°æŒ‡æ¨™",
                    "details": {
                        "has_true_labels": False,
                        "evaluation_type": "simulated",
                        "data_source": "real_meter_data"
                    }
                })

                # ç‚ºäº†æ¼”ç¤ºç›®çš„ï¼Œç”Ÿæˆä¸€äº›æ¨¡æ“¬çœŸå¯¦æ¨™ç±¤ï¼ˆåŸºæ–¼åŠŸç‡ç•°å¸¸ï¼‰
                if 'power_consumption' in test_data.columns:
                    # ç°¡å–®è¦å‰‡ï¼šåŠŸç‡æ¶ˆè€—è¶…éå‡å€¼+2å€æ¨™æº–å·®çš„è¦–ç‚ºç•°å¸¸
                    power_mean = test_data['power_consumption'].mean()
                    power_std = test_data['power_consumption'].std()
                    threshold = power_mean + 2 * power_std
                    simulated_labels = (test_data['power_consumption'] > threshold).astype(int)

                    logger.info(f"ğŸ”§ ä½¿ç”¨åŠŸç‡é–¾å€¼ {threshold:.2f} ç”Ÿæˆæ¨¡æ“¬æ¨™ç±¤")
                else:
                    # éš¨æ©Ÿç”Ÿæˆä¸€äº›ç•°å¸¸æ¨™ç±¤ï¼ˆç´„8%çš„ç•°å¸¸ç‡ï¼‰
                    anomaly_rate = 0.08
                    num_anomalies = int(len(predictions) * anomaly_rate)
                    simulated_labels = np.zeros(len(predictions))
                    anomaly_indices = np.random.choice(len(predictions), num_anomalies, replace=False)
                    simulated_labels[anomaly_indices] = 1

                    logger.info(f"ğŸ”§ éš¨æ©Ÿç”Ÿæˆ {num_anomalies} å€‹æ¨¡æ“¬ç•°å¸¸æ¨™ç±¤")

                simulated_labels = simulated_labels.astype(int)

                # è¨ˆç®—æ¨¡æ“¬æŒ‡æ¨™
                accuracy = accuracy_score(simulated_labels, predictions)
                precision = precision_score(simulated_labels, predictions, zero_division=0)
                recall = recall_score(simulated_labels, predictions, zero_division=0)
                f1 = f1_score(simulated_labels, predictions, zero_division=0)

                # è¨ˆç®—æ··æ·†çŸ©é™£å…ƒç´ 
                tp = int(np.sum((simulated_labels == 1) & (predictions == 1)))
                fp = int(np.sum((simulated_labels == 0) & (predictions == 1)))
                fn = int(np.sum((simulated_labels == 1) & (predictions == 0)))
                tn = int(np.sum((simulated_labels == 0) & (predictions == 0)))

                metrics = {
                    "test_accuracy": float(accuracy),
                    "test_precision": float(precision),
                    "test_recall": float(recall),
                    "test_f1": float(f1),
                    "test_samples": len(test_data),
                    "positive_predictions": int(np.sum(predictions)),
                    "simulated_labels": True,
                    "true_positives": tp,
                    "false_positives": fp,
                    "false_negatives": fn,
                    "true_negatives": tn
                }

                # å»£æ’­æ¨¡æ“¬æŒ‡æ¨™çµæœ
                await broadcast_evaluation_progress({
                    "type": "metrics_calculated",
                    "progress": 95,
                    "stage": "simulated_metrics_completed",
                    "message": "æ¨¡æ“¬è©•ä¼°æŒ‡æ¨™è¨ˆç®—å®Œæˆ",
                    "details": {
                        "has_true_labels": False,
                        "evaluation_type": "simulated",
                        "simulated_accuracy": float(accuracy),
                        "simulated_precision": float(precision),
                        "simulated_recall": float(recall),
                        "simulated_f1": float(f1),
                        "confusion_matrix": {
                            "true_positives": tp,
                            "false_positives": fp,
                            "false_negatives": fn,
                            "true_negatives": tn
                        }
                    }
                })

            return metrics

        except Exception as e:
            logger.error(f"è¨ˆç®—æŒ‡æ¨™å¤±æ•—: {e}")

            # å»£æ’­æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤
            await broadcast_evaluation_progress({
                "type": "metrics_error",
                "progress": 0,
                "stage": "metrics_failed",
                "message": f"æŒ‡æ¨™è¨ˆç®—ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "details": {
                    "error_type": type(e).__name__,
                    "error_message": str(e)
                }
            })

            return {
                "test_accuracy": 0.0,
                "test_precision": 0.0,
                "test_recall": 0.0,
                "test_f1": 0.0,
                "test_samples": len(test_data),
                "error": str(e)
            }

    async def _save_evaluation_results(self, job_id: str, model_id: str, evaluation_request: dict, metrics: dict):
        """ä¿å­˜è©•ä¼°çµæœåˆ°æ•¸æ“šåº«"""
        try:
            from database import db_manager

            # å‰µå»ºè©•ä¼°è¨˜éŒ„
            await db_manager.create_evaluation_run(
                name=evaluation_request.get("name", f"Evaluation {job_id}"),
                trained_model_id=model_id,
                scenario_type=evaluation_request.get("scenario_type", "GENERALIZATION_CHALLENGE"),
                test_set_source=evaluation_request.get("test_set_source", {}),
                evaluation_metrics=metrics
            )

            logger.info(f"âœ… è©•ä¼°çµæœå·²ä¿å­˜: {job_id}")

        except Exception as e:
            logger.error(f"ä¿å­˜è©•ä¼°çµæœå¤±æ•—: {e}")

    async def _update_evaluation_results(self, evaluation_run_id: str, metrics: dict):
        """æ›´æ–°å·²å­˜åœ¨çš„ EvaluationRun è¨˜éŒ„"""
        try:
            await self.db_manager.update_evaluation_run(
                evaluation_run_id,
                status="COMPLETED",
                evaluation_metrics=metrics
            )
            logger.info(f"âœ… è©•ä¼°çµæœå·²æ›´æ–°: {evaluation_run_id}")
        except Exception as e:
            logger.error(f"æ›´æ–°è©•ä¼°çµæœå¤±æ•—: {e}")

    def get_evaluation_status(self, job_id: str) -> Dict[str, Any]:
        """ç²å–è©•ä¼°ä»»å‹™ç‹€æ…‹"""
        return evaluation_jobs.get(job_id, {
            "status": "not_found",
            "error": "Evaluation job not found"
        })

# å‰µå»ºå…¨åŸŸè©•ä¼°å™¨å¯¦ä¾‹
evaluator = ModelEvaluator()

# ========================
# WebSocket ç®¡ç†å‡½æ•¸
# ========================

async def broadcast_evaluation_progress(message: dict):
    """
    å‘æ‰€æœ‰é€£æ¥çš„ WebSocket å®¢æˆ¶ç«¯å»£æ’­è©•ä¼°é€²åº¦

    Args:
        message: è¦å»£æ’­çš„æ¶ˆæ¯å­—å…¸
    """
    if not evaluation_websocket_connections:
        logger.debug("ğŸ“¡ No WebSocket connections to broadcast to")
        return

    async with evaluation_websocket_lock:
        logger.info(f"ğŸ”Œ WebSocket Connections: {len(evaluation_websocket_connections)}")
        logger.info(f"ğŸ“¡ Broadcasting evaluation message: {message}")

        # å‰µå»ºé€£æ¥å‰¯æœ¬ä»¥é¿å…åœ¨è¿­ä»£æœŸé–“ä¿®æ”¹é›†åˆ
        connections_copy = list(evaluation_websocket_connections)

        for ws in connections_copy:
            try:
                # æª¢æŸ¥é€£æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                if ws.client_state.name == "DISCONNECTED":
                    logger.warning("ğŸ”Œ WebSocket already disconnected, removing...")
                    evaluation_websocket_connections.discard(ws)
                    continue

                await ws.send_text(json.dumps(message))
                logger.debug(f"âœ… Successfully sent message to WebSocket: {id(ws)}")

            except Exception as e:
                logger.error(f"âŒ Failed to send WebSocket message to {id(ws)}: {e}")
                # ç§»é™¤å¤±æ•ˆçš„é€£æ¥
                try:
                    evaluation_websocket_connections.discard(ws)
                    logger.info(f"ğŸ—‘ï¸ Removed invalid WebSocket connection: {id(ws)}")
                except Exception as cleanup_error:
                    logger.error(f"âŒ Error cleaning up WebSocket: {cleanup_error}")

async def add_evaluation_websocket_connection(websocket: WebSocket):
    """æ·»åŠ è©•ä¼° WebSocket é€£æ¥"""
    async with evaluation_websocket_lock:
        evaluation_websocket_connections.add(websocket)
        logger.info(f"âœ… Added evaluation WebSocket connection: {id(websocket)}")

    logger.info(f"ğŸ“Š Total evaluation connections: {len(evaluation_websocket_connections)}")

async def remove_evaluation_websocket_connection(websocket: WebSocket):
    """ç§»é™¤è©•ä¼° WebSocket é€£æ¥"""
    async with evaluation_websocket_lock:
        evaluation_websocket_connections.discard(websocket)
        logger.info(f"ğŸ—‘ï¸ Removed evaluation WebSocket connection: {id(websocket)}")

    logger.info(f"ğŸ“Š Remaining evaluation connections: {len(evaluation_websocket_connections)}")

# å°å‡ºä¾›å¤–éƒ¨ä½¿ç”¨çš„å‡½æ•¸
__all__ = [
    'evaluator',
    'evaluation_jobs',
    'add_evaluation_websocket_connection',
    'remove_evaluation_websocket_connection',
    'broadcast_evaluation_progress'
]
