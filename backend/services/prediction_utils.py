"""
é æ¸¬å·¥å…·æ¨¡çµ„ - æä¾›çµ±ä¸€çš„æ¨¡å‹é æ¸¬åŠŸèƒ½
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)

class ModelPredictor:
    """çµ±ä¸€çš„æ¨¡å‹é æ¸¬å™¨"""

    @staticmethod
    async def make_predictions_with_evaluation_run(
        model: Any,
        X_test: np.ndarray,
        y_test: Optional[np.ndarray] = None,
        model_id: str = None,
        evaluation_config: Optional[Dict[str, Any]] = None,
        db_manager = None,
        job_id: Optional[str] = None,
        model_info: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬ä¸¦è‡ªå‹•ç®¡ç† EvaluationRun è¨˜éŒ„

        Args:
            model: è¨“ç·´å¥½çš„æ¨¡å‹
            X_test: æ¸¬è©¦ç‰¹å¾µçŸ©é™£
            y_test: æ¸¬è©¦æ¨™ç±¤ï¼ˆå¯é¸ï¼Œå¦‚æœæä¾›å‰‡è¨ˆç®—æŒ‡æ¨™ï¼‰
            model_id: æ¨¡å‹ID
            evaluation_config: è©•ä¼°é…ç½®
            db_manager: è³‡æ–™åº«ç®¡ç†å™¨
            job_id: ä½œæ¥­IDï¼ˆå¯é¸ï¼‰
            model_info: æ¨¡å‹è³‡è¨Šï¼ˆå¯é¸ï¼‰

        Returns:
            Dict: åŒ…å«é æ¸¬çµæœã€æŒ‡æ¨™å’Œ evaluation_run_id çš„å­—å…¸
        """
        evaluation_run = None
        evaluation_run_id = None

        try:
            logger.info(f"ğŸ”® é–‹å§‹æ¨¡å‹é æ¸¬å’Œè©•ä¼°è¨˜éŒ„ï¼Œæ¸¬è©¦æ¨£æœ¬æ•¸: {len(X_test)}")

            # 1. å‰µå»º EvaluationRun è¨˜éŒ„
            if model_id and db_manager and evaluation_config:
                evaluation_run = await ModelPredictor._create_evaluation_run(
                    model_id=model_id,
                    evaluation_config=evaluation_config,
                    model_info=model_info,
                    job_id=job_id,
                    db_manager=db_manager
                )
                evaluation_run_id = evaluation_run["id"] if evaluation_run else None
                logger.info(f"âœ… å‰µå»º EvaluationRun è¨˜éŒ„: {evaluation_run_id}")

            # 2. é€²è¡Œé æ¸¬
            prediction_result = await ModelPredictor.make_predictions_with_metrics(
                model=model,
                X_test=X_test,
                y_test=y_test,
                evaluation_run_id=evaluation_run_id,
                db_manager=db_manager
            )

            # 3. æ›´æ–° EvaluationRun ç‹€æ…‹
            if evaluation_run and db_manager:
                metrics = prediction_result.get("evaluation_metrics", {})
                try:
                    await db_manager.update_evaluation_run(
                        evaluation_run_id,
                        status="COMPLETED",
                        evaluation_metrics=metrics
                    )
                    logger.info(f"âœ… æ›´æ–° EvaluationRun {evaluation_run_id} å®Œæˆ")
                except Exception as e:
                    logger.error(f"âŒ æ›´æ–° EvaluationRun å¤±æ•—: {e}")
                    try:
                        await db_manager.update_evaluation_run(evaluation_run_id, status="FAILED")
                    except:
                        pass

            # æ·»åŠ  evaluation_run_id åˆ°çµæœä¸­
            prediction_result["evaluation_run_id"] = evaluation_run_id
            prediction_result["evaluation_run"] = evaluation_run

            return prediction_result

        except Exception as e:
            logger.error(f"âŒ é æ¸¬å’Œè©•ä¼°è¨˜éŒ„å¤±æ•—: {e}")

            # å¦‚æœå¤±æ•—ï¼Œæ¨™è¨˜ EvaluationRun ç‚ºå¤±æ•—
            if evaluation_run and db_manager:
                try:
                    await db_manager.update_evaluation_run(evaluation_run_id, status="FAILED")
                except:
                    pass

            raise e

    @staticmethod
    async def _create_evaluation_run(
        model_id: str,
        evaluation_config: Dict[str, Any],
        model_info: Optional[Dict[str, Any]],
        job_id: Optional[str],
        db_manager
    ) -> Optional[Dict[str, Any]]:
        """å‰µå»º EvaluationRun è¨˜éŒ„"""
        try:
            # ç¢ºå®šè©•ä¼°å ´æ™¯é¡å‹
            scenario_type = evaluation_config.get("scenario_type", "ERM_BASELINE")

            # ç¢ºå®šè©•ä¼°åç¨±
            if "name" in evaluation_config:
                eval_name = evaluation_config["name"]
            elif job_id:
                eval_name = f"External Evaluation {job_id[:8]}"
            else:
                eval_name = f"Test Set Evaluation - {model_info.get('name', 'Unknown Model') if model_info else 'Unknown Model'}"

            # æ§‹å»º test_set_source
            test_set_source = ModelPredictor._build_test_set_source(
                evaluation_config, model_info, job_id
            )

            # å‰µå»º EvaluationRun è¨˜éŒ„
            evaluation_run = await db_manager.create_evaluation_run(
                name=eval_name,
                scenario_type=scenario_type,
                trained_model_id=model_id,
                test_set_source=test_set_source
            )

            return evaluation_run

        except Exception as e:
            logger.error(f"âŒ å‰µå»º EvaluationRun è¨˜éŒ„å¤±æ•—: {e}")
            return None

    @staticmethod
    def _build_test_set_source(
        evaluation_config: Dict[str, Any],
        model_info: Optional[Dict[str, Any]],
        job_id: Optional[str]
    ) -> Dict[str, Any]:
        """æ§‹å»º test_set_source é…ç½®"""

        # åŸºæœ¬é…ç½®
        test_set_source = {
            "timestamp": datetime.now().isoformat()
        }

        # å¦‚æœæ˜¯å¤–éƒ¨è©•ä¼°ï¼ˆé€šé /evaluate APIï¼‰
        if job_id:
            test_set_source.update({
                "type": "external_evaluation",
                "evaluation_scenario": evaluation_config.get("scenario_type", "GENERALIZATION_CHALLENGE"),
                "job_id": job_id,
                "evaluation_config": evaluation_config.get("test_set_source", {}),
                "original_model_info": {
                    "model_id": model_info.get("id") if model_info else None,
                    "scenario_type": model_info.get("scenario_type") if model_info else None,
                    "experiment_run_id": model_info.get("experiment_run_id") if model_info else None
                }
            })

            # å¦‚æœæœ‰æ¨¡å‹è³‡è¨Šï¼ŒåŠ å…¥è©³ç´°çš„ P/U æºè³‡è¨Š
            if model_info:
                data_source_config = model_info.get("data_source_config", {})
                test_set_source.update({
                    "p_source": {
                        "type": data_source_config.get("p_source", {}).get("type", "anomaly_events"),
                        "experiment_run_id": data_source_config.get("p_source", {}).get("experiment_run_id"),
                        "filter_criteria": data_source_config.get("p_source", {}).get("filter_criteria"),
                        "description": data_source_config.get("p_source", {}).get("description", "Positive samples from training data")
                    },
                    "u_source": {
                        "type": data_source_config.get("u_source", {}).get("type", "anomaly_events"),
                        "time_range": data_source_config.get("u_source", {}).get("time_range"),
                        "building_floors": data_source_config.get("u_source", {}).get("building_floors"),
                        "sample_limit": data_source_config.get("u_source", {}).get("sample_limit"),
                        "experiment_run_id": data_source_config.get("u_source", {}).get("experiment_run_id"),
                        "filter_criteria": data_source_config.get("u_source", {}).get("filter_criteria"),
                        "description": data_source_config.get("u_source", {}).get("description", "Unlabeled samples from training data")
                    },
                    "prediction_period": {
                        "start_date": data_source_config.get("prediction_config", {}).get("start_date"),
                        "end_date": data_source_config.get("prediction_config", {}).get("end_date")
                    }
                })
        else:
            # å¦‚æœæ˜¯è¨“ç·´æœŸé–“çš„æ¸¬è©¦é›†è©•ä¼°
            test_set_source.update({
                "type": "auto_split_from_training",
                "scenario_type": evaluation_config.get("scenario_type", "ERM_BASELINE")
            })

            if model_info:
                data_source_config = model_info.get("data_source_config", {})
                data_split_config = data_source_config.get("data_split", {})
                p_source_config = data_source_config.get("p_source", {})
                u_source_config = data_source_config.get("u_source", {})
                prediction_config = data_source_config.get("prediction_config", {})

                test_set_source.update({
                    "split_configuration": {
                        "split_ratio": data_split_config.get("test_ratio", 0.2),
                        "enabled": data_split_config.get("enabled", False),
                        "train_ratio": data_split_config.get("train_ratio", 0.6),
                        "validation_ratio": data_split_config.get("validation_ratio", 0.2)
                    },
                    "p_source": {
                        "type": p_source_config.get("type", "anomaly_events"),
                        "experiment_run_id": p_source_config.get("experiment_run_id"),
                        "filter_criteria": p_source_config.get("filter_criteria", "status = 'CONFIRMED_POSITIVE'"),
                        "description": p_source_config.get("description", "Positive samples from anomaly events")
                    },
                    "u_source": {
                        "type": u_source_config.get("type", "anomaly_events"),
                        "time_range": u_source_config.get("time_range"),
                        "building_floors": u_source_config.get("building_floors"),
                        "sample_limit": u_source_config.get("sample_limit"),
                        "experiment_run_id": u_source_config.get("experiment_run_id"),
                        "filter_criteria": u_source_config.get("filter_criteria"),
                        "description": u_source_config.get("description", "Unlabeled samples")
                    },
                    "prediction_period": {
                        "start_date": prediction_config.get("start_date"),
                        "end_date": prediction_config.get("end_date")
                    },
                    "experiment_run_id": model_info.get("experiment_run_id")
                })

        return test_set_source

    @staticmethod
    async def make_predictions_with_metrics(
        model: Any,
        X_test: np.ndarray,
        y_test: Optional[np.ndarray] = None,
        evaluation_run_id: Optional[str] = None,
        db_manager = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬ä¸¦è¨ˆç®—æŒ‡æ¨™

        Args:
            model: è¨“ç·´å¥½çš„æ¨¡å‹
            X_test: æ¸¬è©¦ç‰¹å¾µçŸ©é™£
            y_test: æ¸¬è©¦æ¨™ç±¤ï¼ˆå¯é¸ï¼Œå¦‚æœæä¾›å‰‡è¨ˆç®—æŒ‡æ¨™ï¼‰
            evaluation_run_id: è©•ä¼°é‹è¡ŒIDï¼ˆå¯é¸ï¼Œå¦‚æœæä¾›å‰‡è¨˜éŒ„é æ¸¬çµæœï¼‰
            db_manager: è³‡æ–™åº«ç®¡ç†å™¨ï¼ˆç”¨æ–¼è¨˜éŒ„é æ¸¬çµæœï¼‰

        Returns:
            Dict: åŒ…å«é æ¸¬çµæœå’ŒæŒ‡æ¨™çš„å­—å…¸
        """
        try:
            logger.info(f"ğŸ”® é–‹å§‹æ¨¡å‹é æ¸¬ï¼Œæ¸¬è©¦æ¨£æœ¬æ•¸: {len(X_test)}")

            # é€²è¡Œé æ¸¬
            if hasattr(model, 'predict_proba'):
                # ç²å–é æ¸¬æ¦‚ç‡
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
                logger.info(f"âœ… ä½¿ç”¨ predict_proba é€²è¡Œé æ¸¬")
            else:
                # ç›´æ¥é æ¸¬
                y_pred = model.predict(X_test)
                y_pred_proba = y_pred.astype(float)
                logger.info(f"âœ… ä½¿ç”¨ predict é€²è¡Œé æ¸¬")

            # é æ¸¬çµæœçµ±è¨ˆ
            positive_predictions = int(np.sum(y_pred))
            negative_predictions = int(len(y_pred) - np.sum(y_pred))

            result = {
                "predictions": y_pred,
                "prediction_probabilities": y_pred_proba,
                "prediction_stats": {
                    "total_samples": len(y_pred),
                    "positive_predictions": positive_predictions,
                    "negative_predictions": negative_predictions,
                    "positive_ratio": positive_predictions / len(y_pred) if len(y_pred) > 0 else 0
                }
            }

            # å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤ï¼Œè¨ˆç®—è©•ä¼°æŒ‡æ¨™
            if y_test is not None:
                metrics = ModelPredictor._calculate_evaluation_metrics(y_test, y_pred, y_pred_proba)
                result["evaluation_metrics"] = metrics
                logger.info(f"ğŸ“Š è¨ˆç®—è©•ä¼°æŒ‡æ¨™å®Œæˆ: accuracy={metrics['accuracy']:.3f}")

            # å¦‚æœæä¾›äº†evaluation_run_idï¼Œè¨˜éŒ„å€‹åˆ¥é æ¸¬çµæœ
            if evaluation_run_id and db_manager:
                await ModelPredictor._record_predictions(
                    evaluation_run_id, y_pred_proba, y_test, db_manager
                )

            logger.info(f"ğŸ¯ é æ¸¬å®Œæˆ: {positive_predictions} æ­£æ¨£æœ¬, {negative_predictions} è² æ¨£æœ¬")
            return result

        except Exception as e:
            logger.error(f"âŒ é æ¸¬å¤±æ•—: {e}")
            raise e

    @staticmethod
    def _calculate_evaluation_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_pred_proba: np.ndarray) -> Dict[str, Any]:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
        )

        # è¨ˆç®—æ··æ·†çŸ©é™£
        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
        tn, fp, fn, tp = cm.ravel()

        # è¨ˆç®—å„é …æŒ‡æ¨™
        metrics = {
            "accuracy": float(accuracy_score(y_true, y_pred)),
            "precision": float(precision_score(y_true, y_pred, zero_division=0)),
            "recall": float(recall_score(y_true, y_pred, zero_division=0)),
            "f1_score": float(f1_score(y_true, y_pred, zero_division=0)),
            "confusion_matrix": {
                "tp": int(tp),
                "fp": int(fp),
                "tn": int(tn),
                "fn": int(fn)
            },
            "support": {
                "positive": int(np.sum(y_true == 1)),
                "negative": int(np.sum(y_true == 0)),
                "total": int(len(y_true))
            }
        }

        return metrics

    @staticmethod
    async def _record_predictions(
        evaluation_run_id: str,
        y_pred_proba: np.ndarray,
        y_true: Optional[np.ndarray],
        db_manager
    ) -> None:
        """è¨˜éŒ„å€‹åˆ¥é æ¸¬çµæœåˆ°è³‡æ–™åº«"""
        try:
            predictions = []
            for i in range(len(y_pred_proba)):
                predictions.append({
                    "prediction_score": float(y_pred_proba[i]),
                    "ground_truth": int(y_true[i]) if y_true is not None else None,
                    "timestamp": datetime.utcnow()
                })

            # æ‰¹é‡å‰µå»º ModelPrediction è¨˜éŒ„
            if predictions:
                prediction_ids = await db_manager.create_model_predictions(
                    evaluation_run_id,
                    predictions
                )
                logger.info(f"ğŸ“ è¨˜éŒ„äº† {len(prediction_ids)} å€‹é æ¸¬çµæœåˆ°è³‡æ–™åº«")

        except Exception as e:
            logger.error(f"âŒ è¨˜éŒ„é æ¸¬çµæœå¤±æ•—: {e}")
            # ä¸è¦å› ç‚ºè¨˜éŒ„å¤±æ•—è€Œä¸­æ–·ä¸»æµç¨‹

    @staticmethod
    async def prepare_test_features(
        test_data: pd.DataFrame,
        job_id: Optional[str] = None,
        broadcast_progress_func = None
    ) -> np.ndarray:
        """
        æº–å‚™æ¸¬è©¦ç‰¹å¾µçŸ©é™£

        Args:
            test_data: æ¸¬è©¦æ•¸æ“š
            job_id: ä»»å‹™IDï¼ˆç”¨æ–¼é€²åº¦å»£æ’­ï¼‰
            broadcast_progress_func: é€²åº¦å»£æ’­å‡½æ•¸

        Returns:
            np.ndarray: ç‰¹å¾µçŸ©é™£
        """
        try:
            from services.feature_engineering import feature_engineering

            logger.info(f"ğŸ”§ æº–å‚™æ¸¬è©¦ç‰¹å¾µï¼Œæ•¸æ“šå½¢ç‹€: {test_data.shape}")

            # å»£æ’­é€²åº¦ï¼ˆå¦‚æœæœ‰å»£æ’­å‡½æ•¸ï¼‰
            if broadcast_progress_func:
                await broadcast_progress_func({
                    "stage": "feature_preparation",
                    "message": f"æ­£åœ¨è™•ç† {len(test_data)} æ¢æ¸¬è©¦æ•¸æ“š",
                    "job_id": job_id,
                    "details": {
                        "samples_count": len(test_data),
                        "columns": test_data.columns.tolist()
                    }
                })

            # æª¢æŸ¥æ•¸æ“šæ ¼å¼ä¸¦è™•ç†
            if 'dataWindow' in test_data.columns:
                # ç•°å¸¸äº‹ä»¶æ ¼å¼
                test_events = test_data.to_dict('records')
                logger.info(f"ğŸ”§ ç•°å¸¸äº‹ä»¶æ ¼å¼ï¼Œè½‰æ› {len(test_events)} å€‹äº‹ä»¶")

                if broadcast_progress_func:
                    await broadcast_progress_func({
                        "stage": "feature_engineering",
                        "message": "å¾ç•°å¸¸äº‹ä»¶æ•¸æ“šç”Ÿæˆç‰¹å¾µ",
                        "job_id": job_id,
                        "details": {
                            "data_format": "anomaly_events",
                            "events_count": len(test_events)
                        }
                    })

                # ä½¿ç”¨ç‰¹å¾µå·¥ç¨‹ç”Ÿæˆç‰¹å¾µçŸ©é™£
                feature_matrix, _ = feature_engineering.generate_feature_matrix(test_events)

            else:
                # åŸå§‹é›»è¡¨æ•¸æ“šæ ¼å¼
                logger.info("ğŸ”§ åŸå§‹é›»è¡¨æ•¸æ“šæ ¼å¼ï¼Œæ§‹å»ºç‰¹å¾µçŸ©é™£")

                if broadcast_progress_func:
                    await broadcast_progress_func({
                        "stage": "feature_engineering",
                        "message": "è™•ç†åŸå§‹é›»è¡¨æ•¸æ“šç‰¹å¾µ",
                        "job_id": job_id,
                        "details": {
                            "data_format": "raw_meter_data",
                            "samples_count": len(test_data)
                        }
                    })

                # æ§‹å»ºç‰¹å¾µçŸ©é™£
                feature_matrix = ModelPredictor._build_feature_matrix_from_raw_data(test_data)

            # æ¨™æº–åŒ–ç‰¹å¾µ
            if broadcast_progress_func:
                await broadcast_progress_func({
                    "stage": "feature_normalization",
                    "message": "æ¨™æº–åŒ–ç‰¹å¾µæ•¸æ“š",
                    "job_id": job_id,
                    "details": {
                        "feature_matrix_shape": feature_matrix.shape
                    }
                })

            X_test = feature_engineering.transform_features(feature_matrix)

            # è™•ç† NaN å€¼
            if np.isnan(X_test).any():
                logger.warning("âš ï¸ ç™¼ç¾ NaN å€¼ï¼Œç”¨ 0 å¡«å……")
                X_test = np.nan_to_num(X_test, nan=0.0)

            logger.info(f"âœ… ç‰¹å¾µæº–å‚™å®Œæˆï¼Œå½¢ç‹€: {X_test.shape}")
            return X_test

        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾µæº–å‚™å¤±æ•—: {e}")
            raise e

    @staticmethod
    def _build_feature_matrix_from_raw_data(test_data: pd.DataFrame) -> np.ndarray:
        """å¾åŸå§‹æ•¸æ“šæ§‹å»ºç‰¹å¾µçŸ©é™£"""
        feature_columns = [
            'power_consumption', 'power_variance', 'power_trend', 'power_max', 'power_min',
            'power_std', 'power_range', 'hour_of_day', 'day_of_week', 'is_weekend'
        ]

        # ç¢ºä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
        available_features = []
        for col in feature_columns:
            if col in test_data.columns:
                available_features.append(test_data[col].values)
            else:
                logger.warning(f"ç¼ºå°‘ç‰¹å¾µåˆ—: {col}ï¼Œä½¿ç”¨é»˜èªå€¼0")
                available_features.append(np.zeros(len(test_data)))

        # è£œé½Šåˆ°17ç¶­ç‰¹å¾µ
        while len(available_features) < 17:
            available_features.append(np.zeros(len(test_data)))

        # æ§‹å»ºç‰¹å¾µçŸ©é™£
        feature_matrix = np.column_stack(available_features[:17])
        logger.info(f"ğŸ”§ æ§‹å»ºç‰¹å¾µçŸ©é™£: {feature_matrix.shape}")

        return feature_matrix
