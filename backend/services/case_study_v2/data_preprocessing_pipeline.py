"""
çµ±ä¸€çš„æ•¸æ“šè™•ç†ç®¡é“ (Unified Data Processing Pipeline)
ç¢ºä¿è¨“ç·´å’Œè©•ä¼°æ™‚ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç‰¹å¾µå·¥ç¨‹å’Œæ¨™æº–åŒ–æµç¨‹

=== æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ ===
1. æ‰€æœ‰ç‰¹å¾µå·¥ç¨‹é‚è¼¯é›†ä¸­åœ¨æ­¤æ¨¡çµ„
2. StandardScaler çš„ fit/transform åš´æ ¼åˆ†é›¢
3. å¯åºåˆ—åŒ–ï¼Œèˆ‡æ¨¡å‹ä¸€åŒæ‰“åŒ…å„²å­˜
4. åŒ…å«å®Œæ•´çš„å¥å…¨æ€§æª¢æŸ¥
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import logging
from typing import Dict, List, Any, Optional, Tuple
import pickle
import json
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class DataPreprocessingPipeline:
    """
    çµ±ä¸€çš„æ•¸æ“šè™•ç†ç®¡é“é¡åˆ¥

    æ­¤é¡åˆ¥è² è²¬ï¼š
    1. ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)
    2. ç‰¹å¾µæ¨™æº–åŒ– (Feature Scaling)
    3. å¥å…¨æ€§æª¢æŸ¥ (Sanity Checks)
    4. å¯åºåˆ—åŒ–å„²å­˜/è¼‰å…¥
    """

    def __init__(self,
                 window_config: Dict[str, int] = None,
                 feature_config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–æ•¸æ“šè™•ç†ç®¡é“

        Args:
            window_config: æ™‚é–“çª—å£é…ç½®
            feature_config: ç‰¹å¾µå·¥ç¨‹é…ç½®
        """
        # é è¨­çª—å£é…ç½®
        self.window_config = window_config or {
            "main_window_minutes": 60,
            "short_window_minutes": 30,
            "medium_window_minutes": 60,
            "long_window_minutes": 240
        }

        # é è¨­ç‰¹å¾µé…ç½®
        self.feature_config = feature_config or {
            "include_statistical_features": True,
            "include_temporal_features": True,
            "include_multiscale_features": True
        }

        # StandardScaler ç‰©ä»¶
        self.scaler = StandardScaler()
        self.is_fitted = False

        # è¨“ç·´æ™‚çš„ç‰¹å¾µåç¨± (ç”¨æ–¼é©—è­‰)
        self.expected_feature_names: Optional[List[str]] = None
        self.expected_feature_count: Optional[int] = None

        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            "last_fit_time": None,
            "fit_sample_count": 0,
            "transform_sample_count": 0,
            "feature_count": 0
        }

        logger.info(f"âœ… DataPreprocessingPipeline initialized with config:")
        logger.info(f"   Window config: {self.window_config}")
        logger.info(f"   Feature config: {self.feature_config}")

    def extract_temporal_features_from_analysis_data(self,
                                                   sample: Tuple,
                                                   all_samples_dict: Dict[str, Tuple]) -> np.ndarray:
        """
        å¾ analysis_ready_data æ¨£æœ¬ä¸­æå–æ™‚é–“ç‰¹å¾µ

        é€™æ˜¯æ ¸å¿ƒçš„ç‰¹å¾µå·¥ç¨‹å‡½æ•¸ï¼Œå¿…é ˆèˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´

        Args:
            sample: (id, meter_id, timestamp, raw_l1, raw_l2, wattage_110v, wattage_220v, wattage_total, ...)
            all_samples_dict: æ‰€æœ‰æ¨£æœ¬çš„å­—å…¸ï¼Œç”¨æ–¼æ™‚é–“çª—å£æŸ¥æ‰¾

        Returns:
            np.ndarray: æå–çš„ç‰¹å¾µå‘é‡ (æ‡‰è©²æ˜¯ 8 ç¶­)
        """
        try:
            # è§£ææ¨£æœ¬æ•¸æ“š
            sample_id, meter_id, timestamp_str, raw_l1, raw_l2, wattage_110v, wattage_220v, wattage_total = sample[:8]

            # è½‰æ›æ™‚é–“æˆ³
            sample_time = pd.to_datetime(timestamp_str)

            # ç²å–çª—å£é…ç½®
            main_window = self.window_config["main_window_minutes"]
            short_window = self.window_config["short_window_minutes"]
            medium_window = self.window_config["medium_window_minutes"]
            long_window = self.window_config["long_window_minutes"]

            # è¨ˆç®—æ™‚é–“çª—å£
            window_start = sample_time - timedelta(minutes=main_window)

            # ç²å–çª—å£å…§çš„æ‰€æœ‰æ¨£æœ¬
            window_samples = []
            for other_sample in all_samples_dict.values():
                other_time = pd.to_datetime(other_sample[2])
                if window_start <= other_time <= sample_time and other_sample[1] == meter_id:
                    window_samples.append(other_sample)

            # å¦‚æœçª—å£å…§æ¨£æœ¬ä¸è¶³ï¼Œä½¿ç”¨ç•¶å‰æ¨£æœ¬çš„å€¼
            if len(window_samples) < 5:
                logger.debug(f"âš ï¸ Insufficient window samples ({len(window_samples)} < 5), using current sample values")
                features = np.array([
                    float(wattage_total),  # wattage_total_mean
                    0.0,                   # wattage_total_std
                    float(wattage_total),  # wattage_total_max
                    float(wattage_total),  # wattage_total_min
                    float(wattage_110v) if wattage_110v is not None else 0.0,  # wattage_110v_mean
                    float(wattage_220v) if wattage_220v is not None else 0.0,  # wattage_220v_mean
                    float(raw_l1) if raw_l1 is not None else 0.0,             # raw_l1_mean
                    float(raw_l2) if raw_l2 is not None else 0.0              # raw_l2_mean
                ])
                return features

            # å¾çª—å£æ¨£æœ¬ä¸­æå–æ•¸å€¼
            wattage_total_values = [float(s[7]) for s in window_samples if s[7] is not None]
            wattage_110v_values = [float(s[5]) for s in window_samples if s[5] is not None]
            wattage_220v_values = [float(s[6]) for s in window_samples if s[6] is not None]
            raw_l1_values = [float(s[3]) for s in window_samples if s[3] is not None]
            raw_l2_values = [float(s[4]) for s in window_samples if s[4] is not None]

            # è¨ˆç®—çµ±è¨ˆç‰¹å¾µ (8ç¶­)
            features = np.array([
                np.mean(wattage_total_values) if wattage_total_values else 0.0,   # wattage_total_mean
                np.std(wattage_total_values) if len(wattage_total_values) > 1 else 0.0,  # wattage_total_std
                np.max(wattage_total_values) if wattage_total_values else 0.0,    # wattage_total_max
                np.min(wattage_total_values) if wattage_total_values else 0.0,    # wattage_total_min
                np.mean(wattage_110v_values) if wattage_110v_values else 0.0,     # wattage_110v_mean
                np.mean(wattage_220v_values) if wattage_220v_values else 0.0,     # wattage_220v_mean
                np.mean(raw_l1_values) if raw_l1_values else 0.0,                 # raw_l1_mean
                np.mean(raw_l2_values) if raw_l2_values else 0.0                  # raw_l2_mean
            ])

            # å¥å…¨æ€§æª¢æŸ¥
            if np.any(np.isnan(features)) or np.any(np.isinf(features)):
                logger.warning(f"âš ï¸ NaN/Inf detected in features, replacing with zeros")
                features = np.nan_to_num(features, 0.0)

            return features

        except Exception as e:
            logger.error(f"âŒ Error extracting features from sample {sample[0]}: {e}")
            # è¿”å›é›¶å‘é‡ä½œç‚ºå¾Œå‚™
            return np.zeros(8)

    def get_expected_feature_names(self) -> List[str]:
        """è¿”å›æœŸæœ›çš„ç‰¹å¾µåç¨±åˆ—è¡¨"""
        return [
            'wattage_total_mean',
            'wattage_total_std',
            'wattage_total_max',
            'wattage_total_min',
            'wattage_110v_mean',
            'wattage_220v_mean',
            'raw_l1_mean',
            'raw_l2_mean'
        ]

    def fit_transform(self,
                     raw_samples: List[Tuple],
                     all_samples_dict: Dict[str, Tuple] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ“¬åˆæ•¸æ“šè™•ç†ç®¡é“ä¸¦è½‰æ›è¨“ç·´æ•¸æ“š

        é€™å€‹æ–¹æ³•åªèƒ½åœ¨è¨“ç·´éšæ®µå‘¼å«ä¸€æ¬¡ï¼

        Args:
            raw_samples: åŸå§‹æ¨£æœ¬åˆ—è¡¨
            all_samples_dict: æ‰€æœ‰æ¨£æœ¬çš„å­—å…¸ï¼Œç”¨æ–¼æ™‚é–“çª—å£è¨ˆç®—

        Returns:
            Tuple[features, labels]: è™•ç†å¾Œçš„ç‰¹å¾µçŸ©é™£å’Œæ¨™ç±¤å‘é‡
        """
        if self.is_fitted:
            raise ValueError("âŒ Pipeline already fitted! Use transform() for new data.")

        logger.info(f"ğŸ”§ é–‹å§‹æ“¬åˆæ•¸æ“šè™•ç†ç®¡é“ | Starting to fit data preprocessing pipeline")
        logger.info(f"   è¼¸å…¥æ¨£æœ¬æ•¸: {len(raw_samples)}")

        # å¦‚æœæ²’æœ‰æä¾› all_samples_dictï¼Œå‰µå»ºä¸€å€‹
        if all_samples_dict is None:
            all_samples_dict = {sample[0]: sample for sample in raw_samples}

        # Step 1: ç‰¹å¾µå·¥ç¨‹
        logger.info(f"ğŸ”¬ åŸ·è¡Œç‰¹å¾µå·¥ç¨‹ | Performing feature engineering")
        features_list = []
        labels_list = []

        for sample in raw_samples:
            try:
                # æå–ç‰¹å¾µ
                features = self.extract_temporal_features_from_analysis_data(sample, all_samples_dict)

                # æå–æ¨™ç±¤ (is_positive_label)
                label = sample[9] if len(sample) > 9 else 0

                features_list.append(features)
                labels_list.append(label)

            except Exception as e:
                logger.error(f"âŒ Error processing sample {sample[0]}: {e}")
                # è·³éæœ‰å•é¡Œçš„æ¨£æœ¬
                continue

        if not features_list:
            raise ValueError("âŒ No valid features extracted from training data")

        # è½‰æ›ç‚º numpy é™£åˆ—
        X_features = np.array(features_list)
        y_labels = np.array(labels_list)

        logger.info(f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆ | Feature engineering completed:")
        logger.info(f"   ç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_features.shape}")
        logger.info(f"   æ¨™ç±¤åˆ†å¸ƒ: {np.sum(y_labels == 1)} positive, {np.sum(y_labels == 0)} negative")

        # Step 2: æ“¬åˆä¸¦è½‰æ› StandardScaler
        logger.info(f"ğŸ”§ æ“¬åˆ StandardScaler | Fitting StandardScaler")
        X_scaled = self.scaler.fit_transform(X_features)

        # è¨˜éŒ„æ“¬åˆè³‡è¨Š
        self.is_fitted = True
        self.expected_feature_names = self.get_expected_feature_names()
        self.expected_feature_count = X_features.shape[1]
        self.processing_stats.update({
            "last_fit_time": datetime.now().isoformat(),
            "fit_sample_count": len(raw_samples),
            "feature_count": X_features.shape[1]
        })

        logger.info(f"âœ… æ•¸æ“šè™•ç†ç®¡é“æ“¬åˆå®Œæˆ | Data preprocessing pipeline fitted successfully")
        logger.info(f"   æœŸæœ›ç‰¹å¾µæ•¸: {self.expected_feature_count}")
        logger.info(f"   æœŸæœ›ç‰¹å¾µåç¨±: {self.expected_feature_names}")
        logger.info(f"   Scalerçµ±è¨ˆ: mean={self.scaler.mean_[:3]}..., scale={self.scaler.scale_[:3]}...")

        return X_scaled, y_labels

    def transform(self,
                 raw_samples: List[Tuple],
                 all_samples_dict: Dict[str, Tuple] = None) -> np.ndarray:
        """
        ä½¿ç”¨å·²æ“¬åˆçš„ç®¡é“è½‰æ›æ–°æ•¸æ“š

        é€™å€‹æ–¹æ³•ç”¨æ–¼é©—è­‰é›†ã€æ¸¬è©¦é›†å’Œæœªä¾†çš„è©•ä¼°æ•¸æ“š

        Args:
            raw_samples: åŸå§‹æ¨£æœ¬åˆ—è¡¨
            all_samples_dict: æ‰€æœ‰æ¨£æœ¬çš„å­—å…¸ï¼Œç”¨æ–¼æ™‚é–“çª—å£è¨ˆç®—

        Returns:
            np.ndarray: è™•ç†å¾Œçš„ç‰¹å¾µçŸ©é™£
        """
        if not self.is_fitted:
            raise ValueError("âŒ Pipeline not fitted! Call fit_transform() first.")

        logger.info(f"ğŸ”„ è½‰æ›æ–°æ•¸æ“š | Transforming new data")
        logger.info(f"   è¼¸å…¥æ¨£æœ¬æ•¸: {len(raw_samples)}")

        # å¦‚æœæ²’æœ‰æä¾› all_samples_dictï¼Œå‰µå»ºä¸€å€‹
        if all_samples_dict is None:
            all_samples_dict = {sample[0]: sample for sample in raw_samples}

        # Step 1: ç‰¹å¾µå·¥ç¨‹ (ä½¿ç”¨ç›¸åŒçš„å‡½æ•¸)
        features_list = []
        for sample in raw_samples:
            try:
                features = self.extract_temporal_features_from_analysis_data(sample, all_samples_dict)
                features_list.append(features)
            except Exception as e:
                logger.error(f"âŒ Error processing sample {sample[0]}: {e}")
                # ä½¿ç”¨é›¶å‘é‡ä½œç‚ºå¾Œå‚™
                features_list.append(np.zeros(self.expected_feature_count))

        if not features_list:
            raise ValueError("âŒ No valid features extracted from data")

        X_features = np.array(features_list)

        # Step 2: å¥å…¨æ€§æª¢æŸ¥
        self._validate_features(X_features)

        # Step 3: æ‡‰ç”¨å·²æ“¬åˆçš„ StandardScaler
        X_scaled = self.scaler.transform(X_features)

        # æ›´æ–°çµ±è¨ˆ
        self.processing_stats["transform_sample_count"] += len(raw_samples)

        logger.info(f"âœ… æ•¸æ“šè½‰æ›å®Œæˆ | Data transformation completed")
        logger.info(f"   è¼¸å‡ºç‰¹å¾µçŸ©é™£å½¢ç‹€: {X_scaled.shape}")

        return X_scaled

    def _validate_features(self, X_features: np.ndarray):
        """åŸ·è¡Œç‰¹å¾µå¥å…¨æ€§æª¢æŸ¥"""

        # æª¢æŸ¥ç‰¹å¾µæ•¸é‡
        if X_features.shape[1] != self.expected_feature_count:
            raise ValueError(
                f"âŒ Feature count mismatch! Expected {self.expected_feature_count}, got {X_features.shape[1]}"
            )

        # æª¢æŸ¥æ˜¯å¦æœ‰ç•°å¸¸å€¼
        if np.any(np.isnan(X_features)):
            logger.warning(f"âš ï¸ NaN values detected in features")

        if np.any(np.isinf(X_features)):
            logger.warning(f"âš ï¸ Infinity values detected in features")

        logger.debug(f"âœ… ç‰¹å¾µå¥å…¨æ€§æª¢æŸ¥é€šé | Feature validation passed")

    def get_feature_info(self) -> Dict[str, Any]:
        """ç²å–ç‰¹å¾µè™•ç†è³‡è¨Š"""
        return {
            "is_fitted": self.is_fitted,
            "expected_feature_names": self.expected_feature_names,
            "expected_feature_count": self.expected_feature_count,
            "window_config": self.window_config,
            "feature_config": self.feature_config,
            "processing_stats": self.processing_stats,
            "scaler_mean": self.scaler.mean_.tolist() if self.is_fitted else None,
            "scaler_scale": self.scaler.scale_.tolist() if self.is_fitted else None
        }

    def save_to_dict(self) -> Dict[str, Any]:
        """å°‡ç®¡é“åºåˆ—åŒ–ç‚ºå­—å…¸ (ç”¨æ–¼æ¨¡å‹æ‰“åŒ…)"""
        if not self.is_fitted:
            raise ValueError("âŒ Cannot save unfitted pipeline")

        return {
            "window_config": self.window_config,
            "feature_config": self.feature_config,
            "scaler_params": {
                "mean_": self.scaler.mean_.tolist(),
                "scale_": self.scaler.scale_.tolist(),
                "var_": self.scaler.var_.tolist(),
                "n_features_in_": self.scaler.n_features_in_,
                "n_samples_seen_": self.scaler.n_samples_seen_
            },
            "expected_feature_names": self.expected_feature_names,
            "expected_feature_count": self.expected_feature_count,
            "processing_stats": self.processing_stats,
            "pipeline_version": "1.0"
        }

    @classmethod
    def load_from_dict(cls, pipeline_dict: Dict[str, Any]) -> 'DataPreprocessingPipeline':
        """å¾å­—å…¸è¼‰å…¥ç®¡é“ (ç”¨æ–¼æ¨¡å‹è¼‰å…¥)"""

        # å‰µå»ºæ–°çš„ç®¡é“å¯¦ä¾‹
        pipeline = cls(
            window_config=pipeline_dict["window_config"],
            feature_config=pipeline_dict["feature_config"]
        )

        # é‡å»º StandardScaler
        scaler_params = pipeline_dict["scaler_params"]
        pipeline.scaler.mean_ = np.array(scaler_params["mean_"])
        pipeline.scaler.scale_ = np.array(scaler_params["scale_"])
        pipeline.scaler.var_ = np.array(scaler_params["var_"])
        pipeline.scaler.n_features_in_ = scaler_params["n_features_in_"]
        pipeline.scaler.n_samples_seen_ = scaler_params["n_samples_seen_"]

        # è¨­å®šå…¶ä»–å±¬æ€§
        pipeline.is_fitted = True
        pipeline.expected_feature_names = pipeline_dict["expected_feature_names"]
        pipeline.expected_feature_count = pipeline_dict["expected_feature_count"]
        pipeline.processing_stats = pipeline_dict["processing_stats"]

        logger.info(f"âœ… DataPreprocessingPipeline loaded from dict")
        logger.info(f"   ç‰¹å¾µæ•¸: {pipeline.expected_feature_count}")
        logger.info(f"   ç‰¹å¾µåç¨±: {pipeline.expected_feature_names}")

        return pipeline
