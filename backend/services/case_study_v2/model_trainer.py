"""
Model Trainer Service
Handles Stage 3: Model Training with real-time logging
Uses shared model definitions for consistency with evaluator
"""

import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, Any
import random
import pickle
import os
import numpy as np

from .models import StartTrainingJobRequest
from .database import DatabaseManager
from .data_preprocessing_pipeline import DataPreprocessingPipeline
from .shared_models import LSTMPULearningModel, extract_temporal_features, get_feature_names

logger = logging.getLogger(__name__)

# å°ç£æ™‚å€è¨­å®š (UTC+8)
TAIWAN_TZ = timezone(timedelta(hours=8))

def get_taiwan_time() -> datetime:
    """å–å¾—å°ç£æ™‚é–“"""
    return datetime.now(TAIWAN_TZ)

class ModelTrainer:
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self._last_best_f1_score = 0.0  # Track the best F1 score from training
        self._training_metrics = {}  # Store actual training metrics
        self._preprocessing_pipeline = None  # Store the preprocessing pipeline

    async def train_model(self, job_id: str, trained_model_id: str, config: StartTrainingJobRequest, training_data_info: dict = None):
        """
        Train a PU learning model
        """
        logger.info("="*80)
        logger.info(f"ğŸš€ STARTING TRAINING JOB: {job_id}")
        logger.info(f"   ğŸ†” Model ID: {trained_model_id}")
        logger.info(f"   ğŸ“‹ Model Name: {config.model_name}")
        logger.info(f"   ğŸ¯ Model Type: {config.training_config.modelType}")
        logger.info("="*80)

        try:
            # Update model status to running
            await self.db_manager.update_trained_model(trained_model_id, 'RUNNING')
            logger.info(f"âœ… Model status updated to RUNNING")

            # Real training process
            await self._real_training_process(job_id, trained_model_id, config, training_data_info)

            # Generate comprehensive metrics
            training_metrics = await self._generate_training_metrics(config)
            validation_metrics = await self._generate_validation_metrics(config)
            model_path = await self._save_model_artifact(trained_model_id, config)

            await self.db_manager.update_trained_model(
                trained_model_id,
                'COMPLETED',
                model_path=model_path,
                training_metrics=training_metrics,
                validation_metrics=validation_metrics,  # æ–°å¢é©—è­‰æŒ‡æ¨™
                training_data_info=training_data_info,  # ä¿å­˜è¨“ç·´è³‡æ–™çµ±è¨ˆ
                completed_at=get_taiwan_time()
            )

            logger.info("="*80)
            logger.info(f"âœ… TRAINING JOB COMPLETED SUCCESSFULLY: {job_id}")
            logger.info(f"   ğŸ¯ Final F1 Score: {self._last_best_f1_score:.4f}")
            logger.info(f"   ğŸ“ Model saved to: {model_path}")
            logger.info("="*80)

        except Exception as e:
            logger.error("="*80)
            logger.error(f"âŒ TRAINING JOB FAILED: {job_id}")
            logger.error(f"   ğŸš¨ Error: {str(e)}")
            logger.error("="*80)
            await self.db_manager.update_trained_model(trained_model_id, 'FAILED')

    async def _real_training_process(self, job_id: str, model_id: str, config: StartTrainingJobRequest, training_data_info: dict = None):
        """Real PU Learning training process with proper time-series handling and LSTM architecture"""
        logger.info("ğŸ”¬ STARTING REAL TRAINING PROCESS (TIME-SERIES CORRECTED)")
        logger.info(f"   ğŸ†” Job ID: {job_id}")
        logger.info(f"   ğŸ§  Model ID: {model_id}")
        logger.info("   ğŸš¨ FIXES: Time-based split + LSTM architecture")
        logger.info("-"*60)

        import sqlite3
        import json
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import f1_score, precision_score, recall_score
        import pandas as pd
        from datetime import datetime, timedelta

        # 1. è³‡æ–™è¼‰å…¥ - å¾è³‡æ–™åº«ç²å–å¯¦éš›è¨“ç·´è³‡æ–™
        logger.info("ğŸ“‚ Loading training data from database...")
        db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
        conn = sqlite3.connect(db_path)
        logger.info(f"   ğŸ”— Connected to database: {db_path}")

        try:
            # å–å¾—å¯¦é©—è³‡æ–™
            cursor = conn.cursor()

            # å¾ training_data_info ç²å–è³‡æ–™ä¾†æºé…ç½®
            if training_data_info:
                positive_dataset_ids = training_data_info.get('p_data_sources', {}).get('dataset_ids', [])
                unlabeled_dataset_ids = training_data_info.get('u_data_sources', {}).get('dataset_ids', [])
                split_ratios = training_data_info.get('split_ratios', {'train': 0.7, 'validation': 0.2, 'test': 0.1})
                u_sample_ratio = training_data_info.get('u_sample_ratio', 0.1)
            else:
                positive_dataset_ids = []
                unlabeled_dataset_ids = []
                split_ratios = {'train': 0.7, 'validation': 0.2, 'test': 0.1}
                u_sample_ratio = 0.1

            # è¼‰å…¥æ­£æ¨£æœ¬è³‡æ–™ - ä¿ç•™æ™‚é–“æˆ³
            positive_data = []
            if positive_dataset_ids:
                logger.info(f"ğŸ“Š Loading positive samples from datasets: {positive_dataset_ids}")
                for dataset_id in positive_dataset_ids:
                    cursor.execute('''
                        SELECT timestamp, wattage_total, wattage_110v, wattage_220v, raw_wattage_l1, raw_wattage_l2
                        FROM analysis_ready_data
                        WHERE dataset_id = ? AND is_positive_label = 1
                        ORDER BY timestamp
                    ''', (dataset_id,))
                    rows = cursor.fetchall()
                    dataset_positive_count = len(rows)
                    logger.info(f"  ğŸ“ˆ Dataset {dataset_id}: {dataset_positive_count} positive samples")
                    for row in rows:
                        # ä¿ç•™æ™‚é–“æˆ³ï¼š[timestamp, features..., label]
                        positive_data.append([row[0], row[1], row[2], row[3], row[4], row[5], 1])  # åŒ…å« timestamp

            # è¼‰å…¥æœªæ¨™è¨˜è³‡æ–™ (ä½œç‚ºè² æ¨£æœ¬) - ä¿ç•™æ™‚é–“æˆ³
            unlabeled_data = []
            if unlabeled_dataset_ids:
                logger.info(f"ğŸ“Š Loading unlabeled samples from datasets: {unlabeled_dataset_ids}")
                logger.info(f"  ğŸ¯ U-sample ratio: {u_sample_ratio}")
                for dataset_id in unlabeled_dataset_ids:
                    sample_limit = int(10000 * u_sample_ratio)
                    logger.info(f"  ğŸ“‰ Dataset {dataset_id}: sampling up to {sample_limit} unlabeled samples")
                    cursor.execute('''
                        SELECT timestamp, wattage_total, wattage_110v, wattage_220v, raw_wattage_l1, raw_wattage_l2
                        FROM analysis_ready_data
                        WHERE dataset_id = ? AND (is_positive_label = 0 OR is_positive_label IS NULL)
                        ORDER BY timestamp
                        LIMIT ?
                    ''', (dataset_id, sample_limit))
                    rows = cursor.fetchall()
                    dataset_unlabeled_count = len(rows)
                    logger.info(f"  ğŸ“‰ Dataset {dataset_id}: {dataset_unlabeled_count} unlabeled samples loaded")
                    for row in rows:
                        # ä¿ç•™æ™‚é–“æˆ³ï¼š[timestamp, features..., label]
                        unlabeled_data.append([row[0], row[1], row[2], row[3], row[4], row[5], 0])  # åŒ…å« timestamp


            if len(positive_data) == 0 or len(unlabeled_data) == 0:
                raise Exception("Insufficient training data: need both positive and unlabeled samples")

            logger.info(f"ğŸ“Š Data Summary:")
            logger.info(f"  âœ… Positive samples: {len(positive_data)}")
            logger.info(f"  â“ Unlabeled samples: {len(unlabeled_data)}")
            logger.info(f"  ğŸ“ Total samples: {len(positive_data) + len(unlabeled_data)}")
            logger.info(f"  ğŸ¯ Class balance: P={len(positive_data)/(len(positive_data)+len(unlabeled_data)):.3f}, U={len(unlabeled_data)/(len(positive_data)+len(unlabeled_data)):.3f}")

            # 2. è³‡æ–™é è™•ç† - ä½¿ç”¨çµ±ä¸€çš„è³‡æ–™è™•ç†ç®¡é“ä¸¦ä¿æŒæ™‚é–“é †åº
            logger.info("ğŸ”§ Starting data preprocessing with unified pipeline (time-aware)...")

            # ğŸš¨ é—œéµä¿®æ­£ï¼šåˆä½µè³‡æ–™æ™‚ä¿ç•™æ™‚é–“æˆ³ï¼Œç„¶å¾ŒæŒ‰æ™‚é–“æ’åº
            logger.info("  ğŸ“… Combining data and preserving temporal order...")
            all_data = positive_data + unlabeled_data

            # å‰µå»ºåŒ…å«æ™‚é–“æˆ³çš„ DataFrame
            df = pd.DataFrame(all_data, columns=[
                'timestamp', 'wattage_total', 'wattage_110v', 'wattage_220v', 'raw_l1', 'raw_l2', 'label'
            ])
            logger.info(f"  ğŸ“‹ Combined dataframe shape: {df.shape}")

            # å°‡æ™‚é–“æˆ³è½‰æ›ç‚º datetime æ ¼å¼
            df['timestamp'] = pd.to_datetime(df['timestamp'])

            # ğŸš¨ æœ€é—œéµæ­¥é©Ÿï¼šåš´æ ¼æŒ‰æ™‚é–“æˆ³æ’åºï¼Œæ¢å¾©æ­£ç¢ºçš„æ™‚é–“åºåˆ—é †åº
            logger.info("  â° CRITICAL: Sorting data by timestamp to preserve sequence for LSTM...")
            df_before_sort = df.copy()
            df = df.sort_values(by='timestamp').reset_index(drop=True)

            # é©—è­‰æ’åºæ•ˆæœ
            time_span = df['timestamp'].max() - df['timestamp'].min()
            logger.info(f"  âœ… Data sorted successfully:")
            logger.info(f"    ğŸ“… Time range: {df['timestamp'].min()} to {df['timestamp'].max()}")
            logger.info(f"    â±ï¸ Total time span: {time_span}")
            logger.info(f"    ğŸ”¢ Sample count: {len(df)}")

            # æª¢æŸ¥æ’åºå‰å¾Œçš„å·®ç•°
            position_changes = (df_before_sort.index != df.index).sum()
            logger.info(f"    ğŸ”„ Rows reordered: {position_changes} out of {len(df)}")

            if position_changes > 0:
                logger.info(f"    âš ï¸ Data was NOT in chronological order - fixed by sorting")
            else:
                logger.info(f"    âœ… Data was already in chronological order")

            # è™•ç†ç¼ºå¤±å€¼
            missing_before = df.isnull().sum().sum()
            df = df.fillna(0)
            missing_after = df.isnull().sum().sum()
            logger.info(f"  ğŸ§¹ Missing values handled: {missing_before} â†’ {missing_after}")

            # ğŸš¨ ä¿®æ­£ï¼šæ”¹é€²çš„æ™‚é–“åˆ†å‰²ç­–ç•¥ - ç¢ºä¿ P å’Œ U æ¨£æœ¬åœ¨æ‰€æœ‰åˆ†å‰²ä¸­éƒ½æœ‰åˆç†åˆ†ä½ˆ
            logger.info("ğŸ”¥ CRITICAL FIX: Improved time-based split with balanced P/U distribution")
            logger.info("  ğŸ“… Performing separate time-based split for P and U samples...")

            # ç²å–åˆ†å‰²æ¯”ä¾‹
            train_ratio = split_ratios['train']
            val_ratio = split_ratios['validation']
            test_ratio = split_ratios['test']

            # 1. åˆ†åˆ¥è™•ç† P å’Œ U æ¨£æœ¬
            p_df = df[df['label'] == 1].copy()  # æ­£æ¨£æœ¬
            u_df = df[df['label'] == 0].copy()  # æœªæ¨™è¨˜æ¨£æœ¬

            logger.info(f"  ğŸ“Š Sample distribution before split:")
            logger.info(f"    âœ… Positive samples: {len(p_df)}")
            logger.info(f"    â“ Unlabeled samples: {len(u_df)}")

            # 2. å° P æ¨£æœ¬é€²è¡Œæ™‚é–“åˆ†å‰²
            p_total = len(p_df)
            p_train_end = int(p_total * train_ratio)
            p_val_end = int(p_total * (train_ratio + val_ratio))

            p_train_df = p_df.iloc[:p_train_end].copy()
            p_val_df = p_df.iloc[p_train_end:p_val_end].copy()
            p_test_df = p_df.iloc[p_val_end:].copy()

            # 3. å° U æ¨£æœ¬é€²è¡Œæ™‚é–“åˆ†å‰²
            u_total = len(u_df)
            u_train_end = int(u_total * train_ratio)
            u_val_end = int(u_total * (train_ratio + val_ratio))

            u_train_df = u_df.iloc[:u_train_end].copy()
            u_val_df = u_df.iloc[u_train_end:u_val_end].copy()
            u_test_df = u_df.iloc[u_val_end:].copy()

            # 4. åˆä½µæˆæœ€çµ‚çš„æ•¸æ“šé›†ï¼Œä¿æŒæ™‚é–“é †åº
            train_df = pd.concat([p_train_df, u_train_df]).sort_values(by='timestamp').reset_index(drop=True)
            val_df = pd.concat([p_val_df, u_val_df]).sort_values(by='timestamp').reset_index(drop=True)
            test_df = pd.concat([p_test_df, u_test_df]).sort_values(by='timestamp').reset_index(drop=True)

            logger.info(f"  ğŸ“Š Improved time-based split completed:")
            logger.info(f"    ğŸ‹ï¸ Training data: {len(train_df)} samples ({len(p_train_df)} pos, {len(u_train_df)} unlab)")
            logger.info(f"      ğŸ“… Time range: {train_df['timestamp'].min()} to {train_df['timestamp'].max()}")
            logger.info(f"    ğŸ¯ Validation data: {len(val_df)} samples ({len(p_val_df)} pos, {len(u_val_df)} unlab)")
            logger.info(f"      ğŸ“… Time range: {val_df['timestamp'].min()} to {val_df['timestamp'].max()}")
            logger.info(f"    ğŸ§ª Test data: {len(test_df)} samples ({len(p_test_df)} pos, {len(u_test_df)} unlab)")
            logger.info(f"      ğŸ“… Time range: {test_df['timestamp'].min()} to {test_df['timestamp'].max()}")

            # 5. é©—è­‰åˆ†å‰²å“è³ª
            logger.info(f"  ğŸ” Split quality verification:")
            logger.info(f"    ğŸ“Š Training set: P={len(p_train_df)/(len(p_train_df)+len(u_train_df)):.3f}, U={len(u_train_df)/(len(p_train_df)+len(u_train_df)):.3f}")
            logger.info(f"    ğŸ“Š Validation set: P={len(p_val_df)/(len(p_val_df)+len(u_val_df)):.3f}, U={len(u_val_df)/(len(p_val_df)+len(u_val_df)):.3f}")
            logger.info(f"    ğŸ“Š Test set: P={len(p_test_df)/(len(p_test_df)+len(u_test_df)):.3f}, U={len(u_test_df)/(len(p_test_df)+len(u_test_df)):.3f}")
            logger.info(f"    âœ… All splits now contain both P and U samples for meaningful nnPU evaluation!")

            # 2. å°æ¯å€‹åˆ†å‰²åˆ†åˆ¥é€²è¡Œç‰¹å¾µå·¥ç¨‹ï¼ˆé¿å…æ´©æ¼ï¼‰- ä½¿ç”¨å…±äº«å‡½æ•¸ç¢ºä¿ä¸€è‡´æ€§
            window_size = config.training_config.windowSize
            logger.info(f"  ğŸ”§ Applying temporal feature engineering separately to each split...")

            X_train, y_train, train_timestamps = extract_temporal_features(train_df, window_size, "training")
            X_val, y_val, val_timestamps = extract_temporal_features(val_df, window_size, "validation")
            X_test, y_test, test_timestamps = extract_temporal_features(test_df, window_size, "test")

            # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„ç‰¹å¾µæ•¸æ“š
            if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:
                raise Exception("Insufficient data after feature engineering. Consider reducing window_size or increasing dataset size.")

            # 3. æ¨™æº–åŒ–ï¼šåœ¨è¨“ç·´é›†ä¸Š fitï¼Œåœ¨æ‰€æœ‰é›†åˆä¸Š transform
            logger.info("  ğŸ”§ Applying StandardScaler (fit on train, transform on all)...")

            # å‰µå»ºä¸¦é…ç½®é è™•ç†ç®¡é“
            preprocessing_pipeline = DataPreprocessingPipeline(
                window_config={
                    "main_window_minutes": config.training_config.windowSize * 5,
                    "short_window_minutes": config.training_config.windowSize * 2,
                    "medium_window_minutes": config.training_config.windowSize * 5,
                    "long_window_minutes": config.training_config.windowSize * 10
                }
            )
            self._preprocessing_pipeline = preprocessing_pipeline

            # åˆå§‹åŒ–ä¸¦æ“¬åˆ StandardScalerï¼ˆåªåœ¨è¨“ç·´é›†ä¸Šï¼‰
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test)

            # ä¿å­˜ scaler åˆ°é è™•ç†ç®¡é“
            preprocessing_pipeline.scaler = scaler
            preprocessing_pipeline.is_fitted = True

            logger.info(f"  ğŸ“Š Feature scaling completed:")
            logger.info(f"    ğŸ‹ï¸ Training: {X_train_scaled.shape} (scaled means: {np.mean(X_train_scaled, axis=0)[:4]}...)")
            logger.info(f"    ğŸ¯ Validation: {X_val_scaled.shape}")
            logger.info(f"    ğŸ§ª Test: {X_test_scaled.shape}")
            logger.info(f"    âš ï¸ NO DATA LEAKAGE: Scaler fitted only on training data")

            logger.info(f"  ğŸ“Š Data split summary:")
            logger.info(f"    ğŸ‹ï¸ Training set: {X_train_scaled.shape[0]} samples ({np.sum(y_train==1)} pos, {np.sum(y_train==0)} unlab)")
            logger.info(f"    ğŸ¯ Validation set: {X_val_scaled.shape[0]} samples ({np.sum(y_val==1)} pos, {np.sum(y_val==0)} unlab)")
            logger.info(f"    ğŸ§ª Test set: {X_test_scaled.shape[0]} samples ({np.sum(y_test==1)} pos, {np.sum(y_test==0)} unlab)")

            # 4. å»ºç«‹ LSTM æ¨¡å‹ï¼ˆä¿®æ­£ï¼šå¾ MLP æ”¹ç‚ºçœŸæ­£çš„ LSTMï¼‰
            logger.info("ğŸ§  Building LSTM model for time-series PU Learning...")
            logger.info(f"  ï¿½ ARCHITECTURE FIX: Using LSTM instead of MLP")
            logger.info(f"  ï¿½ğŸ”§ Model configuration:")
            logger.info(f"    ğŸ“ Input size: {X_train_scaled.shape[1]}")
            logger.info(f"    ğŸ§© Hidden size: {config.training_config.hiddenSize}")
            logger.info(f"    ğŸ—ï¸ LSTM layers: {config.training_config.numLayers}")
            logger.info(f"    ğŸ’§ Dropout rate: {config.training_config.dropout}")
            logger.info(f"    ğŸ¯ Optimizer: {config.training_config.optimizer}")
            logger.info(f"    ğŸ“ˆ Learning rate: {config.training_config.learningRate}")
            logger.info(f"    ğŸ”’ L2 regularization: {config.training_config.l2Regularization}")
            logger.info(f"    ğŸ² Batch size: {config.training_config.batchSize}")
            logger.info(f"    ğŸ”„ Max epochs: {config.training_config.epochs}")
            logger.info(f"    â° Early stopping: {config.training_config.earlyStopping} (patience: {config.training_config.patience})")

            # å‰µå»º LSTM æ¨¡å‹ (ä½¿ç”¨å…±äº«å®šç¾©ç¢ºä¿èˆ‡è©•ä¼°å™¨ä¸€è‡´)
            model = LSTMPULearningModel(
                input_size=X_train_scaled.shape[1],
                hidden_size=config.training_config.hiddenSize,
                num_layers=config.training_config.numLayers,
                dropout=config.training_config.dropout
            )

            logger.info(f"  âœ… Enhanced LSTM Model created successfully")
            logger.info(f"    ğŸ§  Architecture: {X_train_scaled.shape[1]} features â†’ Batch Norm â†’ LSTM({config.training_config.hiddenSize}Ã—{config.training_config.numLayers}) â†’ FC({config.training_config.hiddenSize//2}) â†’ FC(1)")
            logger.info(f"    ğŸ“Š Total parameters: {sum(p.numel() for p in model.parameters()):,}")
            logger.info(f"    ğŸ”§ Enhancements: Batch normalization, two-layer classifier, Xavier initialization")

            # ç§»å‹•æ¨¡å‹åˆ°é©ç•¶çš„è¨­å‚™
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            model = model.to(device)
            logger.info(f"    ğŸ–¥ï¸ Model running on: {device}")

            # è¨­å®šå„ªåŒ–å™¨
            if config.training_config.optimizer == 'adam':
                optimizer = optim.Adam(model.parameters(), lr=config.training_config.learningRate,
                                     weight_decay=config.training_config.l2Regularization)
            elif config.training_config.optimizer == 'sgd':
                optimizer = optim.SGD(model.parameters(), lr=config.training_config.learningRate,
                                    weight_decay=config.training_config.l2Regularization, momentum=0.9)
            else:
                optimizer = optim.Adam(model.parameters(), lr=config.training_config.learningRate)

            # nnPU Learning æå¤±å‡½æ•¸å¯¦ç¾
            logger.info("ğŸ§¬ Setting up nnPU (non-negative PU Learning) loss function...")
            class_prior = config.training_config.classPrior  # Ï€ (class prior)
            logger.info(f"  ğŸ¯ Class prior (Ï€): {class_prior}")
            logger.info(f"  ğŸ“Š This means we estimate {class_prior*100:.1f}% of unlabeled data are positive")

            def nnpu_loss(outputs, labels, positive_mask, unlabeled_mask, beta=0.0):
                """
                nnPU Loss Implementation
                Based on: "Positive-Unlabeled Learning with Non-Negative Risk Estimator" (Kiryo et al., 2017)

                Args:
                    outputs: Model predictions (sigmoid output)
                    labels: True labels (1 for positive, 0 for unlabeled)
                    positive_mask: Boolean mask for positive samples
                    unlabeled_mask: Boolean mask for unlabeled samples
                    beta: Non-negative coefficient (0 for nnPU, >0 for regularization)
                """
                sigmoid_loss = nn.BCELoss(reduction='none')

                # Positive risk: E_p[l(f(x), +1)]
                if positive_mask.sum() > 0:
                    positive_loss = sigmoid_loss(outputs[positive_mask], torch.ones_like(outputs[positive_mask]))
                    positive_risk = positive_loss.mean()
                else:
                    # âœ… ä¿®æ­£ï¼šä½¿ç”¨ zeros_like ç¢ºä¿è¨ˆç®—åœ–é€£æ¥å’Œè¨­å‚™ä¸€è‡´æ€§
                    positive_risk = torch.zeros_like(outputs.mean())

                # Unlabeled risk components
                if unlabeled_mask.sum() > 0:
                    unlabeled_outputs = outputs[unlabeled_mask]

                    # E_u[l(f(x), -1)]
                    negative_loss_on_unlabeled = sigmoid_loss(unlabeled_outputs, torch.zeros_like(unlabeled_outputs))
                    negative_risk_unlabeled = negative_loss_on_unlabeled.mean()

                    # E_u[l(f(x), +1)]
                    positive_loss_on_unlabeled = sigmoid_loss(unlabeled_outputs, torch.ones_like(unlabeled_outputs))
                    positive_risk_unlabeled = positive_loss_on_unlabeled.mean()
                else:
                    # âœ… ä¿®æ­£ï¼šä½¿ç”¨ zeros_like ç¢ºä¿è¨ˆç®—åœ–é€£æ¥å’Œè¨­å‚™ä¸€è‡´æ€§
                    negative_risk_unlabeled = torch.zeros_like(outputs.mean())
                    positive_risk_unlabeled = torch.zeros_like(outputs.mean())

                # nnPU risk estimate
                pu_risk = class_prior * positive_risk + negative_risk_unlabeled - class_prior * positive_risk_unlabeled

                # Non-negative constraint
                # âœ… æœ€çµ‚ä¿®æ­£ï¼šç¢ºä¿ beta æª¢æŸ¥ä¹Ÿæ˜¯å¼µé‡æ“ä½œ
                beta_tensor = torch.zeros_like(outputs.mean()) + beta
                negative_risk_condition = pu_risk < -beta_tensor

                # ä½¿ç”¨ torch.where é€²è¡Œæ¢ä»¶é¸æ“‡ï¼Œä¿æŒè¨ˆç®—åœ–å®Œæ•´æ€§
                final_risk = torch.where(
                    negative_risk_condition,
                    class_prior * positive_risk + beta_tensor,  # ç•¶é¢¨éšªéè² æ™‚
                    pu_risk  # æ­£å¸¸æƒ…æ³
                )

                return final_risk


            # 5. å¯¦éš›è¨“ç·´éç¨‹ï¼ˆä½¿ç”¨æ™‚é–“åˆ†å‰²çš„æ•¸æ“šï¼‰

            # è½‰æ›ç‚º PyTorch tensors ä¸¦ç§»å‹•åˆ°è¨­å‚™ï¼ˆä½¿ç”¨æ­£ç¢ºåˆ†å‰²çš„æ•¸æ“šï¼‰
            X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
            y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)
            X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)
            y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1).to(device)

            best_val_f1 = 0.0
            patience_counter = 0
            training_start_time = get_taiwan_time()

            logger.info("ğŸš€ Starting nnPU Learning training with Enhanced LSTM...")
            logger.info(f"  â° Training started at: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"  ğŸ–¥ï¸ Training device: {device}")
            logger.info("  ğŸ§¬ Using nnPU (non-negative PU Learning) loss function")
            logger.info("  ğŸ—ï¸ Architecture: LSTM-based time-series model")
            logger.info("  ğŸš¨ DATA INTEGRITY: Time-based split prevents data leakage")
            logger.info("  ğŸ¯ Monitoring metric: Validation F1 Score (for early stopping and model checkpointing)")
            logger.info("  ğŸ“Š Format: Epoch X/Y - nnPU Loss: X.XXX, Val Loss: X.X, Val F1: X.XX")
            logger.info("-" * 80)

            # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™
            training_history = {
                'train_losses': [],
                'val_losses': [],
                'val_f1_scores': [],
                'val_precisions': [],
                'val_recalls': [],
                'epochs_trained': 0,
                'early_stopped': False,
                'best_epoch': 0,
                'nnpu_risks': [],  # è¨˜éŒ„ nnPU risk
                'negative_risks': []  # è¨˜éŒ„è² é¢¨éšªäº‹ä»¶
            }

            for epoch in range(1, config.training_config.epochs + 1):
                # è¨“ç·´æ¨¡å¼
                model.train()
                epoch_train_loss = 0.0
                epoch_negative_risks = 0  # è¨ˆç®—è² é¢¨éšªç™¼ç”Ÿæ¬¡æ•¸
                batch_size = config.training_config.batchSize

                # Mini-batch è¨“ç·´
                for i in range(0, len(X_train_tensor), batch_size):
                    batch_X = X_train_tensor[i:i+batch_size]
                    batch_y = y_train_tensor[i:i+batch_size]

                    optimizer.zero_grad()
                    outputs = model(batch_X)

                    # nnPU Learning æå¤±è¨ˆç®—
                    positive_mask = (batch_y == 1).squeeze()
                    unlabeled_mask = (batch_y == 0).squeeze()

                    # è¨ˆç®— nnPU loss
                    nnpu_risk = nnpu_loss(outputs, batch_y, positive_mask, unlabeled_mask, beta=0.0)

                    # æª¢æŸ¥æ˜¯å¦ç™¼ç”Ÿè² é¢¨éšª
                    if nnpu_risk < 0:
                        epoch_negative_risks += 1

                    nnpu_risk.backward()
                    optimizer.step()
                    epoch_train_loss += nnpu_risk.item()

                avg_train_loss = epoch_train_loss / (len(X_train_tensor) // batch_size + 1)

                # é©—è­‰
                model.eval()
                with torch.no_grad():
                    val_outputs = model(X_val_tensor)

                    # è¨ˆç®—é©—è­‰æå¤± - ä½¿ç”¨ nnPU loss
                    positive_mask_val = (y_val_tensor == 1).squeeze()
                    unlabeled_mask_val = (y_val_tensor == 0).squeeze()
                    val_nnpu_risk = nnpu_loss(val_outputs, y_val_tensor, positive_mask_val, unlabeled_mask_val, beta=0.0)

                    # è¨ˆç®— F1 åˆ†æ•¸å’Œå…¶ä»–æŒ‡æ¨™
                    # æ³¨æ„ï¼šåœ¨ PU Learning ä¸­ï¼Œæˆ‘å€‘åªèƒ½åœ¨ã€Œæ¨™è¨˜ç‚ºæ­£ã€çš„æ¨£æœ¬ä¸Šè¨ˆç®—çœŸå¯¦çš„æ€§èƒ½æŒ‡æ¨™
                    # å°æ–¼æœªæ¨™è¨˜æ¨£æœ¬ï¼Œæˆ‘å€‘ä¸çŸ¥é“çœŸå¯¦æ¨™ç±¤ï¼Œæ‰€ä»¥ F1 åˆ†æ•¸æ˜¯ä¸€å€‹è¿‘ä¼¼å€¼
                    val_pred = (val_outputs > 0.5).float()
                    val_pred_np = val_pred.cpu().numpy().flatten()
                    y_val_np = y_val_tensor.cpu().numpy().flatten()

                    # åªåœ¨å·²çŸ¥æ¨™ç±¤çš„æ¨£æœ¬ä¸Šè¨ˆç®—æŒ‡æ¨™ï¼ˆæ­£æ¨£æœ¬ï¼‰
                    known_positive_mask = y_val_np == 1
                    if known_positive_mask.sum() > 0:
                        # æ­£æ¨£æœ¬çš„å¬å›ç‡ï¼ˆå¤šå°‘æ­£æ¨£æœ¬è¢«æ­£ç¢ºè­˜åˆ¥ï¼‰
                        true_positive_recall = np.mean(val_pred_np[known_positive_mask])

                        # æ•´é«” F1ï¼ˆåŒ…å«æœªæ¨™è¨˜æ¨£æœ¬çš„è¿‘ä¼¼è¨ˆç®—ï¼‰
                        val_f1 = f1_score(y_val_np, val_pred_np, zero_division=0)
                        val_precision = precision_score(y_val_np, val_pred_np, zero_division=0)
                        val_recall = recall_score(y_val_np, val_pred_np, zero_division=0)
                    else:
                        true_positive_recall = 0.0
                        val_f1 = 0.0
                        val_precision = 0.0
                        val_recall = 0.0

                # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€ä½³æ¨¡å‹ï¼ˆåŸºæ–¼ F1 åˆ†æ•¸ï¼‰
                is_best = val_f1 > best_val_f1
                if is_best:
                    best_val_f1 = val_f1
                    patience_counter = 0
                    training_history['best_epoch'] = epoch
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_state = model.state_dict().copy()
                else:
                    patience_counter += 1

                # è¨˜éŒ„è¨“ç·´æ­·å²
                training_history['train_losses'].append(avg_train_loss)
                training_history['val_losses'].append(val_nnpu_risk.item())
                training_history['val_f1_scores'].append(val_f1)
                training_history['val_precisions'].append(val_precision)
                training_history['val_recalls'].append(val_recall)
                training_history['epochs_trained'] = epoch
                training_history['nnpu_risks'].append(avg_train_loss)
                training_history['negative_risks'].append(epoch_negative_risks)

                # è¨˜éŒ„è¨“ç·´é€²åº¦ - nnPU Learning å°ˆç”¨æ ¼å¼
                best_indicator = " ğŸŒŸ (New best!)" if is_best else ""
                negative_risk_indicator = f" âš ï¸ ({epoch_negative_risks} neg.risks)" if epoch_negative_risks > 0 else ""

                # çµ±ä¸€çš„æ—¥èªŒæ ¼å¼ï¼šEpoch X/Y - nnPU Loss: X.XXX, Val Loss: X.X, Val F1: X.XX
                logger.info(f"Epoch {epoch:3d}/{config.training_config.epochs} - "
                           f"nnPU Loss: {avg_train_loss:.3f}, "
                           f"Val Loss: {val_nnpu_risk.item():.3f}, "
                           f"Val F1: {val_f1:.3f}{best_indicator}{negative_risk_indicator}")

                # æ¯ 10 å€‹ epoch æˆ–æœ€ä½³çµæœæ™‚é¡¯ç¤ºé¡å¤–è©³ç´°ä¿¡æ¯
                if epoch % 10 == 0 or is_best or epoch <= 5:
                    logger.info(f"  ğŸ“Š Additional metrics - Precision: {val_precision:.4f}, "
                               f"Recall: {val_recall:.4f}, True Pos Recall: {true_positive_recall:.4f}")
                    logger.info(f"  ğŸ§¬ nnPU status - Patience: {patience_counter}/{config.training_config.patience}, "
                               f"Class prior: {class_prior}")

                # æ—©åœæª¢æŸ¥
                if config.training_config.earlyStopping and patience_counter >= config.training_config.patience:
                    training_history['early_stopped'] = True
                    logger.info(f"â° Early stopping triggered at epoch {epoch}")
                    logger.info(f"  ğŸ¯ Best validation F1: {best_val_f1:.4f} at epoch {training_history['best_epoch']}")
                    break

            # è¼‰å…¥æœ€ä½³æ¨¡å‹
            if 'best_model_state' in locals():
                model.load_state_dict(best_model_state)
                logger.info(f"âœ… Loaded best nnPU model from epoch {training_history['best_epoch']}")

            # è¨ˆç®—è¨“ç·´å®Œæˆæ™‚é–“
            training_end_time = get_taiwan_time()
            training_duration = (training_end_time - training_start_time).total_seconds()

            # åˆ†æ nnPU è¨“ç·´ç‰¹æ€§
            total_negative_risks = sum(training_history['negative_risks'])
            avg_negative_risks_per_epoch = total_negative_risks / training_history['epochs_trained'] if training_history['epochs_trained'] > 0 else 0

            logger.info("-" * 80)
            logger.info("ğŸ nnPU Learning training completed!")
            logger.info(f"  â° Training duration: {training_duration:.2f} seconds ({training_duration/60:.2f} minutes)")
            logger.info(f"  ğŸ”„ Total epochs trained: {training_history['epochs_trained']}")
            logger.info(f"  â° Early stopped: {'Yes' if training_history.get('early_stopped', False) else 'No'}")
            logger.info(f"  ğŸ¯ Best validation F1 achieved: {best_val_f1:.4f} at epoch {training_history['best_epoch']}")
            logger.info(f"  ğŸ§¬ nnPU training analysis:")
            logger.info(f"    âš ï¸ Total negative risk events: {total_negative_risks}")
            logger.info(f"    ğŸ“Š Avg negative risks per epoch: {avg_negative_risks_per_epoch:.1f}")
            logger.info(f"    ğŸ¯ Class prior used: {class_prior}")

            # æœ€çµ‚è©•ä¼° - åœ¨æ¸¬è©¦é›†ä¸Šé€²è¡Œ PU Learning è©•ä¼°
            logger.info("ğŸ§ª Evaluating LSTM nnPU model on test set...")
            model.eval()
            with torch.no_grad():
                X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
                y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)

                test_outputs = model(X_test_tensor)
                test_pred = (test_outputs > 0.5).float()
                test_pred_np = test_pred.cpu().numpy().flatten()
                y_test_np = y_test_tensor.cpu().numpy().flatten()

                # æ¸¬è©¦é›†ä¸Šçš„ nnPU loss
                positive_mask_test = (y_test_tensor == 1).squeeze()
                unlabeled_mask_test = (y_test_tensor == 0).squeeze()
                test_nnpu_risk = nnpu_loss(test_outputs, y_test_tensor, positive_mask_test, unlabeled_mask_test, beta=0.0)

                # æ¨™æº–åˆ†é¡æŒ‡æ¨™ï¼ˆæ³¨æ„ï¼šé€™äº›æŒ‡æ¨™åœ¨ PU Learning ä¸­çš„è§£é‡‹éœ€è¦è¬¹æ…ï¼‰
                final_test_f1 = f1_score(y_test_np, test_pred_np, zero_division=0)
                final_test_precision = precision_score(y_test_np, test_pred_np, zero_division=0)
                final_test_recall = recall_score(y_test_np, test_pred_np, zero_division=0)

                # PU Learning ç‰¹å®šæŒ‡æ¨™
                known_positive_mask_test = y_test_np == 1
                if known_positive_mask_test.sum() > 0:
                    true_positive_recall_test = np.mean(test_pred_np[known_positive_mask_test])
                    # ä¼°è¨ˆçš„é™½æ€§ç‡ï¼ˆåœ¨æœªæ¨™è¨˜æ¨£æœ¬ä¸­ï¼‰
                    unlabeled_mask_np = y_test_np == 0
                    if unlabeled_mask_np.sum() > 0:
                        estimated_positive_rate_in_unlabeled = np.mean(test_pred_np[unlabeled_mask_np])
                    else:
                        estimated_positive_rate_in_unlabeled = 0.0
                else:
                    true_positive_recall_test = 0.0
                    estimated_positive_rate_in_unlabeled = 0.0

            logger.info("ğŸ“Š Final nnPU Model Performance:")
            logger.info(f"  ğŸ¯ Best Validation F1: {best_val_f1:.4f} (epoch {training_history['best_epoch']})")
            logger.info(f"  ğŸ§ª Test nnPU Risk: {test_nnpu_risk.item():.4f}")
            logger.info(f"  ğŸ“ˆ Standard metrics (interpret with caution in PU setting):")
            logger.info(f"    ğŸ¯ Test F1 Score: {final_test_f1:.4f}")
            logger.info(f"    ğŸ¯ Test Precision: {final_test_precision:.4f}")
            logger.info(f"    ğŸ“Š Test Recall: {final_test_recall:.4f}")
            logger.info(f"  ğŸ§¬ PU Learning specific metrics:")
            logger.info(f"    âœ… True Positive Recall: {true_positive_recall_test:.4f}")
            logger.info(f"    ğŸ” Estimated positive rate in unlabeled: {estimated_positive_rate_in_unlabeled:.4f}")
            logger.info(f"    ğŸ“Š Expected vs Actual: {class_prior:.3f} vs {estimated_positive_rate_in_unlabeled:.3f}")

            # è¨ˆç®—æ¨¡å‹å¤§å°
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            logger.info(f"  ğŸ§  Model parameters: {total_params:,} total, {trainable_params:,} trainable")


            # ä¿å­˜æœ€ä½³ F1 åˆ†æ•¸å’Œå®Œæ•´è¨“ç·´æŒ‡æ¨™ä¾›å¾ŒçºŒä½¿ç”¨
            self._last_best_f1_score = best_val_f1
            self._training_metrics = {
                'training_history': training_history,
                'training_duration': training_duration,
                'final_test_metrics': {
                    'test_f1': final_test_f1,
                    'test_precision': final_test_precision,
                    'test_recall': final_test_recall,
                    'test_nnpu_risk': test_nnpu_risk.item(),
                    'true_positive_recall_test': true_positive_recall_test,
                    'estimated_positive_rate_in_unlabeled': estimated_positive_rate_in_unlabeled,
                    # ğŸ”¥ CRITICAL FIX: ä¿å­˜æ™‚é–“æˆ³ä¿¡æ¯ç”¨æ–¼çµæœåˆ†æ
                    'test_timestamps': [ts.isoformat() if hasattr(ts, 'isoformat') else str(ts) for ts in test_timestamps],
                    'test_predictions': test_pred_np.tolist(),  # ä¿å­˜é æ¸¬çµæœ
                    'test_true_labels': y_test_np.tolist()  # ä¿å­˜çœŸå¯¦æ¨™ç±¤
                },
                'data_stats': {
                    'positive_samples': len(positive_data),
                    'unlabeled_samples': len(unlabeled_data),
                    'total_features': X_train_scaled.shape[1],
                    'train_samples': len(X_train_scaled),
                    'val_samples': len(X_val_scaled),
                    'test_samples': len(X_test_scaled),
                    'time_based_split': True,  # æ¨™è¨˜ä½¿ç”¨æ™‚é–“åˆ†å‰²
                    'data_leakage_prevented': True,  # ç¢ºèªç„¡æ•¸æ“šæ´©æ¼
                    # æ·»åŠ æ™‚é–“ç¯„åœä¿¡æ¯
                    'time_ranges': {
                        'train_start': train_df['timestamp'].min().isoformat(),
                        'train_end': train_df['timestamp'].max().isoformat(),
                        'val_start': val_df['timestamp'].min().isoformat(),
                        'val_end': val_df['timestamp'].max().isoformat(),
                        'test_start': test_df['timestamp'].min().isoformat(),
                        'test_end': test_df['timestamp'].max().isoformat(),
                        'total_time_span': (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 3600  # å°æ™‚
                    }
                },
                'model_params': {
                    'total_parameters': sum(p.numel() for p in model.parameters()),
                    'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)
                },
                'nnpu_stats': {
                    'class_prior_used': class_prior,
                    'total_negative_risks': total_negative_risks,
                    'avg_negative_risks_per_epoch': avg_negative_risks_per_epoch,
                    'nnpu_method': 'non-negative PU Learning (Kiryo et al., 2017)'
                }
            }

            # ä¿å­˜æ¨¡å‹å’Œé è™•ç†å™¨
            logger.info("ğŸ’¾ Saving nnPU model artifacts...")
            model_artifact = {
                'model_state_dict': model.state_dict(),
                'scaler': scaler,
                'model_config': config.training_config.dict(),
                'best_f1_score': best_val_f1,
                'nnpu_class_prior': class_prior,
                'nnpu_method': 'non-negative PU Learning',
                'feature_names': get_feature_names(),  # ä½¿ç”¨å…±äº«çš„ç‰¹å¾µåç¨±å‡½æ•¸
                'training_stats': {
                    'negative_risk_events': total_negative_risks,
                    'epochs_trained': training_history['epochs_trained'],
                    'early_stopped': training_history['early_stopped']
                }
            }

            # ä¿å­˜åˆ°æª”æ¡ˆ
            import pickle

            # ä½¿ç”¨å°ç£æ™‚é–“ç”Ÿæˆæª”æ¡ˆåç¨±
            taiwan_time = get_taiwan_time()
            taiwan_time_str = taiwan_time.strftime("%Y%m%d_%H%M%S")

            model_path = f"/home/infowin/Git-projects/pu-in-practice/backend/trained_models/real_model_{model_id}_{taiwan_time_str}.pkl"
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            logger.info(f"  ğŸ“ Saving to: {model_path}")
            logger.info(f"  ğŸ• Taiwan time: {taiwan_time.strftime('%Y-%m-%d %H:%M:%S %Z')}")

            with open(model_path, 'wb') as f:
                pickle.dump(model_artifact, f)

            # æª¢æŸ¥æª”æ¡ˆå¤§å°
            file_size = os.path.getsize(model_path)
            logger.info(f"  ğŸ“¦ Model file size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
            logger.info("âœ… Model artifacts saved successfully!")

        except Exception as e:
            logger.error(f"âŒ Error during training: {str(e)}")
            logger.error(f"   Error type: {type(e).__name__}")
            import traceback
            logger.error(f"   Traceback: {traceback.format_exc()}")
            raise e

        finally:
            conn.close()
            logger.info("ğŸ”Œ Database connection closed")

    async def _generate_training_metrics(self, config: StartTrainingJobRequest) -> Dict[str, Any]:
        """Generate training metrics from actual nnPU training results"""
        model_config = config.training_config

        # ä½¿ç”¨å¯¦éš› nnPU è¨“ç·´çµæœ
        if hasattr(self, '_training_metrics') and self._training_metrics:
            training_history = self._training_metrics['training_history']
            training_duration = self._training_metrics['training_duration']
            final_test_metrics = self._training_metrics['final_test_metrics']
            data_stats = self._training_metrics['data_stats']
            model_params = self._training_metrics['model_params']
            nnpu_stats = self._training_metrics['nnpu_stats']
        else:
            # å‚™ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ²’æœ‰è¨“ç·´è¨˜éŒ„ï¼Œä½¿ç”¨åŸºæœ¬çš„ F1 åˆ†æ•¸
            return {
                'best_val_f1_score': self._last_best_f1_score,
                'error': 'nnPU training metrics not available - training may have failed'
            }

        # è¨ˆç®— nnPU ç‰¹å®šçš„ç©©å®šæ€§æŒ‡æ¨™
        f1_scores = training_history['val_f1_scores']
        nnpu_risks = training_history.get('nnpu_risks', [])
        negative_risks = training_history.get('negative_risks', [])

        if len(f1_scores) > 5:
            recent_f1_std = np.std(f1_scores[-10:])  # æœ€å¾Œ10å€‹epochçš„æ¨™æº–å·®
            training_stability = max(0.3, 1.0 - (recent_f1_std * 2))  # nnPU Learning é€šå¸¸è¼ƒä¸ç©©å®š
        else:
            training_stability = 0.6

        # nnPU ç‰¹å®šçš„é¢¨éšªåˆ†æ
        total_negative_risks = nnpu_stats['total_negative_risks']
        risk_frequency = total_negative_risks / training_history['epochs_trained'] if training_history['epochs_trained'] > 0 else 0

        # è¨ˆç®—éæ“¬åˆé¢¨éšªï¼ˆåŸºæ–¼ nnPU risk è®ŠåŒ–ï¼‰
        if len(nnpu_risks) > 5:
            final_nnpu_risk = nnpu_risks[-1] if nnpu_risks else 0.5
            initial_nnpu_risk = nnpu_risks[2] if len(nnpu_risks) > 2 else 0.6
            risk_trend = (final_nnpu_risk - initial_nnpu_risk) / initial_nnpu_risk if initial_nnpu_risk != 0 else 0
            overfitting_risk = min(0.9, max(0.1, 0.5 + risk_trend))
        else:
            overfitting_risk = 0.5

        # ç²å–æœ€ä½³é©—è­‰æŒ‡æ¨™
        best_epoch = training_history['best_epoch']
        if best_epoch > 0 and best_epoch <= len(f1_scores):
            best_val_f1 = f1_scores[best_epoch - 1]
            best_val_precision = training_history['val_precisions'][best_epoch - 1]
            best_val_recall = training_history['val_recalls'][best_epoch - 1]
            best_val_loss = training_history['val_losses'][best_epoch - 1]
        else:
            best_val_f1 = max(f1_scores) if f1_scores else self._last_best_f1_score
            best_val_precision = max(training_history['val_precisions']) if training_history['val_precisions'] else 0.5
            best_val_recall = max(training_history['val_recalls']) if training_history['val_recalls'] else 0.5
            best_val_loss = min(training_history['val_losses']) if training_history['val_losses'] else 0.5

        # ç¢ºä¿æ‰€æœ‰æ•¸å€¼éƒ½æ˜¯ JSON å¯åºåˆ—åŒ–çš„ï¼ˆè½‰æ› numpy é¡å‹ç‚º Python åŸç”Ÿé¡å‹ï¼‰
        def ensure_json_serializable(value):
            """ç¢ºä¿å€¼å¯ä»¥ JSON åºåˆ—åŒ–"""
            if isinstance(value, (np.integer, np.floating)):
                return float(value)
            elif isinstance(value, np.ndarray):
                return value.tolist()
            elif isinstance(value, list):
                return [ensure_json_serializable(item) for item in value]
            elif isinstance(value, dict):
                return {key: ensure_json_serializable(val) for key, val in value.items()}
            else:
                return value

        return {
            # æœ€çµ‚è¨“ç·´çµæœ - ä¾†è‡ªå¯¦éš› nnPU è¨“ç·´ï¼ˆç¢ºä¿ JSON å¯åºåˆ—åŒ–ï¼‰
            'final_train_loss': ensure_json_serializable(nnpu_risks[-1] if nnpu_risks else 0.5),
            'final_val_loss': ensure_json_serializable(training_history['val_losses'][-1] if training_history['val_losses'] else 0.5),
            'final_val_f1_score': ensure_json_serializable(f1_scores[-1] if f1_scores else self._last_best_f1_score),
            'final_precision': ensure_json_serializable(training_history['val_precisions'][-1] if training_history['val_precisions'] else 0.5),
            'final_recall': ensure_json_serializable(training_history['val_recalls'][-1] if training_history['val_recalls'] else 0.5),

            # æ¸¬è©¦é›†æœ€çµ‚çµæœ - nnPU ç‰¹å®šæŒ‡æ¨™ï¼ˆç¢ºä¿ JSON å¯åºåˆ—åŒ–ï¼‰
            'final_test_f1_score': ensure_json_serializable(final_test_metrics['test_f1']),
            'final_test_precision': ensure_json_serializable(final_test_metrics['test_precision']),
            'final_test_recall': ensure_json_serializable(final_test_metrics['test_recall']),
            'final_test_nnpu_risk': ensure_json_serializable(final_test_metrics['test_nnpu_risk']),
            'true_positive_recall_test': ensure_json_serializable(final_test_metrics['true_positive_recall_test']),
            'estimated_positive_rate_in_unlabeled': ensure_json_serializable(final_test_metrics['estimated_positive_rate_in_unlabeled']),

            # æœ€ä½³é©—è­‰æŒ‡æ¨™ï¼ˆç¢ºä¿ JSON å¯åºåˆ—åŒ–ï¼‰
            'best_val_f1_score': ensure_json_serializable(best_val_f1),
            'best_val_loss': ensure_json_serializable(best_val_loss),
            'best_val_precision': ensure_json_serializable(best_val_precision),
            'best_val_recall': ensure_json_serializable(best_val_recall),
            'best_epoch': int(best_epoch),

            # è¨“ç·´éç¨‹ä¿¡æ¯ - nnPU ç‰¹å®š
            'training_time_seconds': int(training_duration),
            'total_epochs_trained': int(training_history['epochs_trained']),
            'convergence_epoch': int(best_epoch),
            'early_stopped': bool(training_history['early_stopped']),
            'early_stopping_metric': 'val_f1_score_nnpu',

            # nnPU Learning ç‰¹å®šæŒ‡æ¨™ï¼ˆç¢ºä¿ JSON å¯åºåˆ—åŒ–ï¼‰
            'nnpu_method': str(nnpu_stats['nnpu_method']),
            'class_prior_used': ensure_json_serializable(nnpu_stats['class_prior_used']),
            'total_negative_risk_events': int(nnpu_stats['total_negative_risks']),
            'negative_risk_frequency': ensure_json_serializable(risk_frequency),
            'avg_negative_risks_per_epoch': ensure_json_serializable(nnpu_stats['avg_negative_risks_per_epoch']),

            # è³‡æ–™çµ±è¨ˆ - å¯¦éš›æ•¸é‡
            'positive_samples_estimated': int(data_stats['positive_samples']),
            'unlabeled_samples_used': int(data_stats['unlabeled_samples']),
            'total_training_samples': int(data_stats['train_samples']),
            'total_validation_samples': int(data_stats['val_samples']),
            'total_test_samples': int(data_stats['test_samples']),
            'feature_count': int(data_stats['total_features']),

            # æ¨¡å‹åƒæ•¸çµ±è¨ˆ
            'model_parameters': {
                'total_parameters': int(model_params['total_parameters']),
                'trainable_parameters': int(model_params['trainable_parameters'])
            },

            # æ¨¡å‹è¶…åƒæ•¸
            'hyperparameters': {
                'model_type': str(model_config.modelType) + '_nnPU',  # æ¨™ç¤ºé€™æ˜¯ nnPU ç‰ˆæœ¬
                'hidden_size': int(model_config.hiddenSize),
                'num_layers': int(model_config.numLayers),
                'activation_function': str(model_config.activationFunction),
                'dropout': ensure_json_serializable(model_config.dropout),
                'window_size': int(model_config.windowSize),
                'learning_rate': ensure_json_serializable(model_config.learningRate),
                'batch_size': int(model_config.batchSize),
                'optimizer': str(model_config.optimizer),
                'l2_regularization': ensure_json_serializable(model_config.l2Regularization),
                'early_stopping': bool(model_config.earlyStopping),
                'patience': int(model_config.patience) if model_config.earlyStopping else None,
                'lr_scheduler': str(model_config.learningRateScheduler)
            },

            # è³‡æ–™é…ç½®
            'data_config': {
                'train_ratio': ensure_json_serializable(config.data_source_config.trainRatio),
                'validation_ratio': ensure_json_serializable(config.data_source_config.validationRatio),
                'test_ratio': ensure_json_serializable(config.data_source_config.testRatio),
                'time_range': str(config.data_source_config.timeRange)
            },

            # æ€§èƒ½æŒ‡æ¨™ - åŸºæ–¼ nnPU è¨“ç·´è¨ˆç®—ï¼ˆç¢ºä¿ JSON å¯åºåˆ—åŒ–ï¼‰
            'training_stability': ensure_json_serializable(training_stability),
            'overfitting_risk': ensure_json_serializable(overfitting_risk),
            'model_complexity_score': ensure_json_serializable(model_params['total_parameters'] / 10000),
            'nnpu_training_quality': ensure_json_serializable(max(0.1, 1.0 - risk_frequency)),  # nnPU ç‰¹å®šçš„è¨“ç·´å“è³ªæŒ‡æ¨™

            # nnPU è¨“ç·´æ­·å²ï¼ˆç¢ºä¿ JSON å¯åºåˆ—åŒ–ï¼‰
            'training_curves': {
                'train_losses': ensure_json_serializable(training_history['train_losses']),
                'val_losses': ensure_json_serializable(training_history['val_losses']),
                'val_f1_scores': ensure_json_serializable(training_history['val_f1_scores']),
                'val_precisions': ensure_json_serializable(training_history['val_precisions']),
                'val_recalls': ensure_json_serializable(training_history['val_recalls']),
                'nnpu_risks': ensure_json_serializable(nnpu_risks),
                'negative_risk_events': ensure_json_serializable(negative_risks)
            },

            # PU Learning å“è³ªè©•ä¼°ï¼ˆç¢ºä¿ JSON å¯åºåˆ—åŒ–ï¼‰
            'pu_learning_assessment': {
                'class_prior_vs_estimated': {
                    'expected': ensure_json_serializable(nnpu_stats['class_prior_used']),
                    'estimated': ensure_json_serializable(final_test_metrics['estimated_positive_rate_in_unlabeled']),
                    'difference': ensure_json_serializable(abs(nnpu_stats['class_prior_used'] - final_test_metrics['estimated_positive_rate_in_unlabeled']))
                },
                'pu_vs_supervised_difference': 'Metrics should be interpreted differently than standard supervised learning',
                'reliability_note': 'F1/Precision/Recall are approximations in PU Learning setting'
            },

            # æ™‚é–“æˆ³ (ä½¿ç”¨å°ç£æ™‚é–“)
            'training_started_at': (get_taiwan_time() - timedelta(seconds=training_duration)).strftime('%Y-%m-%d %H:%M:%S %Z'),
            'training_completed_at': get_taiwan_time().strftime('%Y-%m-%d %H:%M:%S %Z'),
            'training_method': 'nnPU_Learning'
        }

    async def _save_model_artifact(self, model_id: str, config: StartTrainingJobRequest) -> str:
        """Save model artifact with complete preprocessing pipeline to disk"""
        # Create models directory if it doesn't exist
        models_dir = "/home/infowin/Git-projects/pu-in-practice/backend/trained_models"
        os.makedirs(models_dir, exist_ok=True)

        # ä½¿ç”¨å°ç£æ™‚é–“ç”Ÿæˆæª”æ¡ˆåç¨±
        taiwan_time = get_taiwan_time()
        taiwan_time_str = taiwan_time.strftime("%Y%m%d_%H%M%S")

        # Generate model file path with Taiwan timestamp
        model_filename = f"{config.training_config.modelType}_{model_id}_{taiwan_time_str}.pkl"
        model_path = os.path.join(models_dir, model_filename)

        # æº–å‚™å®Œæ•´çš„æ¨¡å‹åŒ…è£ï¼ŒåŒ…å«é è™•ç†ç®¡é“
        model_package = {
            'model_type': config.training_config.modelType,
            'model_id': model_id,
            'config': config.training_config.dict(),
            'created_at': taiwan_time.isoformat(),  # ä½¿ç”¨å°ç£æ™‚é–“

            # åŒ…å«å®Œæ•´çš„é è™•ç†ç®¡é“
            'preprocessing_pipeline': None,
            'pipeline_fitted': False,

            # æ¨¡å‹çµæ§‹è³‡è¨Š
            'model_architecture': {
                'input_size': 12,  # æ›´æ–°ç‚ºæ–°çš„ç‰¹å¾µæ•¸é‡
                'hidden_size': config.training_config.hiddenSize,
                'num_layers': config.training_config.numLayers,
                'dropout': config.training_config.dropout,
                'activation': config.training_config.activationFunction
            },

            # è¨“ç·´çµæœæŒ‡æ¨™
            'training_metrics': self._training_metrics if hasattr(self, '_training_metrics') else {},

            # ç‰ˆæœ¬è³‡è¨Š
            'pipeline_version': '2.0',
            'format_description': 'Complete model package with preprocessing pipeline'
        }

        # å¦‚æœæœ‰é è™•ç†ç®¡é“ï¼Œå°‡å…¶åºåˆ—åŒ–ä¸¦åŒ…å«
        if self._preprocessing_pipeline is not None:
            logger.info("ğŸ’¾ Saving preprocessing pipeline with model...")
            try:
                model_package['preprocessing_pipeline'] = self._preprocessing_pipeline.save_to_dict()
                model_package['pipeline_fitted'] = True
                logger.info("âœ… Preprocessing pipeline saved successfully")
            except Exception as e:
                logger.error(f"âŒ Failed to save preprocessing pipeline: {e}")
                model_package['pipeline_error'] = str(e)
        else:
            logger.warning("âš ï¸ No preprocessing pipeline found to save")

        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹åŒ…è£
        with open(model_path, 'wb') as f:
            pickle.dump(model_package, f)

        logger.info(f"ğŸ’¾ Complete model package saved to: {model_path}")
        logger.info(f"ğŸ“¦ Package includes: model config, preprocessing pipeline, training metrics")

        return model_path

    async def _generate_validation_metrics(self, config: StartTrainingJobRequest) -> Dict[str, Any]:
        """Generate validation and test metrics from actual nnPU training results"""

        # ä½¿ç”¨å¯¦éš› nnPU è¨“ç·´çµæœ
        if hasattr(self, '_training_metrics') and self._training_metrics:
            training_history = self._training_metrics['training_history']
            final_test_metrics = self._training_metrics['final_test_metrics']
            data_stats = self._training_metrics['data_stats']
            nnpu_stats = self._training_metrics['nnpu_stats']
        else:
            # å‚™ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ²’æœ‰è¨“ç·´è¨˜éŒ„ï¼Œè¿”å›ç©ºæŒ‡æ¨™
            return {
                'error': 'nnPU validation metrics not available - training may have failed'
            }

        # ç¢ºä¿æ‰€æœ‰æ•¸å€¼éƒ½æ˜¯ JSON å¯åºåˆ—åŒ–çš„ï¼ˆè½‰æ› numpy é¡å‹ç‚º Python åŸç”Ÿé¡å‹ï¼‰
        def ensure_json_serializable(value):
            """ç¢ºä¿å€¼å¯ä»¥ JSON åºåˆ—åŒ–"""
            if isinstance(value, (np.integer, np.floating)):
                return float(value)
            elif isinstance(value, np.ndarray):
                return value.tolist()
            elif isinstance(value, list):
                return [ensure_json_serializable(item) for item in value]
            elif isinstance(value, dict):
                return {key: ensure_json_serializable(val) for key, val in value.items()}
            else:
                return value

        # ç²å–æœ€ä½³é©—è­‰æŒ‡æ¨™
        f1_scores = training_history['val_f1_scores']
        best_epoch = training_history['best_epoch']

        if best_epoch > 0 and best_epoch <= len(f1_scores):
            best_val_f1 = f1_scores[best_epoch - 1]
            best_val_precision = training_history['val_precisions'][best_epoch - 1]
            best_val_recall = training_history['val_recalls'][best_epoch - 1]
            best_val_loss = training_history['val_losses'][best_epoch - 1]
        else:
            best_val_f1 = max(f1_scores) if f1_scores else self._last_best_f1_score
            best_val_precision = max(training_history['val_precisions']) if training_history['val_precisions'] else 0.5
            best_val_recall = max(training_history['val_recalls']) if training_history['val_recalls'] else 0.5
            best_val_loss = min(training_history['val_losses']) if training_history['val_losses'] else 0.5

        return {
            # é©—è­‰é›†æœ€ä½³æŒ‡æ¨™
            'validation_metrics': {
                'best_epoch': int(best_epoch),
                'best_f1_score': ensure_json_serializable(best_val_f1),
                'best_precision': ensure_json_serializable(best_val_precision),
                'best_recall': ensure_json_serializable(best_val_recall),
                'best_loss': ensure_json_serializable(best_val_loss),
                'final_f1_score': ensure_json_serializable(f1_scores[-1] if f1_scores else 0.5),
                'final_precision': ensure_json_serializable(training_history['val_precisions'][-1] if training_history['val_precisions'] else 0.5),
                'final_recall': ensure_json_serializable(training_history['val_recalls'][-1] if training_history['val_recalls'] else 0.5),
                'final_loss': ensure_json_serializable(training_history['val_losses'][-1] if training_history['val_losses'] else 0.5),

                # é©—è­‰é›†è¨“ç·´æ›²ç·š
                'training_curves': {
                    'f1_scores': ensure_json_serializable(training_history['val_f1_scores']),
                    'precisions': ensure_json_serializable(training_history['val_precisions']),
                    'recalls': ensure_json_serializable(training_history['val_recalls']),
                    'losses': ensure_json_serializable(training_history['val_losses'])
                }
            },

            # æ¸¬è©¦é›†æœ€çµ‚çµæœ
            'test_metrics': {
                'final_f1_score': ensure_json_serializable(final_test_metrics['test_f1']),
                'final_precision': ensure_json_serializable(final_test_metrics['test_precision']),
                'final_recall': ensure_json_serializable(final_test_metrics['test_recall']),
                'nnpu_risk': ensure_json_serializable(final_test_metrics['test_nnpu_risk']),
                'true_positive_recall': ensure_json_serializable(final_test_metrics['true_positive_recall_test']),
                'estimated_positive_rate_in_unlabeled': ensure_json_serializable(final_test_metrics['estimated_positive_rate_in_unlabeled']),

                # PU Learning ç‰¹å®šè©•ä¼°
                'pu_learning_assessment': {
                    'class_prior_expected': ensure_json_serializable(nnpu_stats['class_prior_used']),
                    'class_prior_estimated': ensure_json_serializable(final_test_metrics['estimated_positive_rate_in_unlabeled']),
                    'prior_estimation_error': ensure_json_serializable(abs(nnpu_stats['class_prior_used'] - final_test_metrics['estimated_positive_rate_in_unlabeled'])),
                    'pu_vs_supervised_note': 'Metrics should be interpreted differently than standard supervised learning',
                    'reliability_note': 'F1/Precision/Recall are approximations in PU Learning setting'
                }
            },

            # æ¨£æœ¬çµ±è¨ˆ
            'sample_statistics': {
                'validation_samples': int(data_stats['val_samples']),
                'test_samples': int(data_stats['test_samples']),
                'positive_samples_in_validation': 'estimated_from_pu_ratio',
                'positive_samples_in_test': 'estimated_from_pu_ratio'
            },

            # nnPU è¨“ç·´å“è³ªæŒ‡æ¨™
            'nnpu_quality_metrics': {
                'method': str(nnpu_stats['nnpu_method']),
                'class_prior_used': ensure_json_serializable(nnpu_stats['class_prior_used']),
                'negative_risk_events': int(nnpu_stats['total_negative_risks']),
                'training_stability': ensure_json_serializable(max(0.1, 1.0 - (nnpu_stats['avg_negative_risks_per_epoch'] * 2))),
                'convergence_quality': 'early_stopped' if training_history['early_stopped'] else 'completed_all_epochs'
            },

            # æ™‚é–“æˆ³
            'evaluation_timestamp': get_taiwan_time().strftime('%Y-%m-%d %H:%M:%S %Z'),
            'evaluation_method': 'nnPU_Learning_Validation'
        }
