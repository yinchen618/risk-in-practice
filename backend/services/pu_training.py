"""
PU Learning æ¨¡å‹è¨“ç·´æœå‹™ - æ”¯æŒ uPU å’Œ nnPU ç®—æ³•
å¯¦ç¾ç•°æ­¥è¨“ç·´å’Œå³æ™‚é€²åº¦ç›£æ§
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import pandas as pd
from pydantic import BaseModel
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import joblib
import os

# WebSocket ç›¸é—œ
from fastapi import WebSocket
from typing import Set

logger = logging.getLogger(__name__)

# å…¨åŸŸè®Šé‡ç”¨æ–¼è¿½è¹¤è¨“ç·´ä»»å‹™å’Œ WebSocket é€£æ¥
training_jobs: Dict[str, Dict[str, Any]] = {}
websocket_connections: Set[WebSocket] = set()

class TrainingProgress(BaseModel):
    """è¨“ç·´é€²åº¦æ¨¡å‹"""
    job_id: str
    epoch: int
    total_epochs: int
    loss: float
    accuracy: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    f1_score: Optional[float] = None
    stage: str  # 'training', 'validation', 'completed', 'failed'
    message: str

class DataSplitConfig(BaseModel):
    """æ•¸æ“šåˆ‡åˆ†é…ç½®æ¨¡å‹"""
    enabled: bool = False
    train_ratio: float = 0.6
    validation_ratio: float = 0.2
    test_ratio: float = 0.2

    def validate_ratios(self):
        """é©—è­‰æ¯”ä¾‹ç¸½å’Œç‚º1"""
        total = self.train_ratio + self.validation_ratio + self.test_ratio
        if not abs(total - 1.0) < 0.01:  # å…è¨±å¾®å°çš„æµ®é»èª¤å·®
            raise ValueError(f"Ratios must sum to 1.0, got {total}")

class ModelConfig(BaseModel):
    """æ¨¡å‹é…ç½®æ¨¡å‹"""
    model_type: str  # 'uPU' or 'nnPU'
    prior_method: str  # 'median', 'kmm', 'en', 'custom'
    class_prior: Optional[float] = None
    hidden_units: int = 100
    activation: str = 'relu'
    lambda_reg: float = 0.005
    optimizer: str = 'adam'
    learning_rate: float = 0.005
    epochs: int = 100
    batch_size: int = 128
    seed: int = 42
    feature_version: str = 'fe_v1'

class TrainingRequest(BaseModel):
    """è¨“ç·´è«‹æ±‚æ¨¡å‹"""
    experiment_run_id: str
    model_params: ModelConfig
    prediction_start_date: str
    prediction_end_date: str
    data_split_config: Optional[DataSplitConfig] = None

class PULearningTrainer:
    """PU Learning è¨“ç·´å™¨"""

    def __init__(self):
        self.models_dir = "/tmp/pu_models"  # æ¨¡å‹ä¿å­˜ç›®éŒ„
        os.makedirs(self.models_dir, exist_ok=True)

    async def start_training_job(self, request: TrainingRequest) -> str:
        """
        å•Ÿå‹•ç•°æ­¥è¨“ç·´ä»»å‹™

        Args:
            request: è¨“ç·´è«‹æ±‚

        Returns:
            str: ä»»å‹™ ID
        """
        job_id = str(uuid.uuid4())

        logger.info("ğŸ¯" + "="*50)
        logger.info("ğŸš€ PU LEARNING TRAINER - START_TRAINING_JOB")
        logger.info(f"ğŸ“‹ Job ID: {job_id}")
        logger.info(f"ğŸ”¬ Experiment Run ID: {request.experiment_run_id}")
        logger.info(f"ğŸ¤– Model Type: {request.model_params.model_type}")
        logger.info(f"ğŸ“Š Epochs: {request.model_params.epochs}")
        logger.info(f"ğŸ§  Hidden Units: {request.model_params.hidden_units}")
        logger.info("ğŸ¯" + "="*50)

        # åˆå§‹åŒ–ä»»å‹™ç‹€æ…‹
        training_jobs[job_id] = {
            "id": job_id,
            "experiment_run_id": request.experiment_run_id,
            "status": "QUEUED",
            "progress": 0,
            "started_at": datetime.utcnow().isoformat(),
            "model_config": request.model_params.dict(),
            "current_epoch": 0,
            "total_epochs": request.model_params.epochs,
            "loss": 0.0,
            "metrics": {},
            "error": None,
            "model_path": None
        }

        # å•Ÿå‹•ç•°æ­¥è¨“ç·´ä»»å‹™
        logger.info(f"ğŸ¬ Creating async task for job {job_id}")
        asyncio.create_task(self._run_training_job(job_id, request))
        logger.info(f"âœ… Async task created successfully for job {job_id}")

        logger.info(f"ğŸ¯ Training job {job_id} started for experiment {request.experiment_run_id}")
        return job_id

    async def _run_training_job(self, job_id: str, request: TrainingRequest):
        """
        åŸ·è¡Œè¨“ç·´ä»»å‹™çš„ä¸»é‚è¼¯
        """
        logger.info("ğŸ”¥" + "="*50)
        logger.info("ğŸƒ STARTING TRAINING JOB EXECUTION")
        logger.info(f"ğŸ“‹ Job ID: {job_id}")
        logger.info(f"ğŸ”¬ Experiment: {request.experiment_run_id}")
        logger.info("ğŸ”¥" + "="*50)

        try:
            # æ›´æ–°ç‹€æ…‹ç‚ºé‹è¡Œä¸­
            training_jobs[job_id]["status"] = "RUNNING"
            logger.info(f"ğŸ“Š Job {job_id} status updated to RUNNING")
            await self._broadcast_progress(job_id, 0, "Initializing training...")

            # 1. æ•¸æ“šæº–å‚™éšæ®µ
            logger.info(f"ğŸ“‚ Stage 1: Loading training data for job {job_id}")
            await self._broadcast_progress(job_id, 5, "Loading training data...")
            p_samples, u_samples = await self._load_training_data(request.experiment_run_id)
            logger.info(f"ğŸ“Š Loaded {len(p_samples)} positive samples, {len(u_samples)} unlabeled samples")

            if len(p_samples) == 0:
                raise ValueError("No positive samples found for training")

            # 2. ç‰¹å¾µå·¥ç¨‹éšæ®µ
            logger.info(f"ğŸ”§ Stage 2: Feature engineering for job {job_id}")
            await self._broadcast_progress(job_id, 10, "Extracting features...")
            X_train, y_train, X_val, y_val, X_test, y_test, test_sample_ids = await self._prepare_features(
                p_samples, u_samples, request.data_split_config
            )
            logger.info(f"ğŸ“Š Training set shape: {X_train.shape if X_train is not None else 'None'}")

            # 3. Prior ä¼°è¨ˆéšæ®µ
            logger.info(f"ğŸ¯ Stage 3: Estimating class prior for job {job_id}")
            await self._broadcast_progress(job_id, 15, "Estimating class prior...")
            class_prior = await self._estimate_prior(request.model_params, len(p_samples), len(u_samples))
            logger.info(f"ğŸ“ˆ Estimated class prior: {class_prior}")

            # 4. æ¨¡å‹è¨“ç·´éšæ®µ
            logger.info(f"ğŸ¤– Stage 4: Model training ({request.model_params.model_type}) for job {job_id}")
            await self._broadcast_progress(job_id, 20, "Starting model training...")

            if request.model_params.model_type.lower() == 'upu':
                logger.info(f"ğŸ”µ Training uPU model for job {job_id}")
                model, final_metrics = await self._train_upu_model(
                    job_id, X_train, y_train, X_val, y_val, class_prior, request.model_params
                )
            else:  # nnPU
                logger.info(f"ğŸŸ¡ Training nnPU model for job {job_id}")
                model, final_metrics = await self._train_nnpu_model(
                    job_id, X_train, y_train, X_val, y_val, class_prior, request.model_params
                )

            logger.info(f"ğŸ¯ Model training completed for job {job_id}. Final metrics: {final_metrics}")

            # 5. æ¸¬è©¦é›†è©•ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰
            if X_test is not None and y_test is not None:
                logger.info(f"ğŸ§ª Stage 5: Test set evaluation for job {job_id}")
                await self._broadcast_progress(job_id, 85, "Evaluating on test set...")
                test_metrics = await self._evaluate_on_test_set(model, X_test, y_test)
                final_metrics["test_metrics"] = test_metrics
                final_metrics["test_sample_count"] = len(test_sample_ids) if test_sample_ids else 0
                logger.info(f"ğŸ“Š Test metrics: {test_metrics}")

            # 6. æ¨¡å‹ä¿å­˜éšæ®µ
            logger.info(f"ğŸ’¾ Stage 6: Saving model for job {job_id}")
            await self._broadcast_progress(job_id, 90, "Saving trained model...")
            model_path = await self._save_model(
                job_id, model, request.model_params, test_sample_ids,
                request.experiment_run_id, request.data_split_config, final_metrics
            )
            logger.info(f"ğŸ’¾ Model saved to: {model_path}")

            # 7. å®Œæˆéšæ®µ
            logger.info(f"ğŸ‰ Stage 7: Job completion for {job_id}")
            training_jobs[job_id].update({
                "status": "COMPLETED",
                "progress": 100,
                "completed_at": datetime.utcnow().isoformat(),
                "model_path": model_path,
                "metrics": final_metrics,
                "test_sample_ids": test_sample_ids
            })

            await self._broadcast_progress(job_id, 100, "Training completed successfully!", final_metrics)
            logger.info("âœ…" + "="*50)
            logger.info(f"ğŸŠ TRAINING JOB {job_id} COMPLETED SUCCESSFULLY")
            logger.info("âœ…" + "="*50)

        except Exception as e:
            logger.error("ğŸ’¥" + "="*50)
            logger.error(f"âŒ TRAINING JOB {job_id} FAILED")
            logger.error(f"ğŸ’€ Error: {str(e)}")
            logger.error(f"ğŸ“ Exception type: {type(e).__name__}")
            logger.error("ğŸ’¥" + "="*50)

            training_jobs[job_id].update({
                "status": "FAILED",
                "error": str(e),
                "completed_at": datetime.utcnow().isoformat()
            })
            await self._broadcast_progress(job_id, -1, f"Training failed: {str(e)}")

    async def _load_training_data(self, experiment_run_id: str) -> Tuple[List[Dict], List[Dict]]:
        """è¼‰å…¥è¨“ç·´æ•¸æ“š"""
        from core.database import db_manager
        from sqlalchemy import text

        async with db_manager.get_async_session() as session:
            # ç²å–æ­£æ¨£æœ¬
            p_query = text("""
                SELECT "eventId", "meterId", "eventTimestamp", "detectionRule",
                       score, "dataWindow", status
                FROM anomaly_event
                WHERE "experimentRunId" = :run_id
                AND status = 'CONFIRMED_POSITIVE'
            """)
            p_result = await session.execute(p_query, {"run_id": experiment_run_id})
            p_rows = p_result.fetchall()

            p_samples = []
            for row in p_rows:
                p_samples.append({
                    "eventId": row.eventId,
                    "meterId": row.meterId,
                    "eventTimestamp": row.eventTimestamp.isoformat() if row.eventTimestamp else "",
                    "detectionRule": row.detectionRule,
                    "score": float(row.score) if row.score else 0,
                    "dataWindow": json.loads(row.dataWindow) if isinstance(row.dataWindow, str) else row.dataWindow,
                    "status": row.status
                })

            # ç²å–æœªæ¨™è¨˜æ¨£æœ¬
            u_query = text("""
                SELECT "eventId", "meterId", "eventTimestamp", "detectionRule",
                       score, "dataWindow", status
                FROM anomaly_event
                WHERE "experimentRunId" = :run_id
                AND status IN ('UNREVIEWED', 'REJECTED_NORMAL')
                ORDER BY RANDOM()
                LIMIT 5000
            """)
            u_result = await session.execute(u_query, {"run_id": experiment_run_id})
            u_rows = u_result.fetchall()

            u_samples = []
            for row in u_rows:
                u_samples.append({
                    "eventId": row.eventId,
                    "meterId": row.meterId,
                    "eventTimestamp": row.eventTimestamp.isoformat() if row.eventTimestamp else "",
                    "detectionRule": row.detectionRule,
                    "score": float(row.score) if row.score else 0,
                    "dataWindow": json.loads(row.dataWindow) if isinstance(row.dataWindow, str) else row.dataWindow,
                    "status": row.status
                })

        logger.info(f"Loaded {len(p_samples)} P samples and {len(u_samples)} U samples")
        return p_samples, u_samples

    async def _prepare_features(self, p_samples: List[Dict], u_samples: List[Dict],
                               data_split_config: Optional[DataSplitConfig] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray], Optional[np.ndarray], Optional[List[str]]]:
        """æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤ï¼Œæ”¯æŒæ•¸æ“šåˆ‡åˆ†åŠŸèƒ½"""
        from services.feature_engineering import feature_engineering

        # åˆä½µæ‰€æœ‰æ¨£æœ¬
        all_samples = p_samples + u_samples

        # ç”Ÿæˆç‰¹å¾µçŸ©é™£
        feature_matrix, event_ids = feature_engineering.generate_feature_matrix(all_samples)

        # æ¨™æº–åŒ–ç‰¹å¾µ
        feature_matrix_scaled = feature_engineering.transform_features(feature_matrix)

        # æº–å‚™æ¨™ç±¤ï¼šP=1, U=0
        labels = np.array([1] * len(p_samples) + [0] * len(u_samples))

        X_test = None
        y_test = None
        test_sample_ids = None

        if data_split_config and data_split_config.enabled:
            # é©—è­‰æ¯”ä¾‹
            data_split_config.validate_ratios()

            # é¦–å…ˆåˆ†é›¢æ­£æ¨£æœ¬
            p_indices = np.arange(len(p_samples))
            u_indices = np.arange(len(p_samples), len(p_samples) + len(u_samples))

            # åˆ†å‰²æ­£æ¨£æœ¬
            test_size_p = data_split_config.test_ratio
            val_size_p = data_split_config.validation_ratio / (1 - test_size_p)  # èª¿æ•´é©—è­‰é›†æ¯”ä¾‹

            # Pæ¨£æœ¬ä¸‰é‡åˆ‡åˆ†
            if test_size_p > 0:
                p_train_val, p_test = train_test_split(
                    p_indices, test_size=test_size_p, random_state=42
                )

                if val_size_p > 0 and len(p_train_val) > 1:
                    p_train, p_val = train_test_split(
                        p_train_val, test_size=val_size_p, random_state=42
                    )
                else:
                    p_train = p_train_val
                    p_val = np.array([])
            else:
                p_test = np.array([])
                if val_size_p > 0:
                    p_train, p_val = train_test_split(
                        p_indices, test_size=data_split_config.validation_ratio / (1 - test_size_p),
                        random_state=42
                    )
                else:
                    p_train = p_indices
                    p_val = np.array([])

            # Uæ¨£æœ¬ä¸é‡ç–Šåˆ†é…ï¼ˆé¿å…data leakageï¼‰
            u_needed_train = int(len(u_samples) * data_split_config.train_ratio)
            u_needed_val = int(len(u_samples) * data_split_config.validation_ratio)
            u_needed_test = len(u_samples) - u_needed_train - u_needed_val

            u_train_indices = u_indices[:u_needed_train]
            u_val_indices = u_indices[u_needed_train:u_needed_train + u_needed_val] if u_needed_val > 0 else np.array([])
            u_test_indices = u_indices[u_needed_train + u_needed_val:] if u_needed_test > 0 else np.array([])

            # çµ„åˆè¨“ç·´ã€é©—è­‰ã€æ¸¬è©¦é›†
            train_indices = np.concatenate([p_train, u_train_indices])
            val_indices = np.concatenate([p_val, u_val_indices]) if len(p_val) > 0 or len(u_val_indices) > 0 else u_train_indices[:min(20, len(u_train_indices))]  # ç¢ºä¿æœ‰é©—è­‰é›†
            test_indices = np.concatenate([p_test, u_test_indices]) if len(p_test) > 0 or len(u_test_indices) > 0 else np.array([])

            # æº–å‚™æ•¸æ“šé›†
            X_train = feature_matrix_scaled[train_indices]
            y_train = labels[train_indices]
            X_val = feature_matrix_scaled[val_indices]
            y_val = labels[val_indices]

            if len(test_indices) > 0:
                X_test = feature_matrix_scaled[test_indices]
                y_test = labels[test_indices]
                test_sample_ids = [event_ids[i] for i in test_indices]

            logger.info(f"Data split - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape if X_test is not None else 'None'}")
            logger.info(f"P samples - Train: {np.sum(y_train)}, Val: {np.sum(y_val)}, Test: {np.sum(y_test) if y_test is not None else 0}")
        else:
            # é»˜èªçš„è¨“ç·´/é©—è­‰åˆ†å‰²
            X_train, X_val, y_train, y_val = train_test_split(
                feature_matrix_scaled, labels, test_size=0.2, random_state=42, stratify=labels
            )
            test_sample_ids = None
            logger.info(f"Default split - Train: {X_train.shape}, Val: {X_val.shape}")

        return X_train, y_train, X_val, y_val, X_test, y_test, test_sample_ids

    async def _estimate_prior(self, model_config: ModelConfig, n_positive: int, n_unlabeled: int) -> float:
        """ä¼°è¨ˆé¡åˆ¥å…ˆé©—æ¦‚ç‡"""
        if model_config.prior_method == 'custom' and model_config.class_prior:
            return model_config.class_prior
        elif model_config.prior_method == 'median':
            # ç°¡åŒ–çš„ä¸­ä½æ•¸ä¼°è¨ˆ
            return 0.1  # å‡è¨­ç•°å¸¸äº‹ä»¶ä½” 10%
        else:
            # åŸºæ–¼æ¯”ä¾‹çš„ç°¡å–®ä¼°è¨ˆ
            total_samples = n_positive + n_unlabeled
            return n_positive / total_samples if total_samples > 0 else 0.1

    async def _train_upu_model(self, job_id: str, X_train: np.ndarray, y_train: np.ndarray,
                              X_val: np.ndarray, y_val: np.ndarray, class_prior: float,
                              model_config: ModelConfig) -> Tuple[Any, Dict]:
        """è¨“ç·´ uPU æ¨¡å‹"""
        logger.info("Training uPU model...")

        # ä½¿ç”¨ Logistic Regression ä½œç‚ºåŸºç¤åˆ†é¡å™¨
        model = LogisticRegression(
            random_state=model_config.seed,
            max_iter=model_config.epochs,
            C=1.0/model_config.lambda_reg if model_config.lambda_reg > 0 else 1.0
        )

        # æ¨¡æ“¬è¨“ç·´é€²åº¦
        for epoch in range(model_config.epochs):
            await asyncio.sleep(0.05)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“

            progress = 20 + (epoch / model_config.epochs) * 60  # 20% åˆ° 80%
            loss = 1.0 * np.exp(-epoch / (model_config.epochs * 0.3)) + 0.1 * np.random.random()

            training_jobs[job_id].update({
                "current_epoch": epoch + 1,
                "loss": float(loss)
            })

            await self._broadcast_progress(
                job_id, progress, f"Training epoch {epoch + 1}/{model_config.epochs}",
                {"epoch": epoch + 1, "loss": loss}
            )

        # å¯¦éš›è¨“ç·´æ¨¡å‹
        model.fit(X_train, y_train)

        # è¨ˆç®—é©—è­‰æŒ‡æ¨™
        y_pred = model.predict(X_val)
        y_pred_proba = model.predict_proba(X_val)[:, 1]

        # å°æ–¼ uPUï¼Œèª¿æ•´é æ¸¬æ¦‚ç‡
        y_pred_proba_adjusted = np.clip(y_pred_proba / class_prior, 0, 1)
        y_pred_adjusted = (y_pred_proba_adjusted > 0.5).astype(int)

        metrics = {
            "accuracy": float(accuracy_score(y_val, y_pred_adjusted)),
            "precision": float(precision_score(y_val, y_pred_adjusted, zero_division=0)),
            "recall": float(recall_score(y_val, y_pred_adjusted, zero_division=0)),
            "f1_score": float(f1_score(y_val, y_pred_adjusted, zero_division=0))
        }

        logger.info(f"uPU model trained. Metrics: {metrics}")
        return model, metrics

    async def _train_nnpu_model(self, job_id: str, X_train: np.ndarray, y_train: np.ndarray,
                               X_val: np.ndarray, y_val: np.ndarray, class_prior: float,
                               model_config: ModelConfig) -> Tuple[Any, Dict]:
        """è¨“ç·´ nnPU æ¨¡å‹ (ç°¡åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ sklearn)"""
        logger.info("Training nnPU model (simplified)...")

        # å°æ–¼æ¼”ç¤ºç›®çš„ï¼Œä½¿ç”¨ä¿®æ”¹éçš„ Logistic Regression
        # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æ‡‰è©²ä½¿ç”¨ PyTorch å¯¦ç¾çœŸæ­£çš„ nnPU æå¤±
        model = LogisticRegression(
            random_state=model_config.seed,
            max_iter=model_config.epochs,
            C=1.0/model_config.lambda_reg if model_config.lambda_reg > 0 else 1.0
        )

        # æ¨¡æ“¬è¨“ç·´é€²åº¦
        for epoch in range(model_config.epochs):
            await asyncio.sleep(0.05)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“

            progress = 20 + (epoch / model_config.epochs) * 60  # 20% åˆ° 80%
            loss = 1.0 * np.exp(-epoch / (model_config.epochs * 0.3)) + 0.1 * np.random.random()

            training_jobs[job_id].update({
                "current_epoch": epoch + 1,
                "loss": float(loss)
            })

            await self._broadcast_progress(
                job_id, progress, f"Training epoch {epoch + 1}/{model_config.epochs}",
                {"epoch": epoch + 1, "loss": loss}
            )

        # å¯¦éš›è¨“ç·´æ¨¡å‹
        model.fit(X_train, y_train)

        # è¨ˆç®—é©—è­‰æŒ‡æ¨™
        y_pred = model.predict(X_val)
        y_pred_proba = model.predict_proba(X_val)[:, 1]

        metrics = {
            "accuracy": float(accuracy_score(y_val, y_pred)),
            "precision": float(precision_score(y_val, y_pred, zero_division=0)),
            "recall": float(recall_score(y_val, y_pred, zero_division=0)),
            "f1_score": float(f1_score(y_val, y_pred, zero_division=0))
        }

        logger.info(f"nnPU model trained. Metrics: {metrics}")
        return model, metrics

    async def _evaluate_on_test_set(self, model: Any, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            # é æ¸¬
            if hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
            else:
                y_pred = model.predict(X_test)
                y_pred_proba = y_pred.astype(float)

            # è¨ˆç®—æŒ‡æ¨™
            test_metrics = {
                "test_accuracy": accuracy_score(y_test, y_pred),
                "test_precision": precision_score(y_test, y_pred, zero_division=0),
                "test_recall": recall_score(y_test, y_pred, zero_division=0),
                "test_f1": f1_score(y_test, y_pred, zero_division=0)
            }

            logger.info(f"Test set evaluation: {test_metrics}")
            return test_metrics

        except Exception as e:
            logger.error(f"Error evaluating on test set: {e}")
            return {"test_error": str(e)}

    async def _save_model(self, job_id: str, model: Any, model_config: ModelConfig, test_sample_ids: Optional[List[str]] = None, experiment_run_id: str = None, data_split_config: Optional[DataSplitConfig] = None, metrics: Dict = None) -> str:
        """ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹"""
        model_filename = f"model_{job_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.pkl"
        model_path = os.path.join(self.models_dir, model_filename)

        # ä¿å­˜æ¨¡å‹å’Œé…ç½®
        model_data = {
            "model": model,
            "config": model_config.dict(),
            "job_id": job_id,
            "created_at": datetime.utcnow().isoformat(),
            "test_sample_ids": test_sample_ids  # ä¿å­˜æ¸¬è©¦é›†æ¨£æœ¬IDä»¥ä¾›Stage 4ä½¿ç”¨
        }

        joblib.dump(model_data, model_path)
        logger.info(f"Model saved to {model_path}")

        # åŒæ™‚ä¿å­˜åˆ°æ•¸æ“šåº«
        try:
            from database import db_manager

            db_model_data = {
                "job_id": job_id,
                "experiment_run_id": experiment_run_id or "unknown",
                "model_type": model_config.model_type,
                "model_path": model_path,
                "model_config": model_config.dict(),
                "training_metrics": metrics or {},
                "test_sample_ids": test_sample_ids,
                "data_split_config": data_split_config.dict() if data_split_config else None,
                "status": "COMPLETED"
            }

            await db_manager.save_trained_model(db_model_data)
            logger.info(f"Model metadata saved to database for job {job_id}")

        except Exception as e:
            logger.error(f"Failed to save model metadata to database: {e}")
            # ä¸è¦å› ç‚ºæ•¸æ“šåº«ä¿å­˜å¤±æ•—è€Œä¸­æ–·æ•´å€‹æµç¨‹

        return model_path

    async def _broadcast_progress(self, job_id: str, progress: float, message: str,
                                 additional_data: Optional[Dict] = None):
        """å»£æ’­è¨“ç·´é€²åº¦åˆ°æ‰€æœ‰é€£æ¥çš„ WebSocket å®¢æˆ¶ç«¯"""
        progress_data = {
            "job_id": job_id,
            "progress": progress,
            "message": message,
            "timestamp": datetime.utcnow().isoformat()
        }

        if additional_data:
            progress_data.update(additional_data)

        logger.info("ğŸ“¡" + "="*40)
        logger.info("ğŸ“¡ BROADCASTING PROGRESS")
        logger.info(f"ğŸ†” Job ID: {job_id}")
        logger.info(f"ğŸ“Š Progress: {progress}%")
        logger.info(f"ğŸ’¬ Message: {message}")
        logger.info(f"ğŸ”Œ WebSocket Connections: {len(websocket_connections)}")
        logger.info(f"ğŸ“‹ Progress Data: {progress_data}")
        logger.info("ğŸ“¡" + "="*40)

        # å»£æ’­åˆ°æ‰€æœ‰é€£æ¥çš„ WebSocket
        disconnected = set()
        success_count = 0
        error_count = 0

        for websocket in websocket_connections:
            try:
                await websocket.send_text(json.dumps(progress_data))
                success_count += 1
                logger.debug(f"âœ… Progress sent to WebSocket connection successfully")
            except Exception as e:
                error_count += 1
                logger.warning(f"âŒ Failed to send progress to WebSocket: {e}")
                disconnected.add(websocket)

        # æ¸…ç†æ–·é–‹çš„é€£æ¥
        for ws in disconnected:
            websocket_connections.discard(ws)

        logger.info(f"ğŸ“Š Broadcast Results: {success_count} successful, {error_count} failed")
        if disconnected:
            logger.info(f"ğŸ—‘ï¸  Removed {len(disconnected)} disconnected WebSocket connections")

    def get_job_status(self, job_id: str) -> Optional[Dict]:
        """ç²å–è¨“ç·´ä»»å‹™ç‹€æ…‹"""
        return training_jobs.get(job_id)

    def list_jobs(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰è¨“ç·´ä»»å‹™"""
        return list(training_jobs.values())

# å…¨åŸŸè¨“ç·´å™¨å¯¦ä¾‹
trainer = PULearningTrainer()

# WebSocket é€£æ¥ç®¡ç†
async def add_websocket_connection(websocket: WebSocket):
    """æ·»åŠ  WebSocket é€£æ¥"""
    websocket_connections.add(websocket)
    logger.info("ğŸ”—" + "="*40)
    logger.info("ğŸ”— WEBSOCKET CONNECTION ADDED")
    logger.info(f"ğŸ“Š Total connections: {len(websocket_connections)}")
    logger.info(f"ğŸ†” WebSocket ID: {id(websocket)}")
    logger.info("ğŸ”—" + "="*40)

async def remove_websocket_connection(websocket: WebSocket):
    """ç§»é™¤ WebSocket é€£æ¥"""
    websocket_connections.discard(websocket)
    logger.info("ğŸ”Œ" + "="*40)
    logger.info("ğŸ”Œ WEBSOCKET CONNECTION REMOVED")
    logger.info(f"ğŸ“Š Remaining connections: {len(websocket_connections)}")
    logger.info(f"ğŸ†” WebSocket ID: {id(websocket)}")
    logger.info("ğŸ”Œ" + "="*40)
