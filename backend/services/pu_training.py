"""
PU Learning æ¨¡å‹è¨“ç·´æœå‹™ - æ”¯æŒ uPU å’Œ nnPU ç®—æ³•
å¯¦ç¾ç•°æ­¥è¨“ç·´å’Œå³æ™‚é€²åº¦ç›£æ§
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import pandas as pd
from pydantic import BaseModel
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import joblib
import os

# WebSocket ç›¸é—œ
from fastapi import WebSocket
from typing import Set

logger = logging.getLogger(__name__)

# å…¨åŸŸè®Šé‡ç”¨æ–¼è¿½è¹¤è¨“ç·´ä»»å‹™å’Œ WebSocket é€£æ¥
training_jobs: Dict[str, Dict[str, Any]] = {}
websocket_connections: Set[WebSocket] = set()
websocket_lock = asyncio.Lock()  # æ·»åŠ ç•°æ­¥é–ä¿è­· WebSocket æ“ä½œ

class TrainingProgress(BaseModel):
    """è¨“ç·´é€²åº¦æ¨¡å‹"""
    job_id: str
    epoch: int
    total_epochs: int
    loss: float
    accuracy: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    f1_score: Optional[float] = None
    stage: str  # 'training', 'validation', 'completed', 'failed'
    message: str

class DataSplitConfig(BaseModel):
    """æ•¸æ“šåˆ‡åˆ†é…ç½®æ¨¡å‹"""
    enabled: bool = False
    train_ratio: float = 0.6
    validation_ratio: float = 0.2
    test_ratio: float = 0.2

    def validate_ratios(self):
        """é©—è­‰æ¯”ä¾‹ç¸½å’Œç‚º1"""
        total = self.train_ratio + self.validation_ratio + self.test_ratio
        if not abs(total - 1.0) < 0.01:  # å…è¨±å¾®å°çš„æµ®é»èª¤å·®
            raise ValueError(f"Ratios must sum to 1.0, got {total}")

class ModelConfig(BaseModel):
    """æ¨¡å‹é…ç½®æ¨¡å‹"""
    model_type: str  # 'uPU' or 'nnPU'
    prior_method: str  # 'median', 'kmm', 'en', 'custom'
    class_prior: Optional[float] = None
    hidden_units: int = 100
    activation: str = 'relu'
    lambda_reg: float = 0.005
    optimizer: str = 'adam'
    learning_rate: float = 0.005
    epochs: int = 100
    batch_size: int = 128
    seed: int = 42
    feature_version: str = 'fe_v1'

class TrainingRequest(BaseModel):
    """è¨“ç·´è«‹æ±‚æ¨¡å‹"""
    experiment_run_id: str
    model_params: ModelConfig
    prediction_start_date: str
    prediction_end_date: str
    data_split_config: Optional[DataSplitConfig] = None
    # æ–°å¢çš„ U æ¨£æœ¬ç”Ÿæˆé…ç½®
    u_sample_time_range: Optional[Dict[str, str]] = None  # {"start_date": "2025-08-13", "end_date": "2025-08-14", "start_time": "00:00", "end_time": "23:59"}
    u_sample_building_floors: Optional[Dict[str, List[str]]] = None  # {"Building A": ["2"], "Building B": ["1", "2"]}
    u_sample_limit: Optional[int] = 5000

class PULearningTrainer:
    """PU Learning è¨“ç·´å™¨"""

    def __init__(self):
        self.models_dir = "/tmp/pu_models"  # æ¨¡å‹ä¿å­˜ç›®éŒ„
        os.makedirs(self.models_dir, exist_ok=True)

    async def start_training_job(self, request: TrainingRequest) -> str:
        """
        å•Ÿå‹•ç•°æ­¥è¨“ç·´ä»»å‹™

        Args:
            request: è¨“ç·´è«‹æ±‚

        Returns:
            str: ä»»å‹™ ID
        """
        job_id = str(uuid.uuid4())

        logger.info("ğŸ¯" + "="*50)
        logger.info("ğŸš€ PU LEARNING TRAINER - START_TRAINING_JOB")
        logger.info(f"ğŸ“‹ Job ID: {job_id}")
        logger.info(f"ğŸ”¬ Experiment Run ID: {request.experiment_run_id}")
        logger.info(f"ğŸ¤– Model Type: {request.model_params.model_type}")
        logger.info(f"ğŸ“Š Epochs: {request.model_params.epochs}")
        logger.info(f"ğŸ§  Hidden Units: {request.model_params.hidden_units}")
        logger.info("ğŸ¯" + "="*50)

        # åˆå§‹åŒ–ä»»å‹™ç‹€æ…‹
        training_jobs[job_id] = {
            "id": job_id,
            "experiment_run_id": request.experiment_run_id,
            "status": "QUEUED",
            "progress": 0,
            "started_at": datetime.utcnow().isoformat(),
            "model_config": request.model_params.dict(),
            "current_epoch": 0,
            "total_epochs": request.model_params.epochs,
            "loss": 0.0,
            "metrics": {},
            "error": None,
            "model_path": None
        }

        # å•Ÿå‹•ç•°æ­¥è¨“ç·´ä»»å‹™
        logger.info(f"ğŸ¬ Creating async task for job {job_id}")
        asyncio.create_task(self._run_training_job(job_id, request))
        logger.info(f"âœ… Async task created successfully for job {job_id}")

        logger.info(f"ğŸ¯ Training job {job_id} started for experiment {request.experiment_run_id}")
        return job_id

    async def _run_training_job(self, job_id: str, request: TrainingRequest):
        """
        åŸ·è¡Œè¨“ç·´ä»»å‹™çš„ä¸»é‚è¼¯
        """
        logger.info("ğŸ”¥" + "="*50)
        logger.info("ğŸƒ STARTING TRAINING JOB EXECUTION")
        logger.info(f"ğŸ“‹ Job ID: {job_id}")
        logger.info(f"ğŸ”¬ Experiment: {request.experiment_run_id}")
        logger.info("ğŸ”¥" + "="*50)

        try:
            # æ›´æ–°ç‹€æ…‹ç‚ºé‹è¡Œä¸­
            training_jobs[job_id]["status"] = "RUNNING"
            logger.info(f"ğŸ“Š Job {job_id} status updated to RUNNING")

            # è¨˜éŒ„å’Œå»£æ’­è¶…åƒæ•¸ä¿¡æ¯
            hyperparams = {
                "model_type": request.model_params.model_type,
                "prior_method": request.model_params.prior_method,
                "class_prior": request.model_params.class_prior,
                "hidden_units": request.model_params.hidden_units,
                "activation": request.model_params.activation,
                "lambda_reg": request.model_params.lambda_reg,
                "optimizer": request.model_params.optimizer,
                "learning_rate": request.model_params.learning_rate,
                "epochs": request.model_params.epochs,
                "batch_size": request.model_params.batch_size,
                "seed": request.model_params.seed,
                "feature_version": request.model_params.feature_version,
            }

            logger.info("ğŸ›ï¸" + "="*50)
            logger.info("ğŸ›ï¸ TRAINING HYPERPARAMETERS")
            logger.info(f"ğŸ¤– Model Type: {hyperparams['model_type']}")
            logger.info(f"ğŸ“Š Prior Method: {hyperparams['prior_method']}")
            logger.info(f"ğŸ¯ Class Prior: {hyperparams['class_prior']}")
            logger.info(f"ğŸ§  Hidden Units: {hyperparams['hidden_units']}")
            logger.info(f"âš¡ Activation: {hyperparams['activation']}")
            logger.info(f"ğŸ“ Lambda Reg: {hyperparams['lambda_reg']}")
            logger.info(f"ğŸ”§ Optimizer: {hyperparams['optimizer']}")
            logger.info(f"ğŸ“ˆ Learning Rate: {hyperparams['learning_rate']}")
            logger.info(f"ğŸ”„ Epochs: {hyperparams['epochs']}")
            logger.info(f"ğŸ“¦ Batch Size: {hyperparams['batch_size']}")
            logger.info(f"ğŸŒ± Seed: {hyperparams['seed']}")
            logger.info(f"ğŸ·ï¸ Feature Version: {hyperparams['feature_version']}")
            logger.info("ğŸ›ï¸" + "="*50)

            await self._broadcast_progress(job_id, 0, "Initializing training...", {
                "model_name": f"{request.model_params.model_type} Model",
                "hyperparameters": hyperparams,
                "stage": "initialization"
            })

            # 1. æ•¸æ“šæº–å‚™éšæ®µ
            logger.info(f"ğŸ“‚ Stage 1: Loading training data for job {job_id}")
            await self._broadcast_progress(job_id, 5, "Stage 1: Loading training data from database...", {
                "stage": "data_loading"
            })

            # æª¢æŸ¥æ˜¯å¦æœ‰ U æ¨£æœ¬ç”Ÿæˆé…ç½®
            if (request.u_sample_time_range and
                request.u_sample_building_floors and
                request.u_sample_limit):
                # ä½¿ç”¨æ–°çš„å‹•æ…‹ U æ¨£æœ¬ç”Ÿæˆæ–¹æ³•
                logger.info("ğŸ†• Using dynamic U sample generation from raw data")
                await self._broadcast_progress(job_id, 7, "Loading P samples from anomaly events...", {
                    "stage": "data_loading",
                    "substage": "positive_samples"
                })
                p_samples, u_samples = await self._load_training_data_with_dynamic_u(
                    job_id,
                    request.experiment_run_id,
                    request.u_sample_time_range,
                    request.u_sample_building_floors,
                    request.u_sample_limit
                )
            else:
                # ä½¿ç”¨èˆŠçš„æ–¹æ³•ï¼ˆå¾ anomaly_event è¡¨ï¼‰
                logger.info("ğŸ“‹ Using traditional U sample loading from anomaly_event table")
                await self._broadcast_progress(job_id, 7, "Loading P and U samples from anomaly events...", {
                    "stage": "data_loading",
                    "substage": "traditional_loading"
                })
                p_samples, u_samples = await self._load_training_data(request.experiment_run_id)

            logger.info(f"ğŸ“Š Loaded {len(p_samples)} positive samples, {len(u_samples)} unlabeled samples")
            await self._broadcast_progress(job_id, 9, f"Data loaded: {len(p_samples)} P samples, {len(u_samples)} U samples", {
                "p_sample_count": len(p_samples),
                "u_sample_count": len(u_samples),
                "model_name": f"{request.model_params.model_type} Model",
                "stage": "data_loading",
                "substage": "completed"
            })

            if len(p_samples) == 0:
                raise ValueError("No positive samples found for training")

            # 2. ç‰¹å¾µå·¥ç¨‹éšæ®µ
            logger.info(f"ğŸ”§ Stage 2: Feature engineering for job {job_id}")
            await self._broadcast_progress(job_id, 10, "Stage 2: Starting feature extraction...", {
                "stage": "feature_engineering"
            })
            X_train, y_train, X_val, y_val, X_test, y_test, test_sample_ids = await self._prepare_features(
                p_samples, u_samples, request.data_split_config
            )
            logger.info(f"ğŸ“Š Training set shape: {X_train.shape if X_train is not None else 'None'}")
            await self._broadcast_progress(job_id, 12, f"Features extracted: {X_train.shape} training samples", {
                "data_split_info": {
                    "train_samples": X_train.shape[0] if X_train is not None else 0,
                    "validation_samples": X_val.shape[0] if X_val is not None else 0,
                    "test_samples": X_test.shape[0] if X_test is not None else 0,
                    "train_p_samples": int(np.sum(y_train)) if y_train is not None else 0,
                    "validation_p_samples": int(np.sum(y_val)) if y_val is not None else 0,
                    "test_p_samples": int(np.sum(y_test)) if y_test is not None else 0,
                    "split_enabled": request.data_split_config.enabled if request.data_split_config else False
                },
                "stage": "feature_engineering",
                "substage": "completed"
            })

            # 3. Prior ä¼°è¨ˆéšæ®µ
            logger.info(f"ğŸ¯ Stage 3: Estimating class prior for job {job_id}")
            await self._broadcast_progress(job_id, 15, "Stage 3: Estimating class prior probability...", {
                "stage": "prior_estimation"
            })
            class_prior = await self._estimate_prior(request.model_params, len(p_samples), len(u_samples))
            logger.info(f"ğŸ“ˆ Estimated class prior: {class_prior}")
            await self._broadcast_progress(job_id, 17, f"Class prior estimated: {class_prior:.4f}", {
                "stage": "prior_estimation",
                "substage": "completed"
            })

            # 4. æ¨¡å‹è¨“ç·´éšæ®µ
            logger.info(f"ğŸ¤– Stage 4: Model training ({request.model_params.model_type}) for job {job_id}")
            await self._broadcast_progress(job_id, 20, f"Stage 4: Initializing {request.model_params.model_type} model training...", {
                "stage": "model_training"
            })

            if request.model_params.model_type.lower() == 'upu':
                logger.info(f"ğŸ”µ Training uPU model for job {job_id}")
                model, final_metrics = await self._train_upu_model(
                    job_id, X_train, y_train, X_val, y_val, class_prior, request.model_params
                )
            else:  # nnPU
                logger.info(f"ğŸŸ¡ Training nnPU model for job {job_id}")
                model, final_metrics = await self._train_nnpu_model(
                    job_id, X_train, y_train, X_val, y_val, class_prior, request.model_params
                )

            logger.info(f"ğŸ¯ Model training completed for job {job_id}. Final metrics: {final_metrics}")
            await self._broadcast_progress(job_id, 80, f"{request.model_params.model_type} model training completed successfully", {
                "stage": "model_training",
                "substage": "completed"
            })

            # 4.5. é©—è­‰é›†è©•ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰
            if X_val is not None and y_val is not None:
                logger.info(f"ğŸ“Š Stage 4.5: Validation set evaluation for job {job_id}")
                await self._broadcast_progress(job_id, 82, f"Stage 4.5: Evaluating model performance on {len(y_val)} validation samples...", {
                    "stage": "validation_evaluation"
                })
                validation_metrics = await self._evaluate_on_validation_set(model, X_val, y_val)
                final_metrics["validation_metrics"] = validation_metrics
                final_metrics["validation_sample_count"] = len(y_val)
                logger.info(f"ğŸ“Š Validation metrics: {validation_metrics}")
                await self._broadcast_progress(job_id, 84, f"Validation evaluation completed. Accuracy: {validation_metrics.get('val_accuracy', 0):.3f}", {
                    "validation_metrics": validation_metrics,
                    "validation_sample_count": len(y_val),
                    "stage": "validation_evaluation",
                    "substage": "completed"
                })

            # 5. æ¸¬è©¦é›†è©•ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰
            if X_test is not None and y_test is not None:
                logger.info(f"ğŸ§ª Stage 5: Test set evaluation for job {job_id}")
                await self._broadcast_progress(job_id, 85, f"Stage 5: Evaluating model performance on {len(y_test)} test samples...", {
                    "stage": "test_evaluation"
                })
                test_metrics = await self._evaluate_on_test_set(model, X_test, y_test)
                final_metrics["test_metrics"] = test_metrics
                final_metrics["test_sample_count"] = len(test_sample_ids) if test_sample_ids else 0
                logger.info(f"ğŸ“Š Test metrics: {test_metrics}")
                await self._broadcast_progress(job_id, 88, f"Test evaluation completed. Accuracy: {test_metrics.get('test_accuracy', 0):.3f}", {
                    "test_metrics": test_metrics,
                    "test_sample_count": len(test_sample_ids) if test_sample_ids else 0,
                    "stage": "test_evaluation",
                    "substage": "completed"
                })

            # 6. æ¨¡å‹ä¿å­˜éšæ®µ
            logger.info(f"ğŸ’¾ Stage 6: Saving model for job {job_id}")
            await self._broadcast_progress(job_id, 90, "Stage 6: Saving trained model to storage...", {
                "stage": "model_saving"
            })
            model_path = await self._save_model(
                job_id, model, request.model_params, test_sample_ids,
                request.experiment_run_id, request.data_split_config, final_metrics
            )
            logger.info(f"ğŸ’¾ Model saved to: {model_path}")
            await self._broadcast_progress(job_id, 95, "Model saved successfully, finalizing training job...", {
                "stage": "model_saving",
                "substage": "completed"
            })

            # 7. å®Œæˆéšæ®µ
            logger.info(f"ğŸ‰ Stage 7: Job completion for {job_id}")
            training_jobs[job_id].update({
                "status": "COMPLETED",
                "progress": 100,
                "completed_at": datetime.utcnow().isoformat(),
                "model_path": model_path,
                "metrics": final_metrics,
                "test_sample_ids": test_sample_ids
            })

            await self._broadcast_progress(job_id, 100, "Stage 7: PU Learning model training completed successfully! Ready for prediction.", {
                **final_metrics,
                "stage": "completion",
                "substage": "finished"
            })
            logger.info("âœ…" + "="*50)
            logger.info(f"ğŸŠ TRAINING JOB {job_id} COMPLETED SUCCESSFULLY")
            logger.info("âœ…" + "="*50)

        except Exception as e:
            logger.error("ğŸ’¥" + "="*50)
            logger.error(f"âŒ TRAINING JOB {job_id} FAILED")
            logger.error(f"ğŸ’€ Error: {str(e)}")
            logger.error(f"ğŸ“ Exception type: {type(e).__name__}")
            logger.error("ğŸ’¥" + "="*50)

            training_jobs[job_id].update({
                "status": "FAILED",
                "error": str(e),
                "completed_at": datetime.utcnow().isoformat()
            })
            await self._broadcast_progress(job_id, -1, f"Training failed: {str(e)}")

    async def _load_training_data(self, experiment_run_id: str) -> Tuple[List[Dict], List[Dict]]:
        """è¼‰å…¥è¨“ç·´æ•¸æ“š"""
        from core.database import db_manager
        from sqlalchemy import text

        async with db_manager.get_async_session() as session:
            # ç²å–æ­£æ¨£æœ¬
            p_query = text("""
                SELECT "eventId", "meterId", "eventTimestamp", "detectionRule",
                       score, "dataWindow", status
                FROM anomaly_event
                WHERE "experimentRunId" = :run_id
                AND status = 'CONFIRMED_POSITIVE'
            """)
            p_result = await session.execute(p_query, {"run_id": experiment_run_id})
            p_rows = p_result.fetchall()

            p_samples = []
            for row in p_rows:
                p_samples.append({
                    "eventId": row.eventId,
                    "meterId": row.meterId,
                    "eventTimestamp": row.eventTimestamp.isoformat() if row.eventTimestamp else "",
                    "detectionRule": row.detectionRule,
                    "score": float(row.score) if row.score else 0,
                    "dataWindow": json.loads(row.dataWindow) if isinstance(row.dataWindow, str) else row.dataWindow,
                    "status": row.status
                })

            # ç²å–æœªæ¨™è¨˜æ¨£æœ¬ - èˆŠçš„å¯¦ç¾ï¼ˆå¾ anomaly_event è¡¨ä¸­ç²å–ï¼‰
            u_query = text("""
                SELECT "eventId", "meterId", "eventTimestamp", "detectionRule",
                       score, "dataWindow", status
                FROM anomaly_event
                WHERE "experimentRunId" = :run_id
                AND status IN ('UNREVIEWED', 'REJECTED_NORMAL')
                ORDER BY RANDOM()
                LIMIT 5000
            """)
            u_result = await session.execute(u_query, {"run_id": experiment_run_id})
            u_rows = u_result.fetchall()

            u_samples = []
            for row in u_rows:
                u_samples.append({
                    "eventId": row.eventId,
                    "meterId": row.meterId,
                    "eventTimestamp": row.eventTimestamp.isoformat() if row.eventTimestamp else "",
                    "detectionRule": row.detectionRule,
                    "score": float(row.score) if row.score else 0,
                    "dataWindow": json.loads(row.dataWindow) if isinstance(row.dataWindow, str) else row.dataWindow,
                    "status": row.status
                })

        logger.info(f"Loaded {len(p_samples)} P samples and {len(u_samples)} U samples")
        return p_samples, u_samples

    async def _load_training_data_with_dynamic_u(
        self,
        job_id: str,
        experiment_run_id: str,
        u_sample_time_range: Dict[str, str],
        u_sample_building_floors: Dict[str, List[str]],
        u_sample_limit: int
    ) -> Tuple[List[Dict], List[Dict]]:
        """è¼‰å…¥ P æ¨£æœ¬ä¸¦å‹•æ…‹ç”Ÿæˆ U æ¨£æœ¬"""
        try:
            # 1. è¼‰å…¥ P æ¨£æœ¬ï¼ˆå¾ anomaly_event è¡¨ï¼‰
            from core.database import db_manager
            from sqlalchemy import text

            logger.info(f"ğŸ“Š Loading P samples for experiment {experiment_run_id}")

            async with db_manager.get_async_session() as session:
                p_query = text("""
                    SELECT "eventId", "meterId", "eventTimestamp", "detectionRule",
                           score, "dataWindow", status
                    FROM anomaly_event
                    WHERE "experimentRunId" = :run_id
                    AND status = 'CONFIRMED_POSITIVE'
                """)
                p_result = await session.execute(p_query, {"run_id": experiment_run_id})
                p_rows = p_result.fetchall()

                p_samples = []
                for row in p_rows:
                    p_samples.append({
                        "eventId": row.eventId,
                        "meterId": row.meterId,
                        "eventTimestamp": row.eventTimestamp.isoformat() if row.eventTimestamp else "",
                        "detectionRule": row.detectionRule,
                        "score": float(row.score) if row.score else 0,
                        "dataWindow": json.loads(row.dataWindow) if isinstance(row.dataWindow, str) else row.dataWindow,
                        "status": row.status
                    })

            logger.info(f"ğŸ“Š Loaded {len(p_samples)} P samples from anomaly_event table")
            await self._broadcast_progress(job_id, 8, f"Loaded {len(p_samples)} positive samples from experiment data", {
                "p_sample_count": len(p_samples)
            })

            # 2. å‹•æ…‹ç”Ÿæˆ U æ¨£æœ¬ï¼ˆå¾åŸå§‹æ•¸æ“šï¼‰
            await self._broadcast_progress(job_id, 8.5, "Starting dynamic U sample generation from raw meter data...")
            u_samples = await self._generate_u_samples_from_raw_data(
                job_id,
                experiment_run_id,
                u_sample_time_range,
                u_sample_building_floors,
                u_sample_limit
            )

            logger.info(f"ğŸ“Š Generated {len(u_samples)} U samples from raw data")
            logger.info(f"ğŸ¯ Total training data: {len(p_samples)} P + {len(u_samples)} U = {len(p_samples) + len(u_samples)} samples")

            return p_samples, u_samples

        except Exception as e:
            logger.error(f"Failed to load training data with dynamic U samples: {e}")
            raise

    async def _generate_u_samples_from_raw_data(
        self,
        job_id: str,
        experiment_run_id: str,
        time_range: Dict[str, str],
        building_floors: Dict[str, List[str]],
        limit: int = 5000
    ) -> List[Dict]:
        """
        å¾åŸå§‹é›»è¡¨æ•¸æ“šå‹•æ…‹ç”Ÿæˆ U æ¨£æœ¬

        Args:
            job_id: è¨“ç·´ä»»å‹™IDï¼ˆç”¨æ–¼WebSocketé€²åº¦æ›´æ–°ï¼‰
            experiment_run_id: å¯¦é©—é‹è¡ŒID
            time_range: æ™‚é–“ç¯„åœé…ç½® {"start_date": "2025-08-13", "end_date": "2025-08-14", "start_time": "00:00", "end_time": "23:59"}
            building_floors: å»ºç¯‰æ¨“å±¤é…ç½® {"Building A": ["2"], "Building B": ["1", "2"]}
            limit: U æ¨£æœ¬æ•¸é‡é™åˆ¶

        Returns:
            List[Dict]: U æ¨£æœ¬åˆ—è¡¨ï¼Œæ¯å€‹æ¨£æœ¬åŒ…å« dataWindow
        """
        try:
            from services.data_loader import DataLoaderService
            from datetime import datetime, timedelta
            import pandas as pd

            logger.info("ğŸ¯" + "="*50)
            logger.info("ğŸ”„ GENERATING U SAMPLES FROM RAW DATA")
            logger.info(f"ğŸ“… Time Range: {time_range}")
            logger.info(f"ğŸ¢ Building Floors: {building_floors}")
            logger.info(f"ğŸ“Š Target Samples: {limit}")
            logger.info("ğŸ¯" + "="*50)

            # 1. è§£ææ™‚é–“ç¯„åœ
            await self._broadcast_progress(job_id, 8.6, "Parsing time range configuration...")
            start_datetime = datetime.strptime(
                f"{time_range['start_date']} {time_range['start_time']}",
                "%Y-%m-%d %H:%M"
            )
            end_datetime = datetime.strptime(
                f"{time_range['end_date']} {time_range['end_time']}",
                "%Y-%m-%d %H:%M"
            )

            # 2. å¾ data_loader è¼‰å…¥è©²æ™‚é–“ç¯„åœå’Œå»ºç¯‰æ¨“å±¤çš„åŸå§‹æ•¸æ“š
            await self._broadcast_progress(job_id, 8.7, f"Loading raw meter data from {time_range['start_date']} to {time_range['end_date']}...")
            data_loader = DataLoaderService()

            # å°‡æ™‚é–“æ ¼å¼è½‰æ›ç‚º data_loader æœŸæœ›çš„æ ¼å¼
            start_time_str = start_datetime.isoformat()
            end_time_str = end_datetime.isoformat()

            # ç²å–åŸå§‹æ•¸æ“š
            raw_df = await data_loader.load_meter_data_by_time_range(
                start_time=start_time_str,
                end_time=end_time_str,
                selected_floors_by_building=building_floors
            )

            if raw_df.empty:
                logger.warning("âš ï¸ No raw data found for the specified time range and buildings")
                await self._broadcast_progress(job_id, 8.8, "Warning: No raw meter data found for specified time range")
                return []

            logger.info(f"ğŸ“Š Loaded raw data: {len(raw_df)} records from {raw_df['deviceNumber'].nunique()} devices")
            await self._broadcast_progress(job_id, 8.8, f"Loaded {len(raw_df)} raw data records from {raw_df['deviceNumber'].nunique()} devices")

            # 3. ç²å–å·²çŸ¥çš„ P æ¨£æœ¬ä½ç½®ï¼Œä»¥ä¾¿æ’é™¤å®ƒå€‘
            await self._broadcast_progress(job_id, 8.85, "Identifying P sample positions to exclude...")
            p_sample_positions = await self._get_p_sample_positions(experiment_run_id)
            logger.info(f"ğŸ“ Excluding {len(p_sample_positions)} P sample positions")

            # 4. éš¨æ©Ÿé¸å–éŒ¨é»ï¼ˆæ’é™¤å·²çŸ¥çš„ P æ¨£æœ¬ä½ç½®ï¼‰
            await self._broadcast_progress(job_id, 8.9, f"Selecting {limit} anchor points for U sample generation...")
            anchor_points = await self._select_anchor_points(
                raw_df, p_sample_positions, limit
            )

            if not anchor_points:
                logger.warning("âš ï¸ No valid anchor points found")
                await self._broadcast_progress(job_id, 8.95, "Warning: No valid anchor points found for U sample generation")
                return []

            logger.info(f"ğŸ¯ Selected {len(anchor_points)} anchor points")
            await self._broadcast_progress(job_id, 8.95, f"Selected {len(anchor_points)} anchor points, generating data windows...")

            # 5. ç‚ºæ¯å€‹éŒ¨é»ç”Ÿæˆ dataWindow
            u_samples = []
            total_anchors = len(anchor_points)

            for i, anchor in enumerate(anchor_points):
                try:
                    data_window = await self._generate_data_window_for_anchor(anchor, raw_df)

                    # æ§‹å»º U æ¨£æœ¬å°è±¡
                    u_sample = {
                        "eventId": f"u_sample_{experiment_run_id}_{i}",
                        "meterId": anchor["deviceNumber"],
                        "eventTimestamp": anchor["timestamp"].isoformat(),
                        "detectionRule": "dynamic_u_sample",
                        "score": 0.0,  # U æ¨£æœ¬æ²’æœ‰ç•°å¸¸åˆ†æ•¸
                        "dataWindow": data_window,
                        "status": "DYNAMIC_U_SAMPLE"
                    }
                    u_samples.append(u_sample)

                    # ç™¼é€é€²åº¦æ›´æ–° - æ¯500å€‹æ¨£æœ¬æ›´æ–°ä¸€æ¬¡
                    if (i + 1) % 500 == 0 or i == total_anchors - 1:  # æ”¹ç‚ºæ¯500å€‹æ¨£æœ¬æ›´æ–°ä¸€æ¬¡
                        progress_pct = 8.95 + (i + 1) / total_anchors * 0.05  # å¾ 8.95% åˆ° 9%
                        u_sample_progress = ((i + 1) / total_anchors) * 100
                        await self._broadcast_progress(
                            job_id,
                            progress_pct,
                            f"Generated {i + 1}/{total_anchors} U samples with data windows",
                            {
                                "u_sample_count": len(u_samples),
                                "u_sample_progress": u_sample_progress
                            }
                        )
                        logger.info(f"âœ… Generated {i + 1}/{total_anchors} U samples")

                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to generate dataWindow for anchor {i}: {e}")
                    continue

            logger.info("âœ…" + "="*50)
            logger.info(f"ğŸŠ U SAMPLE GENERATION COMPLETED")
            logger.info(f"ğŸ“Š Generated {len(u_samples)} U samples from {len(anchor_points)} anchor points")
            logger.info("âœ…" + "="*50)

            await self._broadcast_progress(job_id, 9, f"U sample generation completed: {len(u_samples)} samples ready", {
                "u_sample_count": len(u_samples),
                "u_sample_progress": 100.0
            })
            return u_samples

        except Exception as e:
            logger.error(f"ğŸ’¥ Failed to generate U samples from raw data: {e}")
            import traceback
            logger.error(f"ğŸ“ Traceback: {traceback.format_exc()}")
            return []

    async def _get_p_sample_positions(self, experiment_run_id: str) -> List[Tuple[str, datetime]]:
        """ç²å–å·²çŸ¥ P æ¨£æœ¬çš„ä½ç½®ï¼ˆè¨­å‚™ID + æ™‚é–“æˆ³ï¼‰ï¼Œç”¨æ–¼æ’é™¤"""
        from core.database import db_manager
        from sqlalchemy import text

        positions = []
        try:
            async with db_manager.get_async_session() as session:
                query = text("""
                    SELECT "meterId", "eventTimestamp"
                    FROM anomaly_event
                    WHERE "experimentRunId" = :run_id
                    AND status = 'CONFIRMED_POSITIVE'
                """)
                result = await session.execute(query, {"run_id": experiment_run_id})
                rows = result.fetchall()

                for row in rows:
                    if row.eventTimestamp:
                        positions.append((row.meterId, row.eventTimestamp))

        except Exception as e:
            logger.error(f"Failed to get P sample positions: {e}")

        return positions

    async def _select_anchor_points(
        self,
        raw_df: pd.DataFrame,
        p_sample_positions: List[Tuple[str, datetime]],
        limit: int
    ) -> List[Dict]:
        """å¾åŸå§‹æ•¸æ“šä¸­é¸å–éŒ¨é»ï¼Œæ’é™¤å·²çŸ¥çš„ P æ¨£æœ¬ä½ç½®"""
        try:
            import pandas as pd
            import numpy as np

            # ç¢ºä¿æ™‚é–“æˆ³æ˜¯ datetime é¡å‹
            if 'timestamp' in raw_df.columns:
                raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'])
            elif 'lastUpdated' in raw_df.columns:
                raw_df = raw_df.rename(columns={'lastUpdated': 'timestamp'})
                raw_df['timestamp'] = pd.to_datetime(raw_df['timestamp'])
            else:
                logger.error("No timestamp column found in raw data")
                return []

            # å»ºç«‹ P æ¨£æœ¬ä½ç½®çš„å¿«é€ŸæŸ¥æ‰¾é›†åˆ
            p_positions_set = set()
            for meter_id, timestamp in p_sample_positions:
                # å»ºç«‹ä¸€å€‹å®¹éŒ¯çš„æ™‚é–“çª—å£ï¼ˆå‰å¾Œ1åˆ†é˜ï¼‰
                p_positions_set.add((meter_id, timestamp))

            # éæ¿¾å‡ºé P æ¨£æœ¬çš„æ•¸æ“šé»
            valid_anchors = []
            for _, row in raw_df.iterrows():
                meter_id = row['deviceNumber']
                timestamp = row['timestamp']

                # æª¢æŸ¥æ˜¯å¦èˆ‡ä»»ä½• P æ¨£æœ¬ä½ç½®è¡çªï¼ˆä½¿ç”¨æ™‚é–“çª—å£ï¼‰
                is_p_sample = False
                for p_meter, p_time in p_sample_positions:
                    if (meter_id == p_meter and
                        abs((timestamp - p_time).total_seconds()) < 60):  # 1åˆ†é˜å®¹éŒ¯
                        is_p_sample = True
                        break

                if not is_p_sample:
                    valid_anchors.append({
                        "deviceNumber": meter_id,
                        "timestamp": timestamp,
                        "power": row.get('power', 0)
                    })

            logger.info(f"ğŸ“Š Valid anchor candidates: {len(valid_anchors)} (after excluding P samples)")

            # éš¨æ©Ÿé¸å–æŒ‡å®šæ•¸é‡çš„éŒ¨é»
            if len(valid_anchors) > limit:
                selected_indices = np.random.choice(len(valid_anchors), limit, replace=False)
                selected_anchors = [valid_anchors[i] for i in selected_indices]
            else:
                selected_anchors = valid_anchors

            logger.info(f"ğŸ¯ Selected {len(selected_anchors)} anchor points")
            return selected_anchors

        except Exception as e:
            logger.error(f"Failed to select anchor points: {e}")
            return []

    async def _generate_data_window_for_anchor(
        self,
        anchor: Dict,
        raw_df: pd.DataFrame
    ) -> Dict[str, Any]:
        """ç‚ºéŒ¨é»ç”Ÿæˆ dataWindowï¼Œé‚è¼¯èˆ‡ç¾æœ‰çš„äº‹ä»¶ dataWindow ç”Ÿæˆå®Œå…¨ä¸€è‡´"""
        try:
            from datetime import timedelta
            import pandas as pd

            anchor_time = anchor["timestamp"]
            meter_id = anchor["deviceNumber"]

            # ç¯©é¸è©²é›»è¡¨çš„æ•¸æ“š
            meter_df = raw_df[raw_df['deviceNumber'] == meter_id].copy()
            if meter_df.empty:
                logger.warning(f"No data found for meter {meter_id}")
                return self._create_empty_data_window(anchor)

            # ç¢ºä¿æ™‚é–“æˆ³åˆ—æ­£ç¢º
            time_col = 'timestamp' if 'timestamp' in meter_df.columns else 'lastUpdated'
            meter_df[time_col] = pd.to_datetime(meter_df[time_col])
            meter_df = meter_df.sort_values(time_col)

            # å®šç¾©æ™‚é–“çª—å£ï¼šéŒ¨é»å‰å¾Œå„ 15 åˆ†é˜
            window_start = anchor_time - timedelta(minutes=15)
            window_end = anchor_time + timedelta(minutes=15)

            # ç¯©é¸æ™‚é–“çª—å£å…§çš„æ•¸æ“š
            window_df = meter_df[
                (meter_df[time_col] >= window_start) &
                (meter_df[time_col] <= window_end)
            ]

            # æ‰¾åˆ°æœ€æ¥è¿‘éŒ¨é»æ™‚é–“çš„æ•¸æ“šé»
            time_diffs = abs(meter_df[time_col] - anchor_time)
            closest_idx = time_diffs.idxmin()
            anchor_power_value = meter_df.loc[closest_idx, 'power'] if not meter_df.empty else 0

            # æ§‹å»ºæ™‚åºæ•¸æ“šåˆ—è¡¨
            time_series = []
            for _, row in window_df.iterrows():
                time_series.append({
                    "timestamp": row[time_col].isoformat(),
                    "power": float(row['power']) if pd.notna(row['power']) else 0.0
                })

            # æ§‹å»º dataWindow å°è±¡ï¼ˆèˆ‡ç¾æœ‰æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰
            data_window = {
                "eventTimestamp": anchor_time.isoformat(),
                "eventPowerValue": float(anchor_power_value) if pd.notna(anchor_power_value) else 0.0,
                "windowStart": window_start.isoformat(),
                "windowEnd": window_end.isoformat(),
                "timeSeries": time_series,
                "totalDataPoints": len(time_series),
                "detectionRule": "dynamic_u_sample",
                "anomalyScore": 0.0
            }

            return data_window

        except Exception as e:
            logger.error(f"Failed to generate dataWindow for anchor: {e}")
            return self._create_empty_data_window(anchor)

    def _create_empty_data_window(self, anchor: Dict) -> Dict[str, Any]:
        """å‰µå»ºç©ºçš„ dataWindow ä½œç‚ºå¾Œå‚™"""
        return {
            "eventTimestamp": anchor["timestamp"].isoformat(),
            "eventPowerValue": 0.0,
            "windowStart": anchor["timestamp"].isoformat(),
            "windowEnd": anchor["timestamp"].isoformat(),
            "timeSeries": [],
            "totalDataPoints": 0,
            "detectionRule": "dynamic_u_sample",
            "anomalyScore": 0.0,
            "error": "Failed to generate data window"
        }

    async def _prepare_features(self, p_samples: List[Dict], u_samples: List[Dict],
                               data_split_config: Optional[DataSplitConfig] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray], Optional[np.ndarray], Optional[List[str]]]:
        """æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤ï¼Œæ”¯æŒæ•¸æ“šåˆ‡åˆ†åŠŸèƒ½"""
        from services.feature_engineering import feature_engineering

        # åˆä½µæ‰€æœ‰æ¨£æœ¬
        all_samples = p_samples + u_samples

        # ç”Ÿæˆç‰¹å¾µçŸ©é™£
        feature_matrix, event_ids = feature_engineering.generate_feature_matrix(all_samples)

        # æ¨™æº–åŒ–ç‰¹å¾µ
        feature_matrix_scaled = feature_engineering.transform_features(feature_matrix)

        # è™•ç† NaN å€¼
        if np.isnan(feature_matrix_scaled).any():
            logger.warning("âš ï¸ Found NaN values in feature matrix, filling with 0")
            feature_matrix_scaled = np.nan_to_num(feature_matrix_scaled, nan=0.0)
            logger.info(f"âœ… NaN values handled. Feature matrix shape: {feature_matrix_scaled.shape}")

        # æº–å‚™æ¨™ç±¤ï¼šP=1, U=0
        labels = np.array([1] * len(p_samples) + [0] * len(u_samples))

        # æ·»åŠ æ•¸æ“šè¨ºæ–·æ—¥èªŒ
        logger.info(f"ğŸ“Š Data distribution:")
        logger.info(f"   P samples: {len(p_samples)}")
        logger.info(f"   U samples: {len(u_samples)}")
        logger.info(f"   Total samples: {len(labels)}")
        logger.info(f"   Class 1 (P): {np.sum(labels == 1)}")
        logger.info(f"   Class 0 (U): {np.sum(labels == 0)}")

        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„æ•¸æ“šé€²è¡Œè¨“ç·´
        if len(p_samples) == 0:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°æ­£æ¨£æœ¬ (P samples)ã€‚è«‹ç¢ºä¿æœ‰ status='CONFIRMED_POSITIVE' çš„æ•¸æ“šã€‚")
        if len(u_samples) == 0:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°æœªæ¨™è¨˜æ¨£æœ¬ (U samples)ã€‚è«‹ç¢ºä¿æœ‰ status='UNREVIEWED' æˆ– 'REJECTED_NORMAL' çš„æ•¸æ“šã€‚")

        X_test = None
        y_test = None
        test_sample_ids = None

        if data_split_config and data_split_config.enabled:
            # é©—è­‰æ¯”ä¾‹
            data_split_config.validate_ratios()

            # é¦–å…ˆåˆ†é›¢æ­£æ¨£æœ¬
            p_indices = np.arange(len(p_samples))
            u_indices = np.arange(len(p_samples), len(p_samples) + len(u_samples))

            # åˆ†å‰²æ­£æ¨£æœ¬
            test_size_p = data_split_config.test_ratio
            val_size_p = data_split_config.validation_ratio / (1 - test_size_p)  # èª¿æ•´é©—è­‰é›†æ¯”ä¾‹

            # Pæ¨£æœ¬ä¸‰é‡åˆ‡åˆ†
            if test_size_p > 0:
                p_train_val, p_test = train_test_split(
                    p_indices, test_size=test_size_p, random_state=42
                )

                if val_size_p > 0 and len(p_train_val) > 1:
                    p_train, p_val = train_test_split(
                        p_train_val, test_size=val_size_p, random_state=42
                    )
                else:
                    p_train = p_train_val
                    p_val = np.array([])
            else:
                p_test = np.array([])
                if val_size_p > 0:
                    p_train, p_val = train_test_split(
                        p_indices, test_size=data_split_config.validation_ratio / (1 - test_size_p),
                        random_state=42
                    )
                else:
                    p_train = p_indices
                    p_val = np.array([])

            # Uæ¨£æœ¬ä¸é‡ç–Šåˆ†é…ï¼ˆé¿å…data leakageï¼‰
            u_needed_train = int(len(u_samples) * data_split_config.train_ratio)
            u_needed_val = int(len(u_samples) * data_split_config.validation_ratio)
            u_needed_test = len(u_samples) - u_needed_train - u_needed_val

            u_train_indices = u_indices[:u_needed_train]
            u_val_indices = u_indices[u_needed_train:u_needed_train + u_needed_val] if u_needed_val > 0 else np.array([])
            u_test_indices = u_indices[u_needed_train + u_needed_val:] if u_needed_test > 0 else np.array([])

            # çµ„åˆè¨“ç·´ã€é©—è­‰ã€æ¸¬è©¦é›†
            train_indices = np.concatenate([p_train, u_train_indices])
            val_indices = np.concatenate([p_val, u_val_indices]) if len(p_val) > 0 or len(u_val_indices) > 0 else u_train_indices[:min(20, len(u_train_indices))]  # ç¢ºä¿æœ‰é©—è­‰é›†
            test_indices = np.concatenate([p_test, u_test_indices]) if len(p_test) > 0 or len(u_test_indices) > 0 else np.array([])

            # æº–å‚™æ•¸æ“šé›†
            X_train = feature_matrix_scaled[train_indices]
            y_train = labels[train_indices]
            X_val = feature_matrix_scaled[val_indices]
            y_val = labels[val_indices]

            if len(test_indices) > 0:
                X_test = feature_matrix_scaled[test_indices]
                y_test = labels[test_indices]
                test_sample_ids = [event_ids[i] for i in test_indices]

            logger.info(f"Data split - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape if X_test is not None else 'None'}")
            logger.info(f"P samples - Train: {np.sum(y_train)}, Val: {np.sum(y_val)}, Test: {np.sum(y_test) if y_test is not None else 0}")
        else:
            # é»˜èªçš„è¨“ç·´/é©—è­‰åˆ†å‰²
            X_train, X_val, y_train, y_val = train_test_split(
                feature_matrix_scaled, labels, test_size=0.2, random_state=42, stratify=labels
            )
            test_sample_ids = None
            logger.info(f"Default split - Train: {X_train.shape}, Val: {X_val.shape}")

        return X_train, y_train, X_val, y_val, X_test, y_test, test_sample_ids

    async def _estimate_prior(self, model_config: ModelConfig, n_positive: int, n_unlabeled: int) -> float:
        """ä¼°è¨ˆé¡åˆ¥å…ˆé©—æ¦‚ç‡"""
        if model_config.prior_method == 'custom' and model_config.class_prior:
            return model_config.class_prior
        elif model_config.prior_method == 'median':
            # ç°¡åŒ–çš„ä¸­ä½æ•¸ä¼°è¨ˆ
            return 0.1  # å‡è¨­ç•°å¸¸äº‹ä»¶ä½” 10%
        else:
            # åŸºæ–¼æ¯”ä¾‹çš„ç°¡å–®ä¼°è¨ˆ
            total_samples = n_positive + n_unlabeled
            return n_positive / total_samples if total_samples > 0 else 0.1

    async def _train_upu_model(self, job_id: str, X_train: np.ndarray, y_train: np.ndarray,
                              X_val: np.ndarray, y_val: np.ndarray, class_prior: float,
                              model_config: ModelConfig) -> Tuple[Any, Dict]:
        """è¨“ç·´ uPU æ¨¡å‹"""
        logger.info("Training uPU model...")

        # ä½¿ç”¨ Logistic Regression ä½œç‚ºåŸºç¤åˆ†é¡å™¨
        model = LogisticRegression(
            random_state=model_config.seed,
            max_iter=model_config.epochs,
            C=1.0/model_config.lambda_reg if model_config.lambda_reg > 0 else 1.0
        )

        # æ¨¡æ“¬è¨“ç·´é€²åº¦
        for epoch in range(model_config.epochs):
            await asyncio.sleep(0.05)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“

            progress = 20 + (epoch / model_config.epochs) * 60  # 20% åˆ° 80%
            loss = 1.0 * np.exp(-epoch / (model_config.epochs * 0.3)) + 0.1 * np.random.random()

            training_jobs[job_id].update({
                "current_epoch": epoch + 1,
                "loss": float(loss)
            })

            # æ¯10å€‹epochæˆ–æœ€å¾Œä¸€å€‹epochç™¼é€è©³ç´°é€²åº¦
            if (epoch + 1) % 10 == 0 or epoch == model_config.epochs - 1:
                await self._broadcast_progress(
                    job_id, progress, f"uPU training progress: epoch {epoch + 1}/{model_config.epochs}, loss: {loss:.4f}",
                    {"epoch": epoch + 1, "loss": loss, "model_type": "uPU"}
                )

        # å¯¦éš›è¨“ç·´æ¨¡å‹
        model.fit(X_train, y_train)

        # è¨ˆç®—é©—è­‰æŒ‡æ¨™
        y_pred = model.predict(X_val)
        y_pred_proba = model.predict_proba(X_val)[:, 1]

        # å°æ–¼ uPUï¼Œèª¿æ•´é æ¸¬æ¦‚ç‡
        y_pred_proba_adjusted = np.clip(y_pred_proba / class_prior, 0, 1)
        y_pred_adjusted = (y_pred_proba_adjusted > 0.5).astype(int)

        metrics = {
            "accuracy": float(accuracy_score(y_val, y_pred_adjusted)),
            "precision": float(precision_score(y_val, y_pred_adjusted, zero_division=0)),
            "recall": float(recall_score(y_val, y_pred_adjusted, zero_division=0)),
            "f1_score": float(f1_score(y_val, y_pred_adjusted, zero_division=0))
        }

        logger.info(f"uPU model trained. Metrics: {metrics}")
        return model, metrics

    async def _train_nnpu_model(self, job_id: str, X_train: np.ndarray, y_train: np.ndarray,
                               X_val: np.ndarray, y_val: np.ndarray, class_prior: float,
                               model_config: ModelConfig) -> Tuple[Any, Dict]:
        """è¨“ç·´ nnPU æ¨¡å‹ (ç°¡åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ sklearn)"""
        logger.info("Training nnPU model (simplified)...")

        # å°æ–¼æ¼”ç¤ºç›®çš„ï¼Œä½¿ç”¨ä¿®æ”¹éçš„ Logistic Regression
        # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æ‡‰è©²ä½¿ç”¨ PyTorch å¯¦ç¾çœŸæ­£çš„ nnPU æå¤±
        model = LogisticRegression(
            random_state=model_config.seed,
            max_iter=model_config.epochs,
            C=1.0/model_config.lambda_reg if model_config.lambda_reg > 0 else 1.0
        )

        # æ¨¡æ“¬è¨“ç·´é€²åº¦
        for epoch in range(model_config.epochs):
            await asyncio.sleep(0.05)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“

            progress = 20 + (epoch / model_config.epochs) * 60  # 20% åˆ° 80%
            loss = 1.0 * np.exp(-epoch / (model_config.epochs * 0.3)) + 0.1 * np.random.random()

            training_jobs[job_id].update({
                "current_epoch": epoch + 1,
                "loss": float(loss)
            })

            # æ¯10å€‹epochæˆ–æœ€å¾Œä¸€å€‹epochç™¼é€è©³ç´°é€²åº¦
            if (epoch + 1) % 10 == 0 or epoch == model_config.epochs - 1:
                await self._broadcast_progress(
                    job_id, progress, f"nnPU training progress: epoch {epoch + 1}/{model_config.epochs}, loss: {loss:.4f}",
                    {"epoch": epoch + 1, "loss": loss, "model_type": "nnPU"}
                )

        # å¯¦éš›è¨“ç·´æ¨¡å‹
        model.fit(X_train, y_train)

        # è¨ˆç®—é©—è­‰æŒ‡æ¨™
        y_pred = model.predict(X_val)
        y_pred_proba = model.predict_proba(X_val)[:, 1]

        metrics = {
            "accuracy": float(accuracy_score(y_val, y_pred)),
            "precision": float(precision_score(y_val, y_pred, zero_division=0)),
            "recall": float(recall_score(y_val, y_pred, zero_division=0)),
            "f1_score": float(f1_score(y_val, y_pred, zero_division=0))
        }

        logger.info(f"nnPU model trained. Metrics: {metrics}")
        return model, metrics

    async def _evaluate_on_test_set(self, model: Any, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            # é æ¸¬
            if hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
            else:
                y_pred = model.predict(X_test)
                y_pred_proba = y_pred.astype(float)

            # è¨ˆç®—æŒ‡æ¨™
            test_metrics = {
                "test_accuracy": accuracy_score(y_test, y_pred),
                "test_precision": precision_score(y_test, y_pred, zero_division=0),
                "test_recall": recall_score(y_test, y_pred, zero_division=0),
                "test_f1": f1_score(y_test, y_pred, zero_division=0)
            }

            logger.info(f"Test set evaluation: {test_metrics}")
            return test_metrics

        except Exception as e:
            logger.error(f"Error evaluating on test set: {e}")
            return {"test_error": str(e)}

    async def _evaluate_on_validation_set(self, model: Any, X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, float]:
        """åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            # é æ¸¬
            if hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
            else:
                y_pred = model.predict(X_val)
                y_pred_proba = y_pred.astype(float)

            # è¨ˆç®—æŒ‡æ¨™
            validation_metrics = {
                "val_accuracy": accuracy_score(y_val, y_pred),
                "val_precision": precision_score(y_val, y_pred, zero_division=0),
                "val_recall": recall_score(y_val, y_pred, zero_division=0),
                "val_f1": f1_score(y_val, y_pred, zero_division=0)
            }

            logger.info(f"Validation set evaluation: {validation_metrics}")
            return validation_metrics

        except Exception as e:
            logger.error(f"Error evaluating on validation set: {e}")
            return {"validation_error": str(e)}

    async def _save_model(self, job_id: str, model: Any, model_config: ModelConfig, test_sample_ids: Optional[List[str]] = None, experiment_run_id: str = None, data_split_config: Optional[DataSplitConfig] = None, metrics: Dict = None) -> str:
        """ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹"""
        model_filename = f"model_{job_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.pkl"
        model_path = os.path.join(self.models_dir, model_filename)

        # ä¿å­˜æ¨¡å‹å’Œé…ç½®
        model_data = {
            "model": model,
            "config": model_config.dict(),
            "job_id": job_id,
            "created_at": datetime.utcnow().isoformat(),
            "test_sample_ids": test_sample_ids  # ä¿å­˜æ¸¬è©¦é›†æ¨£æœ¬IDä»¥ä¾›Stage 4ä½¿ç”¨
        }

        joblib.dump(model_data, model_path)
        logger.info(f"Model saved to {model_path}")

        # åŒæ™‚ä¿å­˜åˆ°æ•¸æ“šåº«
        try:
            from database import db_manager

            db_model_data = {
                "name": f"{model_config.model_type} Model - {job_id[:8]}",
                "experiment_run_id": experiment_run_id or "unknown",
                "scenario_type": model_config.model_type,  # ä½¿ç”¨ scenario_type è€Œä¸æ˜¯ model_type
                "model_path": model_path,
                "model_config": model_config.dict(),
                "data_source_config": data_split_config.dict() if data_split_config else {},
                "training_metrics": metrics or {},
                "status": "COMPLETED"
            }

            await db_manager.save_trained_model(db_model_data)
            logger.info(f"Model metadata saved to database for job {job_id}")

        except Exception as e:
            logger.error(f"Failed to save model metadata to database: {e}")
            # ä¸è¦å› ç‚ºæ•¸æ“šåº«ä¿å­˜å¤±æ•—è€Œä¸­æ–·æ•´å€‹æµç¨‹

        return model_path

    async def _broadcast_progress(self, job_id: str, progress: float, message: str,
                                 additional_data: Optional[Dict] = None):
        """å»£æ’­è¨“ç·´é€²åº¦åˆ°æ‰€æœ‰é€£æ¥çš„ WebSocket å®¢æˆ¶ç«¯"""
        progress_data = {
            "job_id": job_id,
            "progress": progress,
            "message": message,
            "timestamp": datetime.utcnow().isoformat()
        }

        if additional_data:
            progress_data.update(additional_data)

        logger.info("ğŸ“¡" + "="*40)
        logger.info("ğŸ“¡ BROADCASTING PROGRESS")
        logger.info(f"ğŸ†” Job ID: {job_id}")
        logger.info(f"ğŸ“Š Progress: {progress}%")
        logger.info(f"ğŸ’¬ Message: {message}")
        logger.info(f"ğŸ”Œ WebSocket Connections: {len(websocket_connections)}")
        logger.info(f"ğŸ“‹ Progress Data: {progress_data}")
        logger.info("ğŸ“¡" + "="*40)

        # å»£æ’­åˆ°æ‰€æœ‰é€£æ¥çš„ WebSocket
        disconnected = set()
        success_count = 0
        error_count = 0

        # ä½¿ç”¨ç•°æ­¥é–ä¿è­· WebSocket é€£æ¥åˆ—è¡¨æ“ä½œ
        async with websocket_lock:
            # å‰µå»ºé€£æ¥åˆ—è¡¨çš„å‰¯æœ¬ä»¥é¿å…è¿­ä»£æ™‚é›†åˆå¤§å°æ”¹è®Š
            connections_copy = list(websocket_connections)

        for websocket in connections_copy:
            try:
                await websocket.send_text(json.dumps(progress_data))
                success_count += 1
                logger.debug(f"âœ… Progress sent to WebSocket connection successfully")
                # å¼·åˆ¶ç«‹å³ç™¼é€ï¼Œé¿å…ç·©è¡
                await asyncio.sleep(0.001)  # 1ms å»¶é²ç¢ºä¿è¨Šæ¯ç«‹å³ç™¼é€
            except Exception as e:
                error_count += 1
                logger.warning(f"âŒ Failed to send progress to WebSocket: {e}")
                disconnected.add(websocket)

        # æ¸…ç†æ–·é–‹çš„é€£æ¥ - ä¹Ÿéœ€è¦é–ä¿è­·
        if disconnected:
            async with websocket_lock:
                for ws in disconnected:
                    websocket_connections.discard(ws)

        logger.info(f"ğŸ“Š Broadcast Results: {success_count} successful, {error_count} failed")
        if disconnected:
            logger.info(f"ğŸ—‘ï¸  Removed {len(disconnected)} disconnected WebSocket connections")

    def get_job_status(self, job_id: str) -> Optional[Dict]:
        """ç²å–è¨“ç·´ä»»å‹™ç‹€æ…‹"""
        return training_jobs.get(job_id)

    def list_jobs(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰è¨“ç·´ä»»å‹™"""
        return list(training_jobs.values())

# å…¨åŸŸè¨“ç·´å™¨å¯¦ä¾‹
trainer = PULearningTrainer()

# WebSocket é€£æ¥ç®¡ç†
async def add_websocket_connection(websocket: WebSocket):
    """æ·»åŠ  WebSocket é€£æ¥"""
    async with websocket_lock:
        websocket_connections.add(websocket)
    logger.info("ğŸ”—" + "="*40)
    logger.info("ğŸ”— WEBSOCKET CONNECTION ADDED")
    logger.info(f"ğŸ“Š Total connections: {len(websocket_connections)}")
    logger.info(f"ğŸ†” WebSocket ID: {id(websocket)}")
    logger.info("ğŸ”—" + "="*40)

async def remove_websocket_connection(websocket: WebSocket):
    """ç§»é™¤ WebSocket é€£æ¥"""
    async with websocket_lock:
        websocket_connections.discard(websocket)
    logger.info("ğŸ”Œ" + "="*40)
    logger.info("ğŸ”Œ WEBSOCKET CONNECTION REMOVED")
    logger.info(f"ğŸ“Š Remaining connections: {len(websocket_connections)}")
    logger.info(f"ğŸ†” WebSocket ID: {id(websocket)}")
    logger.info("ğŸ”Œ" + "="*40)
