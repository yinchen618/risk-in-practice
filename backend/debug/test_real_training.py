#!/usr/bin/env python3
"""
æ¸¬è©¦å¾Œç«¯æ˜¯å¦çœŸçš„åœ¨é€²è¡Œè¨“ç·´
"""

import requests
import json
import time

def test_real_training():
    """æ¸¬è©¦å¾Œç«¯æ˜¯å¦çœŸçš„åœ¨é€²è¡Œè¨“ç·´"""
    
    url = "http://localhost:8000/api/pu-learning/run-simulation"
    
    # æ¸¬è©¦é…ç½® - ä½¿ç”¨ä¸åŒçš„åƒæ•¸ä¾†è§€å¯Ÿè®ŠåŒ–
    test_configs = [
        {
            "name": "é…ç½®1 - nnPU é«˜æ–¯åˆ†å¸ƒ",
            "data": {
                "algorithm": "nnPU",
                "data_params": {
                    "distribution": "gaussian",
                    "dims": 8,
                    "n_p": 50,
                    "n_u": 300,
                    "prior": 0.3
                },
                "model_params": {
                    "activation": "relu",
                    "n_epochs": 50,
                    "learning_rate": 0.01,
                    "hidden_dim": 200,
                    "weight_decay": 0.005
                }
            }
        },
        {
            "name": "é…ç½®2 - uPU é›™æœˆåˆ†å¸ƒ",
            "data": {
                "algorithm": "uPU",
                "data_params": {
                    "distribution": "two_moons",
                    "dims": 2,
                    "n_p": 50,
                    "n_u": 300,
                    "prior": 0.3
                },
                "model_params": {
                    "activation": "relu",
                    "n_epochs": 50,
                    "learning_rate": 0.01,
                    "hidden_dim": 100,
                    "weight_decay": 0.0
                }
            }
        },
        {
            "name": "é…ç½®3 - nnPU èºæ—‹åˆ†å¸ƒ",
            "data": {
                "algorithm": "nnPU",
                "data_params": {
                    "distribution": "spiral",
                    "dims": 2,
                    "n_p": 50,
                    "n_u": 300,
                    "prior": 0.3
                },
                "model_params": {
                    "activation": "tanh",
                    "n_epochs": 100,
                    "learning_rate": 0.005,
                    "hidden_dim": 150,
                    "weight_decay": 0.001
                }
            }
        }
    ]
    
    print("ğŸ§ª æ¸¬è©¦å¾Œç«¯æ˜¯å¦çœŸçš„åœ¨é€²è¡Œè¨“ç·´")
    print("="*60)
    
    results = []
    
    for i, config in enumerate(test_configs, 1):
        print(f"\nğŸ“‹ æ¸¬è©¦ {i}: {config['name']}")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            response = requests.post(url, json=config['data'], timeout=60)
            
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"â±ï¸  éŸ¿æ‡‰æ™‚é–“: {duration:.2f} ç§’")
            print(f"ğŸ“Š ç‹€æ…‹ç¢¼: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                
                # æª¢æŸ¥æ•¸æ“šæ˜¯å¦æ¯æ¬¡éƒ½ä¸åŒ
                viz = result['visualization']
                metrics = result['metrics']
                
                print(f"âœ… æˆåŠŸç²å–çµæœ")
                print(f"   â€¢ æ­£æ¨£æœ¬æ•¸é‡: {len(viz['p_samples'])}")
                print(f"   â€¢ æœªæ¨™è¨˜æ¨£æœ¬æ•¸é‡: {len(viz['u_samples'])}")
                print(f"   â€¢ æ±ºç­–é‚Šç•Œé»æ•¸: {len(viz['decision_boundary'])}")
                print(f"   â€¢ ä¼°è¨ˆå…ˆé©—: {metrics['estimated_prior']:.4f}")
                print(f"   â€¢ éŒ¯èª¤ç‡: {metrics['error_rate']:.4f}")
                print(f"   â€¢ è¨“ç·´éŒ¯èª¤ç‡: {metrics['training_error_rate']:.4f}")
                print(f"   â€¢ é¢¨éšªæ›²ç·šé•·åº¦: {len(metrics['risk_curve'])}")
                
                # æª¢æŸ¥å‰å¹¾å€‹æ•¸æ“šé»
                if len(viz['p_samples']) > 0:
                    print(f"   â€¢ ç¬¬ä¸€å€‹æ­£æ¨£æœ¬: {viz['p_samples'][0]}")
                if len(viz['u_samples']) > 0:
                    print(f"   â€¢ ç¬¬ä¸€å€‹æœªæ¨™è¨˜æ¨£æœ¬: {viz['u_samples'][0]}")
                
                # æª¢æŸ¥é¢¨éšªæ›²ç·š
                if len(metrics['risk_curve']) > 0:
                    first_risk = metrics['risk_curve'][0]
                    last_risk = metrics['risk_curve'][-1]
                    print(f"   â€¢ é¢¨éšªæ›²ç·š: {first_risk} -> {last_risk}")
                
                results.append({
                    'config': config['name'],
                    'duration': duration,
                    'data': result,
                    'success': True
                })
                
            else:
                print(f"âŒ è«‹æ±‚å¤±æ•—: {response.status_code}")
                print(f"éŒ¯èª¤ä¿¡æ¯: {response.text}")
                results.append({
                    'config': config['name'],
                    'duration': duration,
                    'error': response.text,
                    'success': False
                })
                
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            print(f"âŒ è«‹æ±‚ç•°å¸¸: {e}")
            results.append({
                'config': config['name'],
                'duration': duration,
                'error': str(e),
                'success': False
            })
    
    # åˆ†æçµæœ
    print(f"\nğŸ“Š æ¸¬è©¦çµæœåˆ†æ")
    print("="*60)
    
    successful_results = [r for r in results if r['success']]
    
    if len(successful_results) >= 2:
        print("ğŸ” æª¢æŸ¥æ•¸æ“šå¤šæ¨£æ€§:")
        
        # æ¯”è¼ƒä¸åŒé…ç½®çš„çµæœ
        for i in range(len(successful_results)):
            for j in range(i + 1, len(successful_results)):
                result1 = successful_results[i]
                result2 = successful_results[j]
                
                print(f"\næ¯”è¼ƒ {result1['config']} vs {result2['config']}:")
                
                # æ¯”è¼ƒæ­£æ¨£æœ¬
                p1 = result1['data']['visualization']['p_samples'][0]
                p2 = result2['data']['visualization']['p_samples'][0]
                print(f"   â€¢ ç¬¬ä¸€å€‹æ­£æ¨£æœ¬: {p1} vs {p2}")
                
                # æ¯”è¼ƒæœªæ¨™è¨˜æ¨£æœ¬
                u1 = result1['data']['visualization']['u_samples'][0]
                u2 = result2['data']['visualization']['u_samples'][0]
                print(f"   â€¢ ç¬¬ä¸€å€‹æœªæ¨™è¨˜æ¨£æœ¬: {u1} vs {u2}")
                
                # æ¯”è¼ƒæŒ‡æ¨™
                prior1 = result1['data']['metrics']['estimated_prior']
                prior2 = result2['data']['metrics']['estimated_prior']
                error1 = result1['data']['metrics']['error_rate']
                error2 = result2['data']['metrics']['error_rate']
                
                print(f"   â€¢ ä¼°è¨ˆå…ˆé©—: {prior1:.4f} vs {prior2:.4f}")
                print(f"   â€¢ éŒ¯èª¤ç‡: {error1:.4f} vs {error2:.4f}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰é¡¯è‘—å·®ç•°
                if abs(prior1 - prior2) > 0.01 or abs(error1 - error2) > 0.01:
                    print(f"   âœ… æ•¸æ“šæœ‰é¡¯è‘—å·®ç•° - èªªæ˜çœŸçš„åœ¨è¨“ç·´")
                else:
                    print(f"   âš ï¸  æ•¸æ“šå·®ç•°å¾ˆå° - å¯èƒ½ä½¿ç”¨å›ºå®šæ•¸æ“š")
    
    # æª¢æŸ¥éŸ¿æ‡‰æ™‚é–“
    avg_duration = sum(r['duration'] for r in successful_results) / len(successful_results)
    print(f"\nâ±ï¸  å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_duration:.2f} ç§’")
    
    if avg_duration > 1.0:
        print("âœ… éŸ¿æ‡‰æ™‚é–“è¼ƒé•·ï¼Œèªªæ˜å¯èƒ½åœ¨é€²è¡ŒçœŸå¯¦è¨“ç·´")
    else:
        print("âš ï¸  éŸ¿æ‡‰æ™‚é–“å¾ˆçŸ­ï¼Œå¯èƒ½ä½¿ç”¨é è¨ˆç®—æ•¸æ“š")
    
    # ç¸½çµ
    print(f"\nğŸ¯ ç¸½çµ:")
    print(f"   â€¢ æˆåŠŸæ¸¬è©¦: {len(successful_results)}/{len(test_configs)}")
    print(f"   â€¢ å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_duration:.2f} ç§’")
    
    if len(successful_results) >= 2:
        print("   â€¢ æ•¸æ“šå¤šæ¨£æ€§: éœ€è¦é€²ä¸€æ­¥æª¢æŸ¥")
    else:
        print("   â€¢ æ•¸æ“šå¤šæ¨£æ€§: ç„¡æ³•ç¢ºå®š")

if __name__ == "__main__":
    test_real_training() 
