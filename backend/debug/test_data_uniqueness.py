#!/usr/bin/env python3
"""
æ¸¬è©¦æ¯æ¬¡è«‹æ±‚æ˜¯å¦ç”¢ç”Ÿä¸åŒçš„æ•¸æ“š
"""

import requests
import json
import time
import hashlib

def test_data_uniqueness():
    """æ¸¬è©¦æ•¸æ“šçš„å”¯ä¸€æ€§"""
    
    url = "http://localhost:8000/api/pu-learning/run-simulation"
    
    # ä½¿ç”¨ç›¸åŒçš„é…ç½®ï¼Œä½†å¤šæ¬¡è«‹æ±‚
    test_config = {
        "algorithm": "nnPU",
        "data_params": {
            "distribution": "two_moons",
            "dims": 2,
            "n_p": 50,
            "n_u": 300,
            "prior": 0.3
        },
        "model_params": {
            "activation": "relu",
            "n_epochs": 50,
            "learning_rate": 0.01,
            "hidden_dim": 100,
            "weight_decay": 0.0
        }
    }
    
    print("ğŸ§ª æ¸¬è©¦æ•¸æ“šå”¯ä¸€æ€§ - ç›¸åŒé…ç½®å¤šæ¬¡è«‹æ±‚")
    print("="*60)
    
    results = []
    
    for i in range(5):
        print(f"\nğŸ“‹ è«‹æ±‚ {i+1}/5")
        print("-" * 30)
        
        start_time = time.time()
        
        try:
            response = requests.post(url, json=test_config, timeout=60)
            
            end_time = time.time()
            duration = end_time - start_time
            
            if response.status_code == 200:
                result = response.json()
                
                # è¨ˆç®—æ•¸æ“šçš„å“ˆå¸Œå€¼
                viz_data = result['visualization']
                p_samples = viz_data['p_samples']
                u_samples = viz_data['u_samples']
                
                # å‰µå»ºæ•¸æ“šçš„å“ˆå¸Œå€¼
                data_str = json.dumps(p_samples + u_samples, sort_keys=True)
                data_hash = hashlib.md5(data_str.encode()).hexdigest()[:8]
                
                print(f"â±ï¸  éŸ¿æ‡‰æ™‚é–“: {duration:.2f} ç§’")
                print(f"ğŸ”¢ æ•¸æ“šå“ˆå¸Œ: {data_hash}")
                print(f"ğŸ“Š ç¬¬ä¸€å€‹æ­£æ¨£æœ¬: {p_samples[0]}")
                print(f"ğŸ“Š ç¬¬ä¸€å€‹æœªæ¨™è¨˜æ¨£æœ¬: {u_samples[0]}")
                print(f"ğŸ“ˆ ä¼°è¨ˆå…ˆé©—: {result['metrics']['estimated_prior']:.4f}")
                print(f"ğŸ“ˆ éŒ¯èª¤ç‡: {result['metrics']['error_rate']:.4f}")
                
                results.append({
                    'request_num': i+1,
                    'duration': duration,
                    'data_hash': data_hash,
                    'first_p': p_samples[0],
                    'first_u': u_samples[0],
                    'prior': result['metrics']['estimated_prior'],
                    'error_rate': result['metrics']['error_rate'],
                    'success': True
                })
                
            else:
                print(f"âŒ è«‹æ±‚å¤±æ•—: {response.status_code}")
                results.append({
                    'request_num': i+1,
                    'error': response.text,
                    'success': False
                })
                
        except Exception as e:
            print(f"âŒ è«‹æ±‚ç•°å¸¸: {e}")
            results.append({
                'request_num': i+1,
                'error': str(e),
                'success': False
            })
        
        # ç­‰å¾…ä¸€ä¸‹å†ç™¼é€ä¸‹ä¸€å€‹è«‹æ±‚
        time.sleep(1)
    
    # åˆ†æçµæœ
    print(f"\nğŸ“Š å”¯ä¸€æ€§åˆ†æ")
    print("="*60)
    
    successful_results = [r for r in results if r['success']]
    
    if len(successful_results) >= 2:
        print("ğŸ” æª¢æŸ¥æ•¸æ“šå“ˆå¸Œå€¼:")
        hashes = [r['data_hash'] for r in successful_results]
        unique_hashes = set(hashes)
        
        print(f"   â€¢ ç¸½è«‹æ±‚æ•¸: {len(successful_results)}")
        print(f"   â€¢ å”¯ä¸€å“ˆå¸Œå€¼: {len(unique_hashes)}")
        print(f"   â€¢ å“ˆå¸Œå€¼åˆ—è¡¨: {hashes}")
        
        if len(unique_hashes) == len(successful_results):
            print("   âœ… æ¯æ¬¡è«‹æ±‚éƒ½ç”¢ç”Ÿä¸åŒçš„æ•¸æ“šï¼")
        else:
            print("   âš ï¸  ç™¼ç¾é‡è¤‡çš„æ•¸æ“šï¼")
            print("   ğŸ“‹ é‡è¤‡çš„å“ˆå¸Œå€¼:")
            for hash_val in hashes:
                if hashes.count(hash_val) > 1:
                    print(f"      â€¢ {hash_val} å‡ºç¾ {hashes.count(hash_val)} æ¬¡")
        
        print(f"\nğŸ” æª¢æŸ¥ç¬¬ä¸€å€‹æ•¸æ“šé»:")
        for i, result in enumerate(successful_results):
            print(f"   è«‹æ±‚ {result['request_num']}: P={result['first_p']}, U={result['first_u']}")
        
        print(f"\nğŸ” æª¢æŸ¥æŒ‡æ¨™è®ŠåŒ–:")
        priors = [r['prior'] for r in successful_results]
        error_rates = [r['error_rate'] for r in successful_results]
        
        print(f"   ä¼°è¨ˆå…ˆé©—ç¯„åœ: {min(priors):.4f} - {max(priors):.4f}")
        print(f"   éŒ¯èª¤ç‡ç¯„åœ: {min(error_rates):.4f} - {max(error_rates):.4f}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é¡¯è‘—è®ŠåŒ–
        prior_variance = max(priors) - min(priors)
        error_variance = max(error_rates) - min(error_rates)
        
        if prior_variance > 0.01 or error_variance > 0.01:
            print("   âœ… æŒ‡æ¨™æœ‰é¡¯è‘—è®ŠåŒ– - ç¢ºèªçœŸå¯¦è¨“ç·´")
        else:
            print("   âš ï¸  æŒ‡æ¨™è®ŠåŒ–å¾ˆå° - å¯èƒ½ä½¿ç”¨å›ºå®šæ•¸æ“š")
    
    # ç¸½çµ
    print(f"\nğŸ¯ ç¸½çµ:")
    print(f"   â€¢ æˆåŠŸè«‹æ±‚: {len(successful_results)}/5")
    if len(successful_results) >= 2:
        unique_count = len(set(r['data_hash'] for r in successful_results))
        print(f"   â€¢ å”¯ä¸€æ•¸æ“šé›†: {unique_count}/{len(successful_results)}")
        if unique_count == len(successful_results):
            print("   âœ… ç¢ºèªï¼šæ¯æ¬¡è«‹æ±‚éƒ½ç”¢ç”Ÿä¸åŒçš„æ•¸æ“š")
        else:
            print("   âš ï¸  è­¦å‘Šï¼šç™¼ç¾é‡è¤‡æ•¸æ“š")

if __name__ == "__main__":
    test_data_uniqueness() 
