"""
å€™é¸äº‹ä»¶ API è·¯ç”± - è™•ç†ç•°å¸¸æª¢æ¸¬å’Œå€™é¸äº‹ä»¶ç”Ÿæˆ
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, Query
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import logging
import uuid
import asyncio
from datetime import datetime, date, timezone

from services.data_loader import data_loader
from services.anomaly_rules import anomaly_rules
from services.anomaly_service import anomaly_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/candidates", tags=["Candidate Events"])

# Enhanced Pydantic Models
class CandidateCalculationRequest(BaseModel):
    # Top-level filter - çµ±ä¸€ä½¿ç”¨ datetime
    start_datetime: datetime
    end_datetime: datetime
    
    # Value-based rules
    z_score_threshold: Optional[float] = 3.0
    spike_percentage: Optional[float] = 200.0
    
    # Time-based rules
    min_event_duration_minutes: Optional[int] = 30
    detect_holiday_pattern: Optional[bool] = True
    
    # Data integrity rules
    max_time_gap_minutes: Optional[int] = 60
    
    # Peer comparison rules
    peer_agg_window_minutes: Optional[int] = 5
    peer_exceed_percentage: Optional[float] = 150.0

	# Floor filter
    selected_floors_by_building: Optional[Dict[str, List[str]]] = None

class CandidateCalculationResponse(BaseModel):
    success: bool
    candidate_count: int
    message: str
    parameters_used: Dict[str, Any]
    processing_details: Optional[Dict[str, Any]] = None
    stats: Optional[Dict[str, Any]] = None

class CandidateGenerationRequest(CandidateCalculationRequest):
    """Same parameters as calculation but for actual generation"""
    pass

class CandidateGenerationResponse(BaseModel):
    success: bool
    task_id: str
    message: str
    estimated_completion_time: str

class TaskStatusResponse(BaseModel):
    success: bool
    task_id: str
    status: str  # "pending", "running", "completed", "failed"
    progress: Optional[float] = None
    message: str
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

# å…¨åŸŸä»»å‹™è¿½è¹¤
running_tasks: Dict[str, Dict[str, Any]] = {}

@router.post("/calculate", response_model=CandidateCalculationResponse)
async def calculate_candidate_events(request: CandidateCalculationRequest):
    """
    Calculate candidate event count (Stage 1 parameter adjustment)
    Fast calculation without actually generating events
    """
    try:
        logger.info(f"Calculating candidate event count with parameters: {request.dict()}")
        
        # æ™‚é–“åƒæ•¸è™•ç† - ä¿æŒå°ç£æœ¬åœ°æ™‚é–“ï¼Œä¸è½‰æ›ç‚º UTC
        # å› ç‚ºè³‡æ–™åº«ä¸­çš„ timestamp æ¬„ä½å„²å­˜çš„æ˜¯å°ç£æœ¬åœ°æ™‚é–“
        def to_local_naive(dt: datetime) -> datetime:
            if dt.tzinfo is not None:
                # ç§»é™¤æ™‚å€è³‡è¨Šä½†ä¿æŒåŸå§‹æ™‚é–“å€¼ï¼ˆå°ç£æ™‚é–“ï¼‰
                return dt.replace(tzinfo=None)
            return dt

        start_dt = to_local_naive(request.start_datetime)
        end_dt = to_local_naive(request.end_datetime)

        # è¨˜éŒ„æœå°‹æ™‚é–“ç¯„åœ
        time_range_str = f"æ™‚é–“ç¯„åœ: {start_dt.strftime('%Y-%m-%d %H:%M:%S')} ~ {end_dt.strftime('%Y-%m-%d %H:%M:%S')}"
        
        logger.info(f"=== å€™é¸äº‹ä»¶è¨ˆç®—é–‹å§‹ ===")
        logger.info(f"ğŸ“… {time_range_str}")
        
        # è¨˜éŒ„æ¨“å±¤éæ¿¾æ¢ä»¶
        if request.selected_floors_by_building:
            logger.info(f"ğŸ¢ æ¨“å±¤éæ¿¾ - æŒ‰å»ºç¯‰åˆ†çµ„: {request.selected_floors_by_building}")
        else:
            logger.info(f"ğŸ¢ æ¨“å±¤éæ¿¾: ç„¡éæ¿¾æ¢ä»¶ï¼ˆæœå°‹å…¨éƒ¨è¨­å‚™ï¼‰")

        # Get raw data with datetime filtering and floor filtering
        raw_df = await data_loader.get_raw_dataframe(
            start_datetime=start_dt,
            end_datetime=end_dt,
            selected_floors_by_building=request.selected_floors_by_building,
        )
        
        # è¨˜éŒ„æ‰¾åˆ°çš„é›»è¡¨è¨­å‚™è©³æƒ…
        if not raw_df.empty and 'deviceNumber' in raw_df.columns:
            unique_devices = raw_df['deviceNumber'].unique()
            device_room_mapping = data_loader.get_device_room_mapping()
            
            logger.info(f"ğŸ” æ‰¾åˆ°é›»è¡¨è¨­å‚™ ({len(unique_devices)}å€‹):")
            for device_id in sorted(unique_devices):
                room_info = device_room_mapping.get(device_id, {"building": "æœªçŸ¥", "room": "æœªçŸ¥", "floor": "æœªçŸ¥"})
                room_display = f"{room_info['building']} {room_info['floor']}F {room_info['room']}"
                logger.info(f"  ğŸ“ {device_id} â†’ {room_display}")
        else:
            logger.info(f"âŒ æœªæ‰¾åˆ°ä»»ä½•é›»è¡¨è¨­å‚™æ•¸æ“š")
        
        if raw_df.empty:
            logger.info(f"âŒ æŒ‡å®šæ™‚é–“ç¯„åœå…§ç„¡æ•¸æ“š")
            return CandidateCalculationResponse(
                success=True,
                candidate_count=0,
                message="No data available for the specified time range",
                parameters_used=request.dict()
            )
        
        # è¨˜éŒ„æ•¸æ“šçµ±è¨ˆ
        total_records = len(raw_df)
        time_span = raw_df['timestamp'].max() - raw_df['timestamp'].min()
        logger.info(f"ğŸ“Š æ•¸æ“šçµ±è¨ˆ: {total_records:,} ç­†è¨˜éŒ„ï¼Œæ™‚é–“è·¨åº¦: {time_span}")
        logger.info(f"ğŸ“ˆ åŠŸç‡ç¯„åœ: {raw_df['power'].min():.2f}W ~ {raw_df['power'].max():.2f}W (å¹³å‡: {raw_df['power'].mean():.2f}W)")
        
        # Convert parameters for the detection engine
        detection_params = {
            'z_score_threshold': request.z_score_threshold,
            'spike_percentage': request.spike_percentage,
            'min_event_duration_minutes': request.min_event_duration_minutes,
            'detect_holiday_pattern': request.detect_holiday_pattern,
            'max_time_gap_minutes': request.max_time_gap_minutes,
            'peer_agg_window_minutes': request.peer_agg_window_minutes,
            'peer_exceed_percentage': request.peer_exceed_percentage,
        }
        
        # Calculate candidate event count using enhanced rules
        candidate_count = await anomaly_rules.calculate_candidate_count_enhanced(
            raw_df, detection_params
        )

        # Collect richer statistics for display
        stats = await anomaly_rules.calculate_candidate_stats_enhanced(raw_df, detection_params)
        
        # Prepare processing details
        processing_details = {
            'data_range': {
                'start_datetime': start_dt.isoformat(),
                'end_datetime': end_dt.isoformat(),
                'total_records': len(raw_df),
                'unique_devices': raw_df['deviceNumber'].nunique() if 'deviceNumber' in raw_df.columns else 0
            },
            'rules_applied': [
                'Value-based Z-Score Analysis',
                'Power Spike Detection',
                'Time-based Pattern Analysis',
                'Data Integrity Gap Detection',
                'Peer Comparison Analysis'
            ]
        }
        
        logger.info(f"Estimated candidate event count: {candidate_count}")
        
        return CandidateCalculationResponse(
            success=True,
            candidate_count=candidate_count,
            message=f"Estimated {candidate_count} candidate events will be generated",
            parameters_used=detection_params,
            processing_details=processing_details,
            stats=stats
        )
        
    except Exception as e:
        logger.error(f"Failed to calculate candidate events: {e}")
        raise HTTPException(status_code=500, detail=f"Calculation failed: {str(e)}")

@router.post("/generate", response_model=CandidateGenerationResponse)
async def generate_candidate_events(
    request: CandidateGenerationRequest,
    background_tasks: BackgroundTasks
):
    """
    ç”Ÿæˆä¸¦å„²å­˜å€™é¸äº‹ä»¶ï¼ˆStage 1 "Proceed" æŒ‰éˆ•ï¼‰
    ä½¿ç”¨èƒŒæ™¯ä»»å‹™é€²è¡Œç•°æ­¥è™•ç†
    """
    try:
        logger.info(f"é–‹å§‹ç”Ÿæˆå€™é¸äº‹ä»¶ - æ¡ˆä¾‹ç ”ç©¶æ¨¡å¼")
        
        # ç”Ÿæˆä»»å‹™ID
        task_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»å‹™ç‹€æ…‹
        running_tasks[task_id] = {
            'status': 'pending',
            'progress': 0.0,
            'message': 'æ­£åœ¨æº–å‚™ç”Ÿæˆå€™é¸äº‹ä»¶...',
            'start_time': datetime.utcnow(),
            'result': None,
            'error': None
        }
        
        # å•Ÿå‹•èƒŒæ™¯ä»»å‹™
        background_tasks.add_task(
            _generate_events_background,
            task_id,
            request.dict()
        )
        
        estimated_time = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        
        return CandidateGenerationResponse(
            success=True,
            task_id=task_id,
            message="å€™é¸äº‹ä»¶ç”Ÿæˆä»»å‹™å·²å•Ÿå‹•",
            estimated_completion_time=estimated_time
        )
        
    except Exception as e:
        logger.error(f"å•Ÿå‹•å€™é¸äº‹ä»¶ç”Ÿæˆå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"å•Ÿå‹•ä»»å‹™å¤±æ•—: {str(e)}")

@router.get("/generate/{task_id}/status", response_model=TaskStatusResponse)
async def get_generation_status(task_id: str):
    """ç²å–å€™é¸äº‹ä»¶ç”Ÿæˆä»»å‹™ç‹€æ…‹"""
    if task_id not in running_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task_info = running_tasks[task_id]
    
    return TaskStatusResponse(
        success=True,
        task_id=task_id,
        status=task_info['status'],
        progress=task_info.get('progress'),
        message=task_info['message'],
        result=task_info.get('result'),
        error=task_info.get('error')
    )

async def _generate_events_background(task_id: str, params: Dict[str, Any]):
    """èƒŒæ™¯ä»»å‹™ï¼šç”Ÿæˆå€™é¸äº‹ä»¶"""
    try:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        running_tasks[task_id]['status'] = 'running'
        running_tasks[task_id]['message'] = 'æ­£åœ¨è¼‰å…¥åŸå§‹æ•¸æ“š...'
        running_tasks[task_id]['progress'] = 0.1
        
        # æ™‚é–“åƒæ•¸è™•ç†
        def to_local_naive(dt: datetime) -> datetime:
            if dt.tzinfo is not None:
                return dt.replace(tzinfo=None)
            return dt

        start_dt = to_local_naive(datetime.fromisoformat(params['start_datetime'].replace('Z', ''))) if 'start_datetime' in params else None
        end_dt = to_local_naive(datetime.fromisoformat(params['end_datetime'].replace('Z', ''))) if 'end_datetime' in params else None
        
        # ç²å–åŸå§‹æ•¸æ“š - ä½¿ç”¨ç›¸åŒçš„æ™‚é–“ç¯„åœå’Œæ¨“å±¤éæ¿¾æ¢ä»¶
        raw_df = await data_loader.get_raw_dataframe(
            start_datetime=start_dt,
            end_datetime=end_dt,
            selected_floors_by_building=params.get('selected_floors_by_building')
        )
        
        if raw_df.empty:
            raise Exception("æ²’æœ‰å¯ç”¨çš„åŸå§‹æ•¸æ“š")
        
        running_tasks[task_id]['message'] = 'æ­£åœ¨åŸ·è¡Œç•°å¸¸æª¢æ¸¬...'
        running_tasks[task_id]['progress'] = 0.3
        
        # æå–æª¢æ¸¬åƒæ•¸
        detection_params = {
            'zscore_threshold': params.get('zscore_threshold', 3.0),
            'time_window_hours': params.get('time_window_hours', 24),
            'min_duration_minutes': params.get('min_duration_minutes', 30),
            'power_threshold_multiplier': params.get('power_threshold_multiplier', 2.0),
            'night_hour_start': params.get('night_hour_start', 23),
            'night_hour_end': params.get('night_hour_end', 6)
        }
        
        # åŸ·è¡Œç•°å¸¸æª¢æ¸¬
        candidate_events_df = anomaly_rules.get_candidate_events(raw_df, detection_params)
        
        running_tasks[task_id]['message'] = 'æ­£åœ¨å„²å­˜å€™é¸äº‹ä»¶åˆ°è³‡æ–™åº«...'
        running_tasks[task_id]['progress'] = 0.7
        
        # å„²å­˜äº‹ä»¶åˆ°è³‡æ–™åº«
        saved_count = 0
        
        for _, event in candidate_events_df.iterrows():
            try:
                # å»ºç«‹æ•¸æ“šçª—å£ï¼ˆæ¨¡æ“¬ç›¸é—œæ™‚é–“é»çš„æ•¸æ“šï¼‰
                event_time = event['timestamp']
                device_data = raw_df[
                    (raw_df['deviceNumber'] == event['deviceNumber']) &
                    (abs((raw_df['timestamp'] - event_time).dt.total_seconds()) <= 3600)  # Â±1å°æ™‚
                ].sort_values('timestamp')
                
                data_window = {
                    'timestamps': device_data['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S').tolist(),
                    'values': device_data['power'].tolist()
                }
                
                # å„²å­˜åˆ°è³‡æ–™åº«ï¼ˆæ¡ˆä¾‹ç ”ç©¶æ¨¡å¼ä¸ä½¿ç”¨çµ„ç¹”IDï¼‰
                await anomaly_service.create_anomaly_event(
                    event_id=f"AUTO_{event['deviceNumber']}_{event_time.strftime('%Y%m%d_%H%M%S')}",
                    meter_id=event['deviceNumber'],
                    event_timestamp=event_time,
                    detection_rule=event['detection_rule'],
                    score=float(event['score']),
                    data_window=data_window,
                    organization_id=None  # æ¡ˆä¾‹ç ”ç©¶ä¸ä½¿ç”¨çµ„ç¹”ID
                )
                
                saved_count += 1
                
            except Exception as e:
                logger.warning(f"å„²å­˜äº‹ä»¶å¤±æ•—: {e}")
                continue
        
        # ä»»å‹™å®Œæˆ
        running_tasks[task_id]['status'] = 'completed'
        running_tasks[task_id]['message'] = f'æˆåŠŸç”Ÿæˆä¸¦å„²å­˜ {saved_count} å€‹å€™é¸äº‹ä»¶'
        running_tasks[task_id]['progress'] = 1.0
        running_tasks[task_id]['result'] = {
            'events_generated': saved_count,
            'total_candidates': len(candidate_events_df),
            'completion_time': datetime.utcnow().isoformat(),
            'detection_parameters': detection_params
        }
        
        logger.info(f"å€™é¸äº‹ä»¶ç”Ÿæˆå®Œæˆ: {task_id}, å„²å­˜äº† {saved_count} å€‹äº‹ä»¶")
        
    except Exception as e:
        # ä»»å‹™å¤±æ•—
        error_msg = str(e)
        logger.error(f"å€™é¸äº‹ä»¶ç”Ÿæˆå¤±æ•—: {task_id}, éŒ¯èª¤: {error_msg}")
        
        running_tasks[task_id]['status'] = 'failed'
        running_tasks[task_id]['message'] = f'ç”Ÿæˆå¤±æ•—: {error_msg}'
        running_tasks[task_id]['error'] = error_msg
        running_tasks[task_id]['progress'] = 0.0

@router.delete("/tasks/{task_id}")
async def cleanup_task(task_id: str):
    """æ¸…ç†å·²å®Œæˆçš„ä»»å‹™"""
    if task_id in running_tasks:
        del running_tasks[task_id]
        return {"success": True, "message": "ä»»å‹™å·²æ¸…ç†"}
    else:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

@router.get("/tasks")
async def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»å‹™ç‹€æ…‹"""
    return {
        "success": True,
        "tasks": {
            task_id: {
                'status': info['status'],
                'message': info['message'],
                'progress': info.get('progress', 0),
                'start_time': info['start_time'].isoformat()
            }
            for task_id, info in running_tasks.items()
        }
    }
