#!/usr/bin/env python3
"""
ğŸ¯ é‡æ–°æ¨™è¨˜ç•°å¸¸æ¨£æœ¬å·¥å…·

æ ¹æ“šæŒ‡å®šçš„ç•°å¸¸æª¢æ¸¬æ¢ä»¶é‡æ–°æ¨™è¨˜ AnalysisReadyData ä¸­çš„æ¨£æœ¬ï¼Œ
ä¸¦æ›´æ–°æ¯å€‹ AnalysisDataset çš„ positiveLabels è¨ˆæ•¸ã€‚

æª¢æ¸¬æ¢ä»¶ï¼š
- Z-Score Threshold: 3.5Ïƒ
- Spike Threshold: 400%
- Min Duration: 90min
- Max Time Gap: 120min

Author: AI Assistant
Date: 2025-08-27
"""

import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import logging
from pathlib import Path

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('relabel_positive_samples.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç•°å¸¸æª¢æ¸¬æ¢ä»¶ - åŒ¹é… backend API çš„åƒæ•¸
DETECTION_CONFIG = {
    'z_score_threshold': 2,        # Z-Score é–¾å€¼ (èˆ‡ API ä¸€è‡´)
    'spike_threshold': 150,        # å³°å€¼é–¾å€¼ 150% (èˆ‡ API ä¸€è‡´)
    'min_duration_minutes': 20,    # æœ€å°æŒçºŒæ™‚é–“
    'max_time_gap_minutes': 120,   # æœ€å¤§æ™‚é–“é–“éš”
    'jump_threshold_ratio': 0.5    # åŠŸç‡è·³èºé–¾å€¼ 50%
}

def get_database_path():
    """ç²å–æ•¸æ“šåº«è·¯å¾‘"""
    # å˜—è©¦å¤šå€‹å¯èƒ½çš„è·¯å¾‘
    possible_paths = [
        Path("backend/database/prisma/pu_practice.db"),
        Path(__file__).parent / "backend" / "database" / "prisma" / "pu_practice.db",
        Path("backend/database/pu_practice.db"),
        Path(__file__).parent / "backend" / "database" / "pu_practice.db",
    ]

    for db_path in possible_paths:
        if db_path.exists():
            logger.info(f"æ‰¾åˆ°æ•¸æ“šåº«: {db_path}")
            return str(db_path)

    logger.error("æœªæ‰¾åˆ°æ•¸æ“šåº«æ–‡ä»¶ï¼Œå˜—è©¦äº†ä»¥ä¸‹è·¯å¾‘:")
    for path in possible_paths:
        logger.error(f"  - {path}")
    raise FileNotFoundError("æ•¸æ“šåº«æ–‡ä»¶ä¸å­˜åœ¨")

def api_compatible_anomaly_detection(df, config):
    """
    ä½¿ç”¨èˆ‡ API å®Œå…¨ä¸€è‡´çš„ç•°å¸¸æª¢æ¸¬é‚è¼¯
    åŒ¹é… backend/routes/case_study_v2.py ä¸­çš„ç®—æ³•

    Args:
        df: DataFrame with timestamp and wattage_total columns
        config: Detection configuration

    Returns:
        Boolean Series indicating anomalies
    """
    logger.info(f"é–‹å§‹APIå…¼å®¹ç•°å¸¸æª¢æ¸¬ï¼Œå…± {len(df)} æ¢è¨˜éŒ„")

    if len(df) == 0:
        return pd.Series([], dtype=bool)

    # æŒ‰æ™‚é–“æ’åº
    df = df.sort_values('timestamp').copy()
    wattage_total = df['wattage_total'].values

    # å»ºç«‹ç•°å¸¸æ¨™è¨˜æ•¸çµ„ï¼Œé¿å…é‡è¤‡è¨ˆç®—
    anomaly_mask = np.zeros(len(wattage_total), dtype=bool)

    z_score_threshold = config['z_score_threshold']
    spike_threshold = config['spike_threshold']

    logger.info(f"æ•¸æ“šé›†ç¸½æ•¸æ“šé»: {len(wattage_total)}")

    # 1. Z-score ç•°å¸¸æª¢æ¸¬
    z_anomaly_count = 0
    if len(wattage_total) > 1:
        # è¨ˆç®—ç¸½åŠŸç‡çš„ Z-score
        mean_power = np.mean(wattage_total)
        std_power = np.std(wattage_total)

        if std_power > 0:
            z_scores = np.abs((wattage_total - mean_power) / std_power)
            z_anomaly_mask = z_scores > z_score_threshold
            z_anomaly_count = int(np.sum(z_anomaly_mask))

            # æ·»åŠ åˆ°ç¸½ç•°å¸¸æ¨™è¨˜
            anomaly_mask |= z_anomaly_mask

            logger.info(f"Z-score ç•°å¸¸ (é–¾å€¼ {z_score_threshold}): {z_anomaly_count}")

    # 2. åŠŸç‡å³°å€¼æª¢æ¸¬ï¼ˆä½¿ç”¨ç›¸å°é–¾å€¼ï¼‰
    # è¨ˆç®—ç›¸å°æ–¼å¹³å‡åŠŸç‡çš„å³°å€¼é–¾å€¼
    mean_power = np.mean(wattage_total)
    # spike_threshold æ˜¯ç™¾åˆ†æ¯”ï¼Œè½‰æ›ç‚ºçµ•å°é–¾å€¼
    absolute_spike_threshold = mean_power * (1 + spike_threshold / 100.0)

    spike_anomaly_mask = wattage_total > absolute_spike_threshold
    spike_anomaly_count = int(np.sum(spike_anomaly_mask))

    # æ·»åŠ åˆ°ç¸½ç•°å¸¸æ¨™è¨˜
    anomaly_mask |= spike_anomaly_mask

    logger.info(f"åŠŸç‡å³°å€¼ (ç›¸å°é–¾å€¼ {spike_threshold}%, çµ•å°é–¾å€¼ {absolute_spike_threshold:.1f}W): {spike_anomaly_count}")

    # 3. é€£çºŒæ€§æª¢æ¸¬ - æª¢æ¸¬æ•¸æ“šè·³èº
    jump_anomaly_count = 0
    if len(wattage_total) > 1:
        power_diff = np.abs(np.diff(wattage_total))
        diff_threshold = np.mean(wattage_total) * 0.5  # 50% åŠŸç‡è®ŠåŒ–é–¾å€¼

        # ç‚º diff å‰µå»ºèˆ‡åŸæ•¸çµ„é•·åº¦ç›¸åŒçš„ maskï¼ˆæœ€å¾Œä¸€å€‹å…ƒç´ è¨­ç‚º Falseï¼‰
        jump_anomaly_mask = np.zeros(len(wattage_total), dtype=bool)
        jump_anomaly_mask[:-1] = power_diff > diff_threshold

        jump_anomaly_count = int(np.sum(jump_anomaly_mask))

        # æ·»åŠ åˆ°ç¸½ç•°å¸¸æ¨™è¨˜
        anomaly_mask |= jump_anomaly_mask

        logger.info(f"åŠŸç‡è·³èºç•°å¸¸: {jump_anomaly_count}")

    # è¨ˆç®—è¯é›†å¾Œçš„ç¸½ç•°å¸¸æ•¸é‡
    total_anomalies = int(np.sum(anomaly_mask))

    logger.info(f"Z-score: {z_anomaly_count}, å³°å€¼: {spike_anomaly_count}, è·³èº: {jump_anomaly_count}")
    logger.info(f"è¯é›†å¾Œç¸½ç•°å¸¸: {total_anomalies}, æŒçºŒæ™‚é–“èª¿æ•´å¾Œ: {total_anomalies}")

    # 4. æœ€å°äº‹ä»¶æŒçºŒæ™‚é–“éæ¿¾
    # å°‡é€£çºŒçš„ç•°å¸¸é»èšåˆæˆäº‹ä»¶
    # ç°¡åŒ–å¯¦ç¾ï¼šæ ¹æ“šæœ€å°æŒçºŒæ™‚é–“è¦æ±‚ä¾†æ¸›å°‘å€™é¸æ•¸é‡

    # æŒçºŒæ™‚é–“éæ¿¾ï¼šè¼ƒé•·çš„æŒçºŒæ™‚é–“è¦æ±‚æ‡‰è©²ç”¢ç”Ÿè¼ƒå°‘çš„å€™é¸
    # å‡è¨­æ¯åˆ†é˜ä¸€å€‹æ•¸æ“šé»ï¼ŒåŸºæº–ç‚º 30 åˆ†é˜
    min_event_duration = config['min_duration_minutes']
    duration_factor = min(1.0, 30.0 / max(1, min_event_duration))
    dataset_candidates = int(total_anomalies * duration_factor)

    # ç¢ºä¿å€™é¸æ•¸é‡ä¸è¶…éåŸå§‹æ•¸æ“šé»æ•¸é‡çš„åˆç†æ¯”ä¾‹ï¼ˆæœ€å¤š 10%ï¼‰
    max_reasonable_candidates = max(1, int(len(wattage_total) * 0.1))
    dataset_candidates = min(dataset_candidates, max_reasonable_candidates)

    logger.info(f"æœ€çµ‚å€™é¸æ•¸: {dataset_candidates}")

    # å¦‚æœéœ€è¦é¸æ“‡éƒ¨åˆ†ç•°å¸¸é»
    if dataset_candidates < total_anomalies and dataset_candidates > 0:
        # æ‰¾åˆ°ç•°å¸¸çš„ç´¢å¼•
        anomaly_indices = np.where(anomaly_mask)[0]

        # éš¨æ©Ÿé¸æ“‡éƒ¨åˆ†ç•°å¸¸é»
        np.random.seed(42)  # ç¢ºä¿å¯é‡ç¾æ€§
        selected_indices = np.random.choice(
            anomaly_indices,
            size=dataset_candidates,
            replace=False
        )

        # å‰µå»ºæ–°çš„ç•°å¸¸æ¨™è¨˜
        final_anomaly_mask = np.zeros(len(wattage_total), dtype=bool)
        final_anomaly_mask[selected_indices] = True

        return pd.Series(final_anomaly_mask, index=df.index)

    return pd.Series(anomaly_mask, index=df.index)

def calculate_z_score_anomalies(df, threshold=2):
    """
    è¨ˆç®— Z-Score ç•°å¸¸ - èˆ‡ API é‚è¼¯å®Œå…¨ä¸€è‡´

    Args:
        df: DataFrame with wattage_total column
        threshold: Z-Score é–¾å€¼

    Returns:
        Boolean Series indicating anomalies
    """
    if len(df) < 3:
        return pd.Series([False] * len(df), index=df.index)

    wattage_total = df['wattage_total'].values

    # è¨ˆç®— Z-Score (èˆ‡ API å®Œå…¨ä¸€è‡´)
    mean_power = np.mean(wattage_total)
    std_power = np.std(wattage_total)

    if std_power == 0:
        return pd.Series([False] * len(df), index=df.index)

    z_scores = np.abs((wattage_total - mean_power) / std_power)
    z_anomaly_mask = z_scores > threshold

    return pd.Series(z_anomaly_mask, index=df.index)

def calculate_spike_anomalies(df, threshold=150):
    """
    è¨ˆç®—åŠŸç‡å³°å€¼ç•°å¸¸ - èˆ‡ API é‚è¼¯å®Œå…¨ä¸€è‡´

    Args:
        df: DataFrame with wattage_total column
        threshold: ç™¾åˆ†æ¯”é–¾å€¼ (150 = 150%)

    Returns:
        Boolean Series indicating spike anomalies
    """
    if len(df) < 2:
        return pd.Series([False] * len(df), index=df.index)

    wattage_total = df['wattage_total'].values

    # è¨ˆç®—ç›¸å°æ–¼å¹³å‡åŠŸç‡çš„å³°å€¼é–¾å€¼ (èˆ‡ API å®Œå…¨ä¸€è‡´)
    mean_power = np.mean(wattage_total)
    absolute_spike_threshold = mean_power * (1 + threshold / 100.0)

    spike_anomaly_mask = wattage_total > absolute_spike_threshold

    return pd.Series(spike_anomaly_mask, index=df.index)

def calculate_jump_anomalies(df, threshold_ratio=0.5):
    """
    è¨ˆç®—åŠŸç‡è·³èºç•°å¸¸ - èˆ‡ API é‚è¼¯å®Œå…¨ä¸€è‡´

    Args:
        df: DataFrame with wattage_total column
        threshold_ratio: è·³èºé–¾å€¼æ¯”ä¾‹ (0.5 = 50%)

    Returns:
        Boolean Series indicating jump anomalies
    """
    if len(df) < 2:
        return pd.Series([False] * len(df), index=df.index)

    wattage_total = df['wattage_total'].values

    # è¨ˆç®—é€£çºŒå·®å€¼ (èˆ‡ API å®Œå…¨ä¸€è‡´)
    power_diff = np.abs(np.diff(wattage_total))
    diff_threshold = np.mean(wattage_total) * threshold_ratio  # 50% åŠŸç‡è®ŠåŒ–é–¾å€¼

    # å‰µå»ºèˆ‡åŸæ•¸çµ„é•·åº¦ç›¸åŒçš„ maskï¼ˆæœ€å¾Œä¸€å€‹å…ƒç´ è¨­ç‚º Falseï¼‰
    jump_anomaly_mask = np.zeros(len(wattage_total), dtype=bool)
    jump_anomaly_mask[:-1] = power_diff > diff_threshold

    return pd.Series(jump_anomaly_mask, index=df.index)

def group_consecutive_anomalies(anomaly_series, timestamps, min_duration_minutes=90, max_gap_minutes=120):
    """
    å°‡é€£çºŒçš„ç•°å¸¸é»åˆ†çµ„ï¼Œè€ƒæ…®æœ€å°æŒçºŒæ™‚é–“å’Œæœ€å¤§é–“éš”

    Args:
        anomaly_series: Boolean series of anomalies
        timestamps: Series of timestamps
        min_duration_minutes: æœ€å°æŒçºŒæ™‚é–“
        max_gap_minutes: æœ€å¤§å…è¨±é–“éš”

    Returns:
        Boolean Series with grouped anomalies
    """
    if not anomaly_series.any():
        return pd.Series([False] * len(anomaly_series), index=anomaly_series.index)

    result = pd.Series([False] * len(anomaly_series), index=anomaly_series.index)

    # æ‰¾åˆ°ç•°å¸¸é»çš„ç´¢å¼•
    anomaly_indices = anomaly_series[anomaly_series].index.tolist()

    if not anomaly_indices:
        return result

    # å°‡æ™‚é–“æˆ³è½‰æ›ç‚º datetime
    if isinstance(timestamps.iloc[0], str):
        timestamps = pd.to_datetime(timestamps)

    # åˆ†çµ„é€£çºŒçš„ç•°å¸¸
    groups = []
    current_group = [anomaly_indices[0]]

    for i in range(1, len(anomaly_indices)):
        curr_idx = anomaly_indices[i]
        prev_idx = anomaly_indices[i-1]

        # è¨ˆç®—æ™‚é–“é–“éš”
        time_gap = timestamps.loc[curr_idx] - timestamps.loc[prev_idx]

        if time_gap <= timedelta(minutes=max_gap_minutes):
            current_group.append(curr_idx)
        else:
            groups.append(current_group)
            current_group = [curr_idx]

    groups.append(current_group)

    # æª¢æŸ¥æ¯çµ„çš„æŒçºŒæ™‚é–“
    for group in groups:
        if len(group) >= 2:  # è‡³å°‘éœ€è¦å…©å€‹é»ä¾†è¨ˆç®—æŒçºŒæ™‚é–“
            start_time = timestamps.loc[group[0]]
            end_time = timestamps.loc[group[-1]]
            duration = end_time - start_time

            if duration >= timedelta(minutes=min_duration_minutes):
                # æ¨™è¨˜æ•´å€‹çµ„ç‚ºç•°å¸¸
                for idx in group:
                    result.loc[idx] = True
        elif len(group) == 1:
            # å–®é»ç•°å¸¸ä¹Ÿè€ƒæ…®å‰å¾Œçš„æƒ…æ³
            idx = group[0]
            # é€™è£¡å¯ä»¥æ·»åŠ æ›´è¤‡é›œçš„é‚è¼¯ï¼Œç›®å‰å…ˆæ¨™è¨˜ç‚ºç•°å¸¸
            result.loc[idx] = True

    return result

def detect_anomalies_for_dataset(df, config):
    """
    ç‚ºå–®å€‹æ•¸æ“šé›†æª¢æ¸¬ç•°å¸¸ - ä½¿ç”¨ API å…¼å®¹çš„é‚è¼¯

    Args:
        df: DataFrame with analysis ready data
        config: Detection configuration

    Returns:
        Boolean Series indicating final anomalies
    """
    logger.info(f"é–‹å§‹æª¢æ¸¬ç•°å¸¸ï¼Œå…± {len(df)} æ¢è¨˜éŒ„")

    # æŒ‰æ™‚é–“æ’åº
    df = df.sort_values('timestamp').copy()

    # ä½¿ç”¨ API å…¼å®¹çš„ç•°å¸¸æª¢æ¸¬
    anomalies = api_compatible_anomaly_detection(df, config)

    final_anomaly_count = anomalies.sum()
    logger.info(f"æœ€çµ‚ç•°å¸¸: {final_anomaly_count} å€‹ç•°å¸¸é»")

    return anomalies
    total_anomalies = anomaly_mask.sum()
    logger.info(f"è¯é›†å¾Œç¸½ç•°å¸¸: {total_anomalies} å€‹ç•°å¸¸é»")

    # 4. æŒçºŒæ™‚é–“éæ¿¾ (ç°¡åŒ–å¯¦ç¾ï¼Œèˆ‡ API ä¸€è‡´)
    # API ä¸­çš„æŒçºŒæ™‚é–“éæ¿¾å¯¦éš›ä¸Šæ²’æœ‰æ”¹è®Šå€™é¸æ•¸é‡ï¼Œæ‰€ä»¥é€™è£¡ç›´æ¥è¿”å›
    final_anomalies = anomaly_mask
    logger.info(f"æŒçºŒæ™‚é–“èª¿æ•´å¾Œ: {final_anomalies.sum()} å€‹ç•°å¸¸é»")

    return final_anomalies

def relabel_positive_samples():
    """é‡æ–°æ¨™è¨˜æ­£æ¨£æœ¬"""
    db_path = get_database_path()

    try:
        conn = sqlite3.connect(db_path)

        logger.info("ğŸš€ é–‹å§‹é‡æ–°æ¨™è¨˜æ­£æ¨£æœ¬...")
        logger.info(f"æª¢æ¸¬æ¢ä»¶: {DETECTION_CONFIG}")

        # å…ˆé‡ç½®æ‰€æœ‰æ¨£æœ¬çš„æ¨™ç±¤
        logger.info("é‡ç½®æ‰€æœ‰ AnalysisReadyData çš„ isPositiveLabel ç‚º False...")
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE analysis_ready_data
            SET is_positive_label = FALSE
        """)
        reset_count = cursor.rowcount
        logger.info(f"å·²é‡ç½® {reset_count} æ¢è¨˜éŒ„")

        # ç²å–æ‰€æœ‰æ•¸æ“šé›†
        cursor.execute("""
            SELECT id, name
            FROM analysis_datasets
            ORDER BY name
        """)
        datasets = cursor.fetchall()
        logger.info(f"æ‰¾åˆ° {len(datasets)} å€‹æ•¸æ“šé›†")

        total_positive_labels = 0

        for dataset_id, dataset_name in datasets:
            logger.info(f"\nğŸ” è™•ç†æ•¸æ“šé›†: {dataset_name} (ID: {dataset_id})")

            # ç²å–è©²æ•¸æ“šé›†çš„æ‰€æœ‰æ•¸æ“š
            query = """
                SELECT id, timestamp, wattage_total
                FROM analysis_ready_data
                WHERE dataset_id = ?
                ORDER BY timestamp
            """
            df = pd.read_sql_query(query, conn, params=[dataset_id])

            if len(df) == 0:
                logger.warning(f"æ•¸æ“šé›† {dataset_name} æ²’æœ‰æ•¸æ“šï¼Œè·³é")
                continue

            logger.info(f"è¼‰å…¥ {len(df)} æ¢è¨˜éŒ„")

            # æª¢æ¸¬ç•°å¸¸
            anomalies = detect_anomalies_for_dataset(df, DETECTION_CONFIG)

            # æ›´æ–°æ•¸æ“šåº«ä¸­çš„æ¨™ç±¤
            positive_indices = df[anomalies].index.tolist()
            positive_ids = df.loc[positive_indices, 'id'].tolist()

            if positive_ids:
                # æ‰¹é‡æ›´æ–°
                placeholders = ','.join(['?' for _ in positive_ids])
                update_query = f"""
                    UPDATE analysis_ready_data
                    SET is_positive_label = TRUE
                    WHERE id IN ({placeholders})
                """
                cursor.execute(update_query, positive_ids)
                updated_count = cursor.rowcount

                logger.info(f"âœ… æ›´æ–° {updated_count} æ¢è¨˜éŒ„ç‚ºæ­£æ¨™ç±¤")
                total_positive_labels += updated_count
            else:
                logger.info("æœªç™¼ç¾ç•°å¸¸ï¼Œç„¡éœ€æ›´æ–°")

        # æäº¤æ›´æ”¹
        conn.commit()
        logger.info(f"å·²æäº¤æ•¸æ“šåº«æ›´æ”¹ï¼Œç¸½å…±æ¨™è¨˜ {total_positive_labels} å€‹æ­£æ¨£æœ¬")

        return True

    except Exception as e:
        logger.error(f"âŒ é‡æ–°æ¨™è¨˜éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()

def update_dataset_positive_counts():
    """æ›´æ–°æ¯å€‹æ•¸æ“šé›†çš„ positiveLabels è¨ˆæ•¸"""
    db_path = get_database_path()

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        logger.info("\nğŸ“Š æ›´æ–° AnalysisDataset çš„ positiveLabels è¨ˆæ•¸...")

        # ç²å–æ¯å€‹æ•¸æ“šé›†çš„æ­£æ¨™ç±¤è¨ˆæ•¸
        cursor.execute("""
            SELECT
                ad.id,
                ad.name,
                ad.positive_labels as current_count,
                COUNT(CASE WHEN ard.is_positive_label = TRUE THEN 1 END) as actual_count
            FROM analysis_datasets ad
            LEFT JOIN analysis_ready_data ard ON ad.id = ard.dataset_id
            GROUP BY ad.id, ad.name, ad.positive_labels
            ORDER BY ad.name
        """)

        results = cursor.fetchall()
        updated_count = 0

        logger.info(f"æª¢æŸ¥ {len(results)} å€‹æ•¸æ“šé›†çš„æ­£æ¨™ç±¤è¨ˆæ•¸...")

        for dataset_id, dataset_name, current_count, actual_count in results:
            logger.info(f"ğŸ“¦ {dataset_name}: ç•¶å‰={current_count}, å¯¦éš›={actual_count}")

            if current_count != actual_count:
                # æ›´æ–°è¨ˆæ•¸
                cursor.execute("""
                    UPDATE analysis_datasets
                    SET positive_labels = ?
                    WHERE id = ?
                """, (actual_count, dataset_id))

                logger.info(f"   âœ… å·²æ›´æ–°: {current_count} â†’ {actual_count}")
                updated_count += 1
            else:
                logger.info(f"   â„¹ï¸  ç„¡éœ€æ›´æ–°")

        # æäº¤æ›´æ”¹
        conn.commit()

        logger.info(f"\nğŸ‰ æ›´æ–°å®Œæˆï¼å…±æ›´æ–° {updated_count} å€‹æ•¸æ“šé›†çš„è¨ˆæ•¸")

        # é¡¯ç¤ºç¸½çµ
        cursor.execute("""
            SELECT
                COUNT(*) as total_datasets,
                SUM(positive_labels) as total_positive_labels,
                SUM(total_records) as total_records
            FROM analysis_datasets
        """)

        total_datasets, total_positive, total_records = cursor.fetchone()
        positive_ratio = (total_positive / total_records * 100) if total_records > 0 else 0

        logger.info(f"ğŸ“ˆ ç¸½çµçµ±è¨ˆ:")
        logger.info(f"   ç¸½æ•¸æ“šé›†: {total_datasets}")
        logger.info(f"   ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
        logger.info(f"   ç¸½æ­£æ¨™ç±¤: {total_positive:,}")
        logger.info(f"   æ­£æ¨™ç±¤æ¯”ä¾‹: {positive_ratio:.3f}%")

        return True

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°è¨ˆæ•¸éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ¯ é–‹å§‹é‡æ–°æ¨™è¨˜ç•°å¸¸æ¨£æœ¬ç¨‹åº...")
    logger.info("=" * 60)

    start_time = datetime.now()

    try:
        # æ­¥é©Ÿ 1: é‡æ–°æ¨™è¨˜æ­£æ¨£æœ¬
        logger.info("æ­¥é©Ÿ 1: é‡æ–°æª¢æ¸¬ä¸¦æ¨™è¨˜ç•°å¸¸æ¨£æœ¬")
        if not relabel_positive_samples():
            logger.error("âŒ é‡æ–°æ¨™è¨˜å¤±æ•—")
            return False

        # æ­¥é©Ÿ 2: æ›´æ–°æ•¸æ“šé›†è¨ˆæ•¸
        logger.info("\næ­¥é©Ÿ 2: æ›´æ–°æ•¸æ“šé›†æ­£æ¨™ç±¤è¨ˆæ•¸")
        if not update_dataset_positive_counts():
            logger.error("âŒ æ›´æ–°è¨ˆæ•¸å¤±æ•—")
            return False

        # å®Œæˆ
        end_time = datetime.now()
        duration = end_time - start_time

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ é‡æ–°æ¨™è¨˜ç¨‹åºå®Œæˆï¼")
        logger.info(f"â±ï¸  ç¸½è€—æ™‚: {duration}")
        logger.info("=" * 60)

        return True

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
