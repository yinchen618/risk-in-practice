"""
æˆ¿é–“ç´š PU Learning æ•¸æ“šé è™•ç† ETL è…³æœ¬

åŸºæ–¼ data_preprocessing_etl_multiscale.py çš„é‚è¼¯ï¼Œä½†ä½¿ç”¨æœ¬åœ° CSV æ•¸æ“šè€Œéé ç«¯è³‡æ–™åº«ã€‚
é€™å€‹è…³æœ¬åŸ·è¡Œå®Œæ•´çš„ Extract, Transform, Load (ETL) æµç¨‹ï¼Œ
å¾å·²è™•ç†çš„è¨­å‚™ CSV æ•¸æ“šä¸­ç”Ÿæˆä»¥æˆ¿é–“ç‚ºè¦–è§’çš„ PU å­¸ç¿’æ¨£æœ¬ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. å¾ backend/meter.csv è®€å–æˆ¿é–“é…ç½®
2. è®€å–æœ¬åœ°è¨­å‚™ CSV æ•¸æ“šï¼ˆL1 + L2 é›»è¡¨é…å°ï¼‰
3. åŸ·è¡Œæˆ¿é–“ç´šæ•¸æ“šåˆä½µå’Œæ™‚é–“æˆ³å°é½Š
4. è¨ˆç®—å–®ç›¸ä¸‰ç·šåŠŸç‡ç‰¹å¾µ (110V/220V/Total)
5. å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹ (15åˆ†é˜ + 60åˆ†é˜çª—å£)
6. ç”Ÿæˆå¯ç›´æ¥ç”¨æ–¼ PU å­¸ç¿’çš„æˆ¿é–“æ¨£æœ¬

ä½œè€…: Auto-Generated
æ—¥æœŸ: 2025-08-24
"""

import os
import sys
import pandas as pd
import numpy as np
import csv
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
from enum import Enum
from tqdm import tqdm

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OccupantType(Enum):
    OFFICE_WORKER = "OFFICE_WORKER"
    STUDENT = "STUDENT"
    DEPOSITORY = "DEPOSITORY"
    UNKNOWN = "UNKNOWN"

@dataclass
class RoomInfo:
    """æˆ¿é–“åŸºæœ¬è³‡è¨Š"""
    building: str
    floor: str
    room: str
    meter_id_l1: str  # L1 é›»éŒ¶ ID
    meter_id_l2: str  # L2 é›»éŒ¶ ID
    occupant_type: OccupantType
    is_high_quality: bool = False

@dataclass
class ProcessingStats:
    """è™•ç†çµ±è¨ˆè³‡è¨Š"""
    total_rooms: int = 0
    successful_rooms: int = 0
    failed_rooms: int = 0
    total_samples: int = 0
    positive_samples: int = 0
    processing_time: float = 0.0

class RoomBasedETLProcessor:
    """æˆ¿é–“ç´š ETL è™•ç†å™¨"""

    def __init__(self, device_data_dir: str = None, meter_csv_path: str = None, output_dir: str = None):
        """
        åˆå§‹åŒ–æˆ¿é–“ç´š ETL è™•ç†å™¨

        Args:
            device_data_dir: è¨­å‚™ CSV æ•¸æ“šç›®éŒ„ï¼ˆé è¨­ç‚º processed_for_dbï¼‰
            meter_csv_path: é›»è¡¨é…ç½® CSV æ–‡ä»¶è·¯å¾‘ï¼ˆé è¨­ç‚º backend/meter.csvï¼‰
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ç‚º room_samples_for_puï¼‰
        """
        # è¨­ç½®è·¯å¾‘
        self.base_dir = Path(__file__).parent
        self.device_data_dir = Path(device_data_dir) if device_data_dir else self.base_dir / "backup_2025-07-21_2025-08-23"
        self.meter_csv_path = Path(meter_csv_path) if meter_csv_path else self.base_dir.parent / "meter.csv"
        self.output_dir = Path(output_dir) if output_dir else self.base_dir / "room_samples_for_pu"

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(exist_ok=True)

        # é…ç½®åƒæ•¸ï¼ˆèˆ‡ data_preprocessing_etl_multiscale.py ä¸€è‡´ï¼‰
        self.config = {
            "resample_frequency": "1T",  # 1åˆ†é˜é‡æ¡æ¨£
            "ffill_limit": 3,           # å‘å‰å¡«å……é™åˆ¶
            "min_completeness_ratio": 0.1,  # æœ€å°å®Œæ•´æ€§æ¯”ä¾‹
            "max_missing_periods": 15000,   # æœ€å¤§ç¼ºå¤±æ™‚æ®µ
            "long_window_size": 60,     # é•·æœŸçª—å£60åˆ†é˜
            "short_window_size": 15,    # çŸ­æœŸçª—å£15åˆ†é˜
            "feature_step_size": 1,     # ç‰¹å¾µæå–æ­¥é•·1åˆ†é˜
            "anomaly_time_tolerance_minutes": 10,  # ç•°å¸¸äº‹ä»¶é—œè¯å®¹å·®
        }

        # çµ±è¨ˆè³‡è¨Š
        self.stats = ProcessingStats()

        logger.info(f"æˆ¿é–“ç´š ETL è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è¨­å‚™æ•¸æ“šç›®éŒ„: {self.device_data_dir}")
        logger.info(f"é›»è¡¨é…ç½®æ–‡ä»¶: {self.meter_csv_path}")
        logger.info(f"è¼¸å‡ºç›®éŒ„: {self.output_dir}")

    def load_meter_mapping(self) -> Dict[str, Tuple[str, str]]:
        """
        å¾ CSV æ–‡ä»¶è¼‰å…¥é›»è¡¨æ˜ å°„é—œä¿‚

        Returns:
            Dict: {room_base_name: (meter_id_l1, meter_id_l2)}
        """
        logger.info(f"é–‹å§‹è¼‰å…¥é›»è¡¨æ˜ å°„æ–‡ä»¶: {self.meter_csv_path}")

        if not self.meter_csv_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°é›»è¡¨æ˜ å°„æ–‡ä»¶: {self.meter_csv_path}")

        meter_mapping = {}
        temp_meters = {}

        try:
            with open(self.meter_csv_path, 'r', encoding='utf-8') as file:
                csv_reader = csv.DictReader(file)

                for row in csv_reader:
                    meter_number = row['ç”µè¡¨å·'].strip()
                    meter_name = row['ç”µè¡¨åç§°'].strip()
                    device_id = row['è®¾å¤‡ç¼–å·'].strip()

                    # åˆ¤æ–·æ˜¯å¦ç‚º 'a' çµå°¾ï¼ˆL2 é›»è¡¨ï¼‰
                    if meter_name.endswith('a'):
                        base_room_name = meter_name[:-1]
                        if base_room_name not in temp_meters:
                            temp_meters[base_room_name] = {}
                        temp_meters[base_room_name]['l2'] = device_id
                    else:
                        base_room_name = meter_name
                        if base_room_name not in temp_meters:
                            temp_meters[base_room_name] = {}
                        temp_meters[base_room_name]['l1'] = device_id

                # é…å° L1 å’Œ L2
                for room_name, meters in temp_meters.items():
                    if 'l1' in meters and 'l2' in meters:
                        meter_mapping[room_name] = (meters['l1'], meters['l2'])
                        logger.debug(f"é…å°æˆåŠŸ: {room_name} -> L1: {meters['l1']}, L2: {meters['l2']}")
                    else:
                        logger.warning(f"æˆ¿é–“ {room_name} ç¼ºå°‘é…å°é›»è¡¨")

        except Exception as e:
            logger.error(f"è®€å–é›»è¡¨æ˜ å°„æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

        logger.info(f"æˆåŠŸè¼‰å…¥ {len(meter_mapping)} å€‹æˆ¿é–“çš„é›»è¡¨æ˜ å°„")
        return meter_mapping

    def create_room_info_from_mapping(
        self,
        meter_mapping: Dict[str, Tuple[str, str]],
        default_occupant_type: OccupantType = OccupantType.STUDENT
    ) -> List[RoomInfo]:
        """
        æ ¹æ“šé›»è¡¨æ˜ å°„å‰µå»º RoomInfo åˆ—è¡¨

        Args:
            meter_mapping: é›»è¡¨æ˜ å°„å­—å…¸
            default_occupant_type: é»˜èªçš„ä½”ç”¨é¡å‹

        Returns:
            List[RoomInfo]: æˆ¿é–“è³‡è¨Šåˆ—è¡¨
        """
        logger.info("é–‹å§‹å‰µå»ºæˆ¿é–“è³‡è¨Šåˆ—è¡¨...")
        room_infos = []

        for room_name, (meter_l1, meter_l2) in meter_mapping.items():
            try:
                # è§£ææˆ¿é–“åç¨±ï¼ˆä¾‹å¦‚ï¼šBuilding A101 -> Building: A, Floor: 1, Room: 01ï¼‰
                parts = room_name.split()
                if len(parts) >= 2 and parts[0].lower() == "building":
                    building_code = parts[1]

                    if len(building_code) >= 4:  # å¦‚ A101
                        building = building_code[0]  # A
                        floor_num = building_code[1]  # 1
                        room_num = building_code[2:]  # 01

                        # åˆ¤æ–·æ˜¯å¦ç‚ºé«˜å“è³ªæˆ¿é–“ï¼ˆA2ã€A3ã€A5æ¨“å±¤ï¼‰
                        is_high_quality = building == 'A' and floor_num in ['2', '3', '5']

                        room_info = RoomInfo(
                            building=f"Building-{building}",
                            floor=f"{floor_num}F",
                            room=f"Room-{room_num}",
                            meter_id_l1=meter_l1,
                            meter_id_l2=meter_l2,
                            occupant_type=default_occupant_type,
                            is_high_quality=is_high_quality
                        )

                        room_infos.append(room_info)
                        logger.debug(f"å‰µå»ºæˆ¿é–“: {room_info.building}-{room_info.floor}-{room_info.room} (é«˜å“è³ª: {is_high_quality})")

                    else:
                        logger.warning(f"ç„¡æ³•è§£ææˆ¿é–“ä»£ç¢¼: {building_code}")
                else:
                    logger.warning(f"ç„¡æ³•è§£ææˆ¿é–“åç¨±æ ¼å¼: {room_name}")

            except Exception as e:
                logger.error(f"è§£ææˆ¿é–“åç¨± '{room_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        logger.info(f"æˆåŠŸå‰µå»º {len(room_infos)} å€‹æˆ¿é–“è³‡è¨Š")

        # çµ±è¨ˆé«˜å“è³ªæˆ¿é–“
        high_quality_count = sum(1 for room in room_infos if room.is_high_quality)
        logger.info(f"å…¶ä¸­é«˜å“è³ªæˆ¿é–“: {high_quality_count} å€‹")

        return room_infos

    def load_device_data(self, device_id: str) -> Optional[pd.DataFrame]:
        """
        è¼‰å…¥åŸå§‹è¨­å‚™æ•¸æ“šä¸¦é€²è¡ŒåŠŸç‡è¨ˆç®—

        Args:
            device_id: è¨­å‚™ ID

        Returns:
            pd.DataFrame: è¨­å‚™æ•¸æ“šï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨å‰‡è¿”å› None
        """
        device_file = self.device_data_dir / f"device_{device_id}.csv"

        if not device_file.exists():
            logger.warning(f"è¨­å‚™æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {device_file}")
            return None

        try:
            df = pd.read_csv(device_file)

            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_columns = ['lastUpdated', 'voltage', 'currents', 'power']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                logger.warning(f"è¨­å‚™ {device_id} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                return None

            # è™•ç†æ™‚é–“æˆ³
            df['timestamp'] = pd.to_datetime(df['lastUpdated'], format='mixed', errors='coerce')
            df = df.dropna(subset=['timestamp'])
            df = df.sort_values('timestamp')

            # è¨ˆç®—åŠŸç‡ï¼ˆé›»å£“ Ã— é›»æµï¼‰ï¼Œå› ç‚ºåŸå§‹ power æ¬„ä½é€šå¸¸æ˜¯ 0
            df['calculated_power'] = round(df['voltage'] * df['currents'], 3)

            # å¦‚æœåŸå§‹åŠŸç‡ç„¡æ•ˆï¼Œä½¿ç”¨è¨ˆç®—çš„åŠŸç‡
            if df['power'].sum() == 0 and df['calculated_power'].sum() > 0:
                df['power'] = df['calculated_power']
                logger.info(f"è¨­å‚™ {device_id}: ä½¿ç”¨è¨ˆç®—åŠŸç‡ (VÃ—I) æ›¿ä»£åŸå§‹åŠŸç‡")

            # è¨­ç½®æ™‚é–“æˆ³ç‚ºç´¢å¼•
            df.set_index('timestamp', inplace=True)

            logger.debug(f"æˆåŠŸè¼‰å…¥è¨­å‚™ {device_id} æ•¸æ“š: {len(df)} ç­†è¨˜éŒ„ï¼ŒåŠŸç‡ç¯„åœ {df['power'].min():.2f} ~ {df['power'].max():.2f}")
            return df

        except Exception as e:
            logger.error(f"è¼‰å…¥è¨­å‚™ {device_id} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def merge_room_data(self, room_info: RoomInfo) -> Optional[pd.DataFrame]:
        """
        åˆä½µæˆ¿é–“çš„ L1 å’Œ L2 é›»è¡¨æ•¸æ“š

        Args:
            room_info: æˆ¿é–“è³‡è¨Š

        Returns:
            pd.DataFrame: åˆä½µå¾Œçš„æˆ¿é–“æ•¸æ“š
        """
        logger.info(f"é–‹å§‹åˆä½µæˆ¿é–“ {room_info.building}-{room_info.floor}-{room_info.room} çš„æ•¸æ“š")

        # è¼‰å…¥ L1 å’Œ L2 æ•¸æ“š
        df_l1 = self.load_device_data(room_info.meter_id_l1)
        df_l2 = self.load_device_data(room_info.meter_id_l2)

        if df_l1 is None or df_l2 is None:
            logger.warning(f"æˆ¿é–“ {room_info.room} ç¼ºå°‘å¿…è¦çš„é›»è¡¨æ•¸æ“š")
            return None

        # æå–åŠŸç‡æ•¸æ“šï¼ˆä½¿ç”¨ power æ¬„ä½ï¼‰
        try:
            df_l1_power = df_l1[['power']].rename(columns={'power': 'wattage_l1'})
            df_l2_power = df_l2[['power']].rename(columns={'power': 'wattage_l2'})
        except KeyError as e:
            logger.error(f"æˆ¿é–“ {room_info.room} æ•¸æ“šç¼ºå°‘ power æ¬„ä½: {e}")
            return None

        logger.info(f"L1 æ•¸æ“š: {len(df_l1_power)} ç­†ï¼Œæ™‚é–“ç¯„åœ: {df_l1_power.index.min()} ~ {df_l1_power.index.max()}")
        logger.info(f"L2 æ•¸æ“š: {len(df_l2_power)} ç­†ï¼Œæ™‚é–“ç¯„åœ: {df_l2_power.index.min()} ~ {df_l2_power.index.max()}")

        # é‡æ¡æ¨£å°é½Š
        resample_freq = self.config["resample_frequency"]
        df_l1_resampled = df_l1_power.resample(resample_freq).mean()
        df_l2_resampled = df_l2_power.resample(resample_freq).mean()

        logger.info(f"é‡æ¡æ¨£å¾Œ L1: {len(df_l1_resampled.dropna())} ç­†ï¼ŒL2: {len(df_l2_resampled.dropna())} ç­†")

        # åˆä½µæ•¸æ“š
        df_merged = pd.concat([df_l1_resampled, df_l2_resampled], axis=1, join='outer')

        # è™•ç†ç¼ºå¤±å€¼
        ffill_limit = self.config["ffill_limit"]
        df_merged = df_merged.ffill(limit=ffill_limit)

        initial_count = len(df_merged)
        df_merged = df_merged.dropna()
        final_count = len(df_merged)

        logger.info(f"åˆä½µå¾Œæ•¸æ“š: {final_count} ç­†è¨˜éŒ„")
        if initial_count > final_count:
            logger.info(f"å› é€£çºŒç¼ºå¤±è¶…é {ffill_limit} å€‹æ™‚é–“é»ï¼Œåˆªé™¤äº† {initial_count - final_count} ç­†æ•¸æ“š")

        return df_merged

    def transform_room_data(self, df_merged: pd.DataFrame, room_id: str) -> pd.DataFrame:
        """
        è½‰æ›æˆ¿é–“æ•¸æ“šï¼Œè¨ˆç®—åŠŸç‡ç‰¹å¾µ

        Args:
            df_merged: åˆä½µå¾Œçš„åŸå§‹æ•¸æ“š
            room_id: æˆ¿é–“ ID

        Returns:
            pd.DataFrame: è½‰æ›å¾Œçš„æ•¸æ“š
        """
        logger.info(f"é–‹å§‹è½‰æ›æˆ¿é–“ {room_id} çš„æ•¸æ“š")

        df_transformed = df_merged.copy()

        # ä¿ç•™åŸå§‹åŠŸç‡æ•¸æ“šï¼ˆå››æ¨äº”å…¥åˆ°ç¬¬ä¸‰ä½ï¼‰
        df_transformed['rawWattageL1'] = round(df_merged['wattage_l1'], 3)
        df_transformed['rawWattageL2'] = round(df_merged['wattage_l2'], 3)

        # æ ¹æ“šå–®ç›¸ä¸‰ç·šé…ç½®è¨ˆç®—æ–°ç‰¹å¾µï¼ˆå››æ¨äº”å…¥åˆ°ç¬¬ä¸‰ä½ï¼‰
        df_transformed['wattageTotal'] = round(df_merged['wattage_l1'] + df_merged['wattage_l2'], 3)
        df_transformed['wattage220v'] = round(2 * np.minimum(df_merged['wattage_l1'], df_merged['wattage_l2']), 3)
        df_transformed['wattage110v'] = round(np.abs(df_merged['wattage_l1'] - df_merged['wattage_l2']), 3)

        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        logger.info(f"åŠŸç‡ç‰¹å¾µè¨ˆç®—å®Œæˆ:")
        logger.info(f"  wattageTotal ç¯„åœ: {df_transformed['wattageTotal'].min():.2f} ~ {df_transformed['wattageTotal'].max():.2f}")
        logger.info(f"  wattage220v ç¯„åœ: {df_transformed['wattage220v'].min():.2f} ~ {df_transformed['wattage220v'].max():.2f}")
        logger.info(f"  wattage110v ç¯„åœ: {df_transformed['wattage110v'].min():.2f} ~ {df_transformed['wattage110v'].max():.2f}")

        # æ·»åŠ æˆ¿é–“ ID
        df_transformed['room_id'] = room_id

        # é‡ç½®ç´¢å¼•
        df_transformed.reset_index(inplace=True)

        # æ¸…ç†ä¸­é–“æ¬„ä½
        df_transformed = df_transformed.drop(columns=['wattage_l1', 'wattage_l2'])

        logger.info(f"æ•¸æ“šè½‰æ›å®Œæˆï¼Œæœ€çµ‚ {len(df_transformed)} ç­†è¨˜éŒ„")
        return df_transformed

    def _extract_time_window_features(
        self,
        df: pd.DataFrame,
        timestamp: pd.Timestamp,
        window_size: int,
        suffix: str
    ) -> Dict[str, float]:
        """æå–æ™‚é–“çª—å£çµ±è¨ˆç‰¹å¾µ"""
        window_start = timestamp - pd.Timedelta(minutes=window_size-1)
        mask = (df['timestamp'] >= window_start) & (df['timestamp'] <= timestamp)
        window_data = df[mask]

        features = {}

        if len(window_data) == 0:
            power_types = ['wattage110v', 'wattage220v', 'wattageTotal']
            stats = ['mean', 'std', 'max', 'min']
            for power_type in power_types:
                for stat in stats:
                    features[f"{power_type}_{stat}_{suffix}"] = np.nan
            return features

        power_types = ['wattage110v', 'wattage220v', 'wattageTotal']

        for power_type in power_types:
            if power_type in window_data.columns:
                values = window_data[power_type].dropna()
                if len(values) > 0:
                    try:
                        features[f"{power_type}_mean_{suffix}"] = round(float(values.mean()), 3)
                        features[f"{power_type}_std_{suffix}"] = round(float(values.std()), 3)
                        features[f"{power_type}_max_{suffix}"] = round(float(values.max()), 3)
                        features[f"{power_type}_min_{suffix}"] = round(float(values.min()), 3)
                        features[f"{power_type}_range_{suffix}"] = round(float(values.max() - values.min()), 3)
                        features[f"{power_type}_var_{suffix}"] = round(float(values.var()), 3)
                    except Exception as e:
                        # å¦‚æœè¨ˆç®—å¤±æ•—ï¼Œè¨­ç½®ç‚º NaN
                        for stat in ['mean', 'std', 'max', 'min', 'range', 'var']:
                            features[f"{power_type}_{stat}_{suffix}"] = np.nan
                else:
                    for stat in ['mean', 'std', 'max', 'min', 'range', 'var']:
                        features[f"{power_type}_{stat}_{suffix}"] = np.nan

        return features

    def _extract_time_features(self, timestamp: pd.Timestamp) -> Dict[str, float]:
        """æå–æ™‚é–“ç›¸é—œç‰¹å¾µ"""
        return {
            'hour_of_day': timestamp.hour,
            'day_of_week': timestamp.dayofweek,
            'is_weekend': 1.0 if timestamp.dayofweek >= 5 else 0.0,
            'is_business_hours': 1.0 if 8 <= timestamp.hour <= 18 else 0.0,
        }

    def generate_multiscale_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹

        Args:
            df: è½‰æ›å¾Œçš„åŸºç¤æ•¸æ“š

        Returns:
            pd.DataFrame: åŒ…å«å¤šå°ºåº¦ç‰¹å¾µçš„æ¨£æœ¬æ•¸æ“š
        """
        logger.info("é–‹å§‹å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹...")

        long_window = self.config["long_window_size"]   # 60åˆ†é˜
        short_window = self.config["short_window_size"]  # 15åˆ†é˜
        step_size = self.config["feature_step_size"]     # 1åˆ†é˜

        logger.info(f"é•·æœŸçª—å£: {long_window}åˆ†é˜, çŸ­æœŸçª—å£: {short_window}åˆ†é˜, æ­¥é•·: {step_size}åˆ†é˜")

        # ç¢ºä¿æ•¸æ“šæŒ‰æ™‚é–“æ’åº
        df_sorted = df.sort_values('timestamp').reset_index(drop=True)

        min_start_idx = long_window - 1

        if len(df_sorted) <= min_start_idx:
            logger.warning(f"æ•¸æ“šé‡ä¸è¶³ä»¥ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾µï¼Œéœ€è¦è‡³å°‘ {min_start_idx + 1} ç­†æ•¸æ“šï¼Œå¯¦éš›åªæœ‰ {len(df_sorted)} ç­†")
            return pd.DataFrame()

        samples = []

        for i in range(min_start_idx, len(df_sorted), step_size):
            current_timestamp = df_sorted.iloc[i]['timestamp']
            current_row = df_sorted.iloc[i]

            sample = {
                'timestamp': current_timestamp,
                'room_id': current_row['room_id'],
                'rawWattageL1': round(current_row['rawWattageL1'], 3),
                'rawWattageL2': round(current_row['rawWattageL2'], 3),
                'wattage110v_current': round(current_row['wattage110v'], 3),
                'wattage220v_current': round(current_row['wattage220v'], 3),
                'wattageTotal_current': round(current_row['wattageTotal'], 3),
                # PU å­¸ç¿’æ¨™ç±¤ï¼ˆé è¨­ç‚º Falseï¼Œä¹‹å¾Œå¯ä»¥æ ¹æ“šéœ€è¦è¨­ç½®ï¼‰
                'isPositiveLabel': False,
                'sourceAnomalyEventId': None,
            }

            # æå–é•·æœŸçª—å£ç‰¹å¾µ (60åˆ†é˜)
            long_features = self._extract_time_window_features(
                df_sorted.iloc[:i+1], current_timestamp, long_window, "60m"
            )
            sample.update(long_features)

            # æå–çŸ­æœŸçª—å£ç‰¹å¾µ (15åˆ†é˜)
            short_features = self._extract_time_window_features(
                df_sorted.iloc[:i+1], current_timestamp, short_window, "15m"
            )
            sample.update(short_features)

            # æå–æ™‚é–“ç‰¹å¾µ
            time_features = self._extract_time_features(current_timestamp)
            sample.update(time_features)

            samples.append(sample)

        result_df = pd.DataFrame(samples)

        logger.info(f"æˆåŠŸç”Ÿæˆ {len(result_df)} å€‹å¤šå°ºåº¦ç‰¹å¾µæ¨£æœ¬")
        logger.info(f"ç‰¹å¾µç¸½æ•¸: {len(result_df.columns)} å€‹")

        return result_df

    def process_room(self, room_info: RoomInfo, room_id: str) -> Optional[pd.DataFrame]:
        """
        è™•ç†å–®å€‹æˆ¿é–“çš„å®Œæ•´ ETL æµç¨‹

        Args:
            room_info: æˆ¿é–“è³‡è¨Š
            room_id: æˆ¿é–“ ID

        Returns:
            pd.DataFrame: è™•ç†å®Œæˆçš„æˆ¿é–“æ¨£æœ¬æ•¸æ“š
        """
        logger.info(f"é–‹å§‹è™•ç†æˆ¿é–“: {room_id} ({room_info.building}-{room_info.floor}-{room_info.room})")

        try:
            # æ­¥é©Ÿä¸€ï¼šåˆä½µæˆ¿é–“æ•¸æ“š
            df_merged = self.merge_room_data(room_info)
            if df_merged is None:
                logger.warning(f"æˆ¿é–“ {room_info.room} æ•¸æ“šåˆä½µå¤±æ•—")
                return None

            # æ­¥é©ŸäºŒï¼šè½‰æ›æ•¸æ“š
            df_transformed = self.transform_room_data(df_merged, room_id)

            # æ­¥é©Ÿä¸‰ï¼šå¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹
            df_final = self.generate_multiscale_features(df_transformed)

            if len(df_final) == 0:
                logger.warning(f"æˆ¿é–“ {room_info.room} å¤šå°ºåº¦ç‰¹å¾µç”Ÿæˆå¤±æ•—")
                return None

            logger.info(f"æˆ¿é–“ {room_info.room} è™•ç†å®Œæˆï¼Œç”Ÿæˆ {len(df_final)} å€‹æ¨£æœ¬")
            return df_final

        except Exception as e:
            logger.error(f"è™•ç†æˆ¿é–“ {room_info.room} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def save_room_samples(self, df: pd.DataFrame, room_id: str) -> str:
        """
        ä¿å­˜æˆ¿é–“æ¨£æœ¬æ•¸æ“š

        Args:
            df: æˆ¿é–“æ¨£æœ¬æ•¸æ“š
            room_id: æˆ¿é–“ ID

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾‘
        """
        # ç”Ÿæˆæ–‡ä»¶å
        csv_file = self.output_dir / f"room_samples_{room_id}.csv"
        json_file = self.output_dir / f"room_summary_{room_id}.json"

        # ä¿å­˜ CSV æ•¸æ“š
        df.to_csv(csv_file, index=False)

        # ç”Ÿæˆæ‘˜è¦ä¿¡æ¯
        summary = {
            "room_id": room_id,
            "data_summary": {
                "total_samples": len(df),
                "positive_samples": int(df['isPositiveLabel'].sum()),
                "time_range": {
                    "start": df['timestamp'].min().isoformat(),
                    "end": df['timestamp'].max().isoformat()
                },
                "features_count": len(df.columns),
                "wattage_stats": {
                    "total_mean": round(float(df['wattageTotal_current'].mean()), 3),
                    "total_std": round(float(df['wattageTotal_current'].std()), 3),
                    "220v_mean": round(float(df['wattage220v_current'].mean()), 3),
                    "110v_mean": round(float(df['wattage110v_current'].mean()), 3)
                }
            },
            "processing_timestamp": datetime.now().isoformat()
        }

        # ä¿å­˜æ‘˜è¦
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        logger.info(f"æˆ¿é–“æ¨£æœ¬å·²ä¿å­˜: {csv_file}")
        logger.info(f"æˆ¿é–“æ‘˜è¦å·²ä¿å­˜: {json_file}")

        return str(csv_file)

    def generate_room_metadata(self, room_infos: List[RoomInfo]) -> None:
        """
        ç”Ÿæˆæˆ¿é–“å…ƒè³‡æ–™ä¸€è¦½è¡¨

        Args:
            room_infos: æˆ¿é–“è³‡è¨Šåˆ—è¡¨
        """
        logger.info("ç”Ÿæˆæˆ¿é–“å…ƒè³‡æ–™ä¸€è¦½è¡¨...")

        metadata_file = self.output_dir / "rooms_metadata.csv"

        with open(metadata_file, 'w', encoding='utf-8', newline='') as f:
            fieldnames = ['room_id', 'building', 'floor', 'room', 'occupant_type', 'l1_device', 'l2_device', 'is_high_quality']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

            for i, room_info in enumerate(room_infos, 1):
                writer.writerow({
                    'room_id': f"R{i:03d}",  # R001, R002, R003...
                    'building': room_info.building,
                    'floor': room_info.floor,
                    'room': room_info.room,
                    'occupant_type': room_info.occupant_type.value,
                    'l1_device': room_info.meter_id_l1,
                    'l2_device': room_info.meter_id_l2,
                    'is_high_quality': room_info.is_high_quality
                })

        logger.info(f"æˆ¿é–“å…ƒè³‡æ–™å·²ä¿å­˜è‡³: {metadata_file}")

    def process_all_rooms(self, filter_high_quality: bool = False) -> List[str]:
        """
        è™•ç†æ‰€æœ‰æˆ¿é–“æ•¸æ“š

        Args:
            filter_high_quality: æ˜¯å¦åªè™•ç†é«˜å“è³ªæˆ¿é–“

        Returns:
            List[str]: æˆåŠŸè™•ç†çš„æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
        """
        start_time = datetime.now()
        logger.info("é–‹å§‹è™•ç†æ‰€æœ‰æˆ¿é–“æ•¸æ“š...")

        # è¼‰å…¥æˆ¿é–“é…ç½®
        meter_mapping = self.load_meter_mapping()
        room_infos = self.create_room_info_from_mapping(meter_mapping)

        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        os.makedirs(self.output_dir, exist_ok=True)

        # ç”Ÿæˆæˆ¿é–“å…ƒè³‡æ–™ä¸€è¦½è¡¨
        self.generate_room_metadata(room_infos)

        if filter_high_quality:
            room_infos = [room for room in room_infos if room.is_high_quality]
            logger.info(f"ç¯©é¸å¾Œè™•ç† {len(room_infos)} å€‹é«˜å“è³ªæˆ¿é–“")
        else:
            logger.info(f"è™•ç†å…¨éƒ¨ {len(room_infos)} å€‹æˆ¿é–“")

        self.stats.total_rooms = len(room_infos)
        successful_files = []
        failed_rooms = []

        # è™•ç†æ¯å€‹æˆ¿é–“
        for i, room_info in enumerate(tqdm(room_infos, desc="è™•ç†æˆ¿é–“"), 1):
            try:
                room_id = f"R{i:03d}"  # R001, R002, R003...
                df_samples = self.process_room(room_info, room_id)

                if df_samples is not None and len(df_samples) > 0:
                    # ä¿å­˜æ¨£æœ¬
                    file_path = self.save_room_samples(df_samples, room_id)
                    successful_files.append(file_path)

                    self.stats.successful_rooms += 1
                    self.stats.total_samples += len(df_samples)
                    self.stats.positive_samples += int(df_samples['isPositiveLabel'].sum())

                    logger.info(f"âœ… æˆ¿é–“ {room_id} è™•ç†æˆåŠŸï¼Œç”Ÿæˆ {len(df_samples)} å€‹æ¨£æœ¬")
                else:
                    failed_rooms.append(room_id)
                    self.stats.failed_rooms += 1
                    logger.warning(f"âŒ æˆ¿é–“ {room_id} è™•ç†å¤±æ•—")

            except Exception as e:
                room_id = f"R{i:03d}"
                failed_rooms.append(room_id)
                self.stats.failed_rooms += 1
                logger.error(f"âŒ æˆ¿é–“ {room_id} è™•ç†ç•°å¸¸: {e}")

        # è¨ˆç®—è™•ç†æ™‚é–“
        end_time = datetime.now()
        self.stats.processing_time = (end_time - start_time).total_seconds()

        # ç”Ÿæˆåˆä½µæ¨£æœ¬æ–‡ä»¶
        if successful_files:
            self.create_merged_samples()

        # ä¿å­˜è™•ç†å ±å‘Š
        self.save_processing_report()

        # è¼¸å‡ºè™•ç†æ‘˜è¦
        self.print_processing_summary()

        return successful_files

    def create_merged_samples(self):
        """å‰µå»ºåˆä½µçš„æ¨£æœ¬æ–‡ä»¶"""
        logger.info("é–‹å§‹å‰µå»ºåˆä½µæ¨£æœ¬æ–‡ä»¶...")

        all_samples = []
        room_files = list(self.output_dir.glob("room_samples_*.csv"))

        for file_path in tqdm(room_files, desc="åˆä½µæˆ¿é–“æ¨£æœ¬"):
            try:
                df = pd.read_csv(file_path)
                all_samples.append(df)
            except Exception as e:
                logger.error(f"è®€å–æ–‡ä»¶ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        if all_samples:
            merged_df = pd.concat(all_samples, ignore_index=True)
            merged_file = self.output_dir / "all_room_samples_for_pu.csv"
            merged_df.to_csv(merged_file, index=False)

            logger.info(f"åˆä½µæ¨£æœ¬å·²ä¿å­˜: {merged_file}")
            logger.info(f"ç¸½æ¨£æœ¬æ•¸: {len(merged_df)}")
            logger.info(f"æ­£æ¨£æœ¬æ•¸: {merged_df['isPositiveLabel'].sum()}")

    def save_processing_report(self):
        """ä¿å­˜è™•ç†å ±å‘Š"""
        report = {
            "processing_summary": {
                "total_rooms": self.stats.total_rooms,
                "successful_rooms": self.stats.successful_rooms,
                "failed_rooms": self.stats.failed_rooms,
                "success_rate": self.stats.successful_rooms / self.stats.total_rooms if self.stats.total_rooms > 0 else 0,
                "total_samples": self.stats.total_samples,
                "positive_samples": self.stats.positive_samples,
                "processing_time_seconds": self.stats.processing_time
            },
            "configuration": self.config,
            "output_directory": str(self.output_dir),
            "processing_timestamp": datetime.now().isoformat()
        }

        report_file = self.output_dir / "processing_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"è™•ç†å ±å‘Šå·²ä¿å­˜: {report_file}")

    def print_processing_summary(self):
        """æ‰“å°è™•ç†æ‘˜è¦"""
        logger.info("=" * 60)
        logger.info("ğŸ‰ æˆ¿é–“ç´š PU å­¸ç¿’æ¨£æœ¬ç”Ÿæˆå®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"ç¸½æˆ¿é–“æ•¸: {self.stats.total_rooms}")
        logger.info(f"æˆåŠŸè™•ç†: {self.stats.successful_rooms}")
        logger.info(f"è™•ç†å¤±æ•—: {self.stats.failed_rooms}")
        logger.info(f"æˆåŠŸç‡: {self.stats.successful_rooms/self.stats.total_rooms:.1%}")
        logger.info(f"ç¸½æ¨£æœ¬æ•¸: {self.stats.total_samples:,}")
        logger.info(f"æ­£æ¨£æœ¬æ•¸: {self.stats.positive_samples}")
        logger.info(f"è™•ç†æ™‚é–“: {self.stats.processing_time:.1f} ç§’")
        logger.info(f"è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        logger.info("=" * 60)

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ  é–‹å§‹æˆ¿é–“ç´š PU å­¸ç¿’æ¨£æœ¬ç”Ÿæˆ...")

    # åˆå§‹åŒ–è™•ç†å™¨
    processor = RoomBasedETLProcessor()

    # è™•ç†æ‰€æœ‰æˆ¿é–“ï¼ˆå¯é¸æ“‡åªè™•ç†é«˜å“è³ªæˆ¿é–“ï¼‰
    # filter_high_quality=True  # åªè™•ç†é«˜å“è³ªæˆ¿é–“
    # filter_high_quality=False # è™•ç†æ‰€æœ‰æˆ¿é–“

    successful_files = processor.process_all_rooms(filter_high_quality=False)

    logger.info(f"ğŸ¯ è™•ç†å®Œæˆï¼æˆåŠŸç”Ÿæˆ {len(successful_files)} å€‹æˆ¿é–“æ¨£æœ¬æ–‡ä»¶")

if __name__ == "__main__":
    main()
