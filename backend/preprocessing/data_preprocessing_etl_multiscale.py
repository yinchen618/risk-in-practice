"""
PU Learning æ•¸æ“šé è™•ç† ETL è…³æœ¬

é€™å€‹è…³æœ¬åŸ·è¡Œå®Œæ•´çš„ Extract, Transform, Load (ETL) æµç¨‹ï¼Œ
å¾åŸå§‹é›»éŒ¶æ•¸æ“šä¸­ç”Ÿæˆå¯ç›´æ¥ç”¨æ–¼æ¨¡å‹è¨“ç·´çš„ä¹¾æ·¨æ•¸æ“šé›†ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. å°‹æ‰¾æœ€ä½³çš„é€£çºŒ 7 å¤©æ•¸æ“šçª—å£ (Golden Window)
2. æŠ½å–åŸå§‹æ•¸æ“šä¸¦é€²è¡Œæ™‚é–“æˆ³å°é½Š
3. è½‰æ›æ•¸æ“šæ ¼å¼ä¸¦è¨ˆç®—åŠŸç‡ç‰¹å¾µ
4. å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹ (15åˆ†é˜ + 60åˆ†é˜çª—å£)
5. è¼‰å…¥åˆ°åˆ†ææ•¸æ“šåº«è¡¨ä¸­

ä½œè€…: Auto-Generated
æ—¥æœŸ: 2025-08-23
"""

import os
import sys
import asyncio
import asyncpg
import pandas as pd
import numpy as np
import csv
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
from enum import Enum

# å°å…¥é…ç½®æ–‡ä»¶
try:
    from etl_config import (
        DATABASE_CONFIG, ETL_CONFIG, PREDEFINED_ROOMS, TIME_RANGES,
        get_database_url, get_etl_config, get_room_by_id, get_time_range,
        list_available_rooms
    )
except ImportError:
    # å¦‚æœæ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é è¨­é…ç½®
    print("è­¦å‘Š: æ‰¾ä¸åˆ° etl_config.pyï¼Œä½¿ç”¨é è¨­é…ç½®")
    DATABASE_CONFIG = {"default_url": "postgresql://postgres:Info4467@supa.clkvfvz5fxb3.ap-northeast-3.rds.amazonaws.com:5432/supa"}
    ETL_CONFIG = {
        "resample_frequency": "1T",
        "ffill_limit": 3,
        "anomaly_time_tolerance_minutes": 5,
        "min_completeness_ratio": 0.1,  # é™ä½åˆ° 10% ä»¥é©æ‡‰ IoT æ„Ÿæ¸¬å™¨æ•¸æ“šçš„å¯¦éš›æƒ…æ³
        "max_missing_periods": 15000,   # å¢åŠ åˆ° 15000 å€‹æ™‚æ®µä»¥æ‡‰å°æ•¸æ“šç¨€ç–æ€§
        "long_window_size": 60,
        "short_window_size": 15,
        "feature_step_size": 1,
    }
    PREDEFINED_ROOMS = []
    TIME_RANGES = {}

    def get_database_url():
        import os
        return os.getenv("DATABASE_URL", DATABASE_CONFIG["default_url"])

    def get_etl_config():
        return ETL_CONFIG.copy()

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.DEBUG,  # æ”¹ç‚º DEBUG ç­‰ç´šä»¥é¡¯ç¤ºè©³ç´°è³‡è¨Š
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass
import asyncio
import asyncpg
import os
import csv
from pathlib import Path
from enum import Enum

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OccupantType(Enum):
    OFFICE_WORKER = "OFFICE_WORKER"
    STUDENT = "STUDENT"
    DEPOSITORY = "DEPOSITORY"

@dataclass
class RoomInfo:
    """æˆ¿é–“åŸºæœ¬è³‡è¨Š"""
    building: str
    floor: str
    room: str
    meter_id_l1: str  # L1 é›»éŒ¶ ID
    meter_id_l2: str  # L2 é›»éŒ¶ ID
    occupant_type: OccupantType
    is_high_quality: bool = False  # æ˜¯å¦ç‚ºé«˜å“è³ªæˆ¿é–“ï¼Œé è¨­ç‚º False

@dataclass
class DataQualityMetrics:
    """æ•¸æ“šå“è³ªæŒ‡æ¨™"""
    completeness_ratio: float  # æ•¸æ“šå®Œæ•´æ€§æ¯”ä¾‹
    missing_periods: int       # ç¼ºå¤±æ™‚æ®µæ•¸é‡
    consecutive_days: int      # é€£çºŒå¤©æ•¸
    start_date: datetime
    end_date: datetime

class DataPreprocessingETL:
    """æ•¸æ“šé è™•ç† ETL ä¸»é¡"""

    def __init__(self, database_url: str = None, config: Dict = None):
        """
        åˆå§‹åŒ– ETL è™•ç†å™¨

        Args:
            database_url: PostgreSQL é€£æ¥å­—ä¸² (å¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¨­å®š)
            config: ETL é…ç½®åƒæ•¸å­—å…¸ (å¯é¸ï¼Œæœƒèˆ‡é…ç½®æ–‡ä»¶åˆä½µ)
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è³‡æ–™åº« URLï¼Œå¦‚æœæ²’æœ‰å‚³å…¥çš„è©±
        self.database_url = database_url or get_database_url()
        self.conn = None

        # å¾é…ç½®æ–‡ä»¶è¼‰å…¥é»˜èªé…ç½®
        self.config = get_etl_config()

        # å¦‚æœæä¾›äº†è‡ªå®šç¾©é…ç½®ï¼Œå‰‡æ›´æ–°é»˜èªé…ç½®
        if config:
            self.config.update(config)

        logger.info(f"ETL åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è³‡æ–™åº«: {self.database_url.split('@')[-1] if '@' in self.database_url else 'localhost'}")
        logger.info(f"é…ç½®è¼‰å…¥å®Œæˆï¼Œé•·æœŸçª—å£: {self.config['long_window_size']}åˆ†é˜ï¼ŒçŸ­æœŸçª—å£: {self.config['short_window_size']}åˆ†é˜")

    def load_meter_mapping(self, csv_path: str = "meter.csv") -> Dict[str, Tuple[str, str]]:
        """
        å¾ CSV æ–‡ä»¶è¼‰å…¥é›»è¡¨æ˜ å°„é—œä¿‚

        Args:
            csv_path: CSV æ–‡ä»¶è·¯å¾‘

        Returns:
            Dict: {room_base_name: (meter_id_l1, meter_id_l2)}

        ä¾‹å¦‚:
            {"Building A101": ("402A8FB04CDC", "402A8FB028E7")}
        """
        logger.info(f"é–‹å§‹è¼‰å…¥é›»è¡¨æ˜ å°„æ–‡ä»¶: {csv_path}")

        # ç¢ºä¿æ–‡ä»¶å­˜åœ¨
        csv_file_path = Path(csv_path)
        if not csv_file_path.exists():
            # å˜—è©¦åœ¨ç•¶å‰è…³æœ¬ç›®éŒ„ä¸‹æŸ¥æ‰¾
            script_dir = Path(__file__).parent
            csv_file_path = script_dir / csv_path

        if not csv_file_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°é›»è¡¨æ˜ å°„æ–‡ä»¶: {csv_path}")

        meter_mapping = {}

        try:
            with open(csv_file_path, 'r', encoding='utf-8') as file:
                csv_reader = csv.DictReader(file)

                # è‡¨æ™‚å­˜å„²ï¼Œç”¨æ–¼é…å° L1 å’Œ L2
                temp_meters = {}

                for row in csv_reader:
                    meter_number = row['ç”µè¡¨å·'].strip()
                    meter_name = row['ç”µè¡¨åç§°'].strip()
                    device_id = row['è®¾å¤‡ç¼–å·'].strip()

                    # åˆ¤æ–·æ˜¯å¦ç‚º 'a' çµå°¾ï¼ˆL2 é›»è¡¨ï¼‰
                    if meter_name.endswith('a'):
                        # é€™æ˜¯ L2 é›»è¡¨ï¼Œå–å¾—åŸºç¤æˆ¿é–“åç¨±
                        base_room_name = meter_name[:-1]  # ç§»é™¤æœ«å°¾çš„ 'a'

                        if base_room_name not in temp_meters:
                            temp_meters[base_room_name] = {}
                        temp_meters[base_room_name]['l2'] = device_id

                    else:
                        # é€™æ˜¯ L1 é›»è¡¨
                        base_room_name = meter_name

                        if base_room_name not in temp_meters:
                            temp_meters[base_room_name] = {}
                        temp_meters[base_room_name]['l1'] = device_id

                # é…å° L1 å’Œ L2ï¼Œç”Ÿæˆæœ€çµ‚æ˜ å°„
                for room_name, meters in temp_meters.items():
                    if 'l1' in meters and 'l2' in meters:
                        meter_mapping[room_name] = (meters['l1'], meters['l2'])
                        logger.debug(f"é…å°æˆåŠŸ: {room_name} -> L1: {meters['l1']}, L2: {meters['l2']}")
                    else:
                        logger.warning(f"æˆ¿é–“ {room_name} ç¼ºå°‘é…å°é›»è¡¨ - L1: {'âœ“' if 'l1' in meters else 'âœ—'}, L2: {'âœ“' if 'l2' in meters else 'âœ—'}")

        except Exception as e:
            logger.error(f"è®€å–é›»è¡¨æ˜ å°„æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

        logger.info(f"æˆåŠŸè¼‰å…¥ {len(meter_mapping)} å€‹æˆ¿é–“çš„é›»è¡¨æ˜ å°„")

        # é¡¯ç¤ºå‰5å€‹æ˜ å°„ä½œç‚ºç¯„ä¾‹
        example_count = min(5, len(meter_mapping))
        logger.info("é›»è¡¨æ˜ å°„ç¯„ä¾‹:")
        for i, (room, (l1, l2)) in enumerate(list(meter_mapping.items())[:example_count]):
            logger.info(f"  {i+1}. {room}: L1={l1}, L2={l2}")

        if len(meter_mapping) > example_count:
            logger.info(f"  ... é‚„æœ‰ {len(meter_mapping) - example_count} å€‹æˆ¿é–“")

        return meter_mapping

    def create_room_info_from_mapping(
        self,
        meter_mapping: Dict[str, Tuple[str, str]],
        default_occupant_type: OccupantType = OccupantType.OFFICE_WORKER
    ) -> List[RoomInfo]:
        """
        æ ¹æ“šé›»è¡¨æ˜ å°„å‰µå»º RoomInfo åˆ—è¡¨

        Args:
            meter_mapping: é›»è¡¨æ˜ å°„å­—å…¸
            default_occupant_type: é»˜èªçš„ä½”ç”¨é¡å‹

        Returns:
            List[RoomInfo]: æˆ¿é–“è³‡è¨Šåˆ—è¡¨
        """
        logger.info("é–‹å§‹å‰µå»ºæˆ¿é–“è³‡è¨Šåˆ—è¡¨...")

        room_infos = []

        for room_name, (meter_l1, meter_l2) in meter_mapping.items():
            # è§£ææˆ¿é–“åç¨±ï¼ˆä¾‹å¦‚ï¼šBuilding A101 -> Building: A, Floor: 1, Room: 01ï¼‰
            try:
                # åŸºæœ¬è§£æé‚è¼¯ï¼Œå¯ä»¥æ ¹æ“šå¯¦éš›éœ€æ±‚èª¿æ•´
                parts = room_name.split()
                if len(parts) >= 2 and parts[0].lower() == "building":
                    building_code = parts[1]

                    # æå–æ¨“å±¤å’Œæˆ¿é–“è™Ÿ
                    if len(building_code) >= 4:  # å¦‚ A101
                        building = building_code[0]  # A
                        floor_num = building_code[1]  # 1
                        room_num = building_code[2:]  # 01

                        room_info = RoomInfo(
                            building=f"Building-{building}",
                            floor=f"{floor_num}F",
                            room=f"Room-{room_num}",
                            meter_id_l1=meter_l1,
                            meter_id_l2=meter_l2,
                            occupant_type=default_occupant_type
                        )

                        room_infos.append(room_info)
                        logger.debug(f"å‰µå»ºæˆ¿é–“: {room_info.building}-{room_info.floor}-{room_info.room}")

                    else:
                        logger.warning(f"ç„¡æ³•è§£ææˆ¿é–“ä»£ç¢¼: {building_code}")
                else:
                    logger.warning(f"ç„¡æ³•è§£ææˆ¿é–“åç¨±æ ¼å¼: {room_name}")

            except Exception as e:
                logger.error(f"è§£ææˆ¿é–“åç¨± '{room_name}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        logger.info(f"æˆåŠŸå‰µå»º {len(room_infos)} å€‹æˆ¿é–“è³‡è¨Š")

        return room_infos

    async def connect_database(self):
        """å»ºç«‹æ•¸æ“šåº«é€£æ¥"""
        try:
            self.conn = await asyncpg.connect(self.database_url)
            logger.info("æˆåŠŸé€£æ¥åˆ°æ•¸æ“šåº«")
        except Exception as e:
            logger.error(f"æ•¸æ“šåº«é€£æ¥å¤±æ•—: {e}")
            raise

    async def close_database(self):
        """é—œé–‰æ•¸æ“šåº«é€£æ¥"""
        if self.conn:
            await self.conn.close()
            logger.info("æ•¸æ“šåº«é€£æ¥å·²é—œé–‰")

    async def find_golden_window(
        self,
        room_info: RoomInfo,
        search_start: datetime,
        search_end: datetime,
        window_days: int = 7
    ) -> Tuple[datetime, datetime]:
        """
        æ­¥é©Ÿä¸€ï¼šå°‹æ‰¾æœ€ä½³æ•¸æ“šçª—å£ (Find Golden Window)

        åœ¨æŒ‡å®šçš„æ™‚é–“ç¯„åœå…§ï¼Œæ‰¾å‡ºæ•¸æ“šå“è³ªæœ€ä½³çš„é€£çºŒ N å¤©ã€‚

        Args:
            room_info: æˆ¿é–“è³‡è¨Š
            search_start: æœç´¢é–‹å§‹æ—¥æœŸ
            search_end: æœç´¢çµæŸæ—¥æœŸ
            window_days: çª—å£å¤©æ•¸ (é è¨­ 7 å¤©)

        Returns:
            (start_date, end_date): æœ€ä½³çª—å£çš„èµ·è¨–æ—¥æœŸ
        """
        logger.info(f"ğŸ” é–‹å§‹ç‚ºæˆ¿é–“ {room_info.room} å°‹æ‰¾æœ€ä½³ {window_days} å¤©æ•¸æ“šçª—å£")
        logger.info(f"   æœç´¢ç¯„åœ: {search_start} è‡³ {search_end}")
        logger.info(f"   é›»è¡¨ L1: {room_info.meter_id_l1}, L2: {room_info.meter_id_l2}")

        best_window = None
        best_quality_score = (0.0, -float('inf'))  # å¤šç´šå„ªå…ˆåˆ¶åˆå§‹å€¼
        total_windows = 0
        valid_windows = 0

        # ä»¥å¤©ç‚ºå–®ä½æ»‘å‹•çª—å£
        current_date = search_start
        while current_date + timedelta(days=window_days) <= search_end:
            window_start = current_date
            window_end = current_date + timedelta(days=window_days)
            total_windows += 1

            logger.info(f"   ğŸ• æª¢æŸ¥çª—å£ {total_windows}: {window_start.strftime('%Y-%m-%d')} è‡³ {window_end.strftime('%Y-%m-%d')}")

            # è©•ä¼°æ­¤çª—å£çš„æ•¸æ“šå“è³ª
            quality_metrics = await self._evaluate_data_quality(
                room_info, window_start, window_end
            )

            # è¨˜éŒ„è©³ç´°çš„å“è³ªæŒ‡æ¨™
            logger.info(f"      ğŸ“Š æ•¸æ“šå“è³ª: å®Œæ•´æ€§={quality_metrics.completeness_ratio:.3f}, ç¼ºå¤±æ™‚æ®µ={quality_metrics.missing_periods}")

            # æª¢æŸ¥æ˜¯å¦ç¬¦åˆæœ€ä½è¦æ±‚
            min_completeness = self.config.get("min_completeness_ratio", 0.1)
            max_missing = self.config.get("max_missing_periods", 15000)

            if (quality_metrics.completeness_ratio >= min_completeness and
                quality_metrics.missing_periods <= max_missing):
                valid_windows += 1

                # è©•åˆ†æ¨™æº–ï¼šæ¡ç”¨å…ƒçµ„ (tuple) é€²è¡Œå¤šç´šæ’åº
                # Python æœƒè‡ªå‹•å…ˆæ¯”è¼ƒå…ƒçµ„çš„ç¬¬ä¸€å€‹å…ƒç´ ï¼Œå¦‚æœç›¸åŒï¼Œå†æ¯”è¼ƒç¬¬äºŒå€‹ï¼Œä»¥æ­¤é¡æ¨
                quality_tuple = (
                    quality_metrics.completeness_ratio,  # ç¬¬ä¸€å„ªå…ˆï¼šå®Œæ•´åº¦è¶Šé«˜è¶Šå¥½
                    -quality_metrics.missing_periods,    # ç¬¬äºŒå„ªå…ˆï¼šç¼ºå¤±æ™‚æ®µè¶Šå°‘è¶Šå¥½ (å–è² å€¼ä½¿å…¶è®Šç‚ºè¶Šå¤§è¶Šå¥½)
                    # æœªä¾†é‚„å¯ä»¥åŠ å…¥ç¬¬ä¸‰å„ªå…ˆç´šï¼Œä¾‹å¦‚ -max_continuous_gap (æœ€é•·ä¸­æ–·æ™‚é–“è¶ŠçŸ­è¶Šå¥½)
                )

                logger.info(f"      âœ… ç¬¦åˆå“è³ªè¦æ±‚ï¼Œå“è³ªå…ƒçµ„: {quality_tuple}")

                if quality_tuple > best_quality_score:
                    best_quality_score = quality_tuple
                    best_window = (window_start, window_end)
                    logger.info(f"      ğŸ† æ–°çš„æœ€ä½³çª—å£ï¼")
            else:
                logger.info(f"      âŒ ä¸ç¬¦åˆå“è³ªè¦æ±‚ (å®Œæ•´æ€§éœ€>={min_completeness}, ç¼ºå¤±<={max_missing})")

            current_date += timedelta(days=1)

        logger.info(f"ğŸ“‹ æœç´¢çµæœ: æª¢æŸ¥äº† {total_windows} å€‹çª—å£ï¼Œ{valid_windows} å€‹ç¬¦åˆè¦æ±‚")
        logger.info(f"   æœ€ä½å“è³ªè¦æ±‚: å®Œæ•´æ€§ >= {self.config.get('min_completeness_ratio', 0.1)}, ç¼ºå¤±æ™‚æ®µ <= {self.config.get('max_missing_periods', 15000)}")

        if best_window is None:
            logger.error(f"âŒ åœ¨æŒ‡å®šç¯„åœå…§æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„ {window_days} å¤©çª—å£")
            logger.error(f"   å»ºè­°: é™ä½å“è³ªè¦æ±‚æˆ–æ“´å¤§æœç´¢æ™‚é–“ç¯„åœ")
            raise ValueError(f"åœ¨æŒ‡å®šç¯„åœå…§æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„ {window_days} å¤©çª—å£")

        logger.info(f"ğŸ¯ æ‰¾åˆ°æœ€ä½³çª—å£: {best_window[0].strftime('%Y-%m-%d')} åˆ° {best_window[1].strftime('%Y-%m-%d')}")
        logger.info(f"   æœ€ä½³å“è³ªå…ƒçµ„: å®Œæ•´æ€§={best_quality_score[0]:.3f}, ç¼ºå¤±æ™‚æ®µ={-best_quality_score[1]}")
        return best_window

    async def _evaluate_data_quality(
        self,
        room_info: RoomInfo,
        start_date: datetime,
        end_date: datetime
    ) -> DataQualityMetrics:
        """
        è©•ä¼°æŒ‡å®šæ™‚é–“çª—å£å…§çš„æ•¸æ“šå“è³ª

        Args:
            room_info: æˆ¿é–“è³‡è¨Š
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            DataQualityMetrics: æ•¸æ“šå“è³ªæŒ‡æ¨™
        """
        # æŸ¥è©¢è©²æ™‚é–“æ®µå…§çš„æ•¸æ“šç­†æ•¸
        # æŸ¥è©¢ L1 é›»è¡¨çš„æ™‚é–“ç¯„åœ
        query_l1 = """
        SELECT COUNT(*) as total_records,
               MIN("lastUpdated") as min_time,
               MAX("lastUpdated") as max_time
        FROM ammeter_log
        WHERE "deviceNumber" = $1
        AND "lastUpdated" BETWEEN $2 AND $3
        """

        query_l2 = """
        SELECT COUNT(*) as total_records,
               MIN("lastUpdated") as min_time,
               MAX("lastUpdated") as max_time
        FROM ammeter_log
        WHERE "deviceNumber" = $1
        AND "lastUpdated" BETWEEN $2 AND $3
        """

        result_l1 = await self.conn.fetchrow(query_l1, room_info.meter_id_l1, start_date, end_date)
        result_l2 = await self.conn.fetchrow(query_l2, room_info.meter_id_l2, start_date, end_date)

        # è¨˜éŒ„è©³ç´°çš„æŸ¥è©¢çµæœ
        logger.info(f"        ğŸ”Œ L1é›»è¡¨ {room_info.meter_id_l1}: {result_l1['total_records']} ç­†è¨˜éŒ„")
        if result_l1['total_records'] > 0:
            logger.info(f"           æ™‚é–“ç¯„åœ: {result_l1['min_time']} è‡³ {result_l1['max_time']}")

        logger.info(f"        ğŸ”Œ L2é›»è¡¨ {room_info.meter_id_l2}: {result_l2['total_records']} ç­†è¨˜éŒ„")
        if result_l2['total_records'] > 0:
            logger.info(f"           æ™‚é–“ç¯„åœ: {result_l2['min_time']} è‡³ {result_l2['max_time']}")

        # è¨ˆç®—æœŸæœ›çš„æ•¸æ“šç­†æ•¸ (å‡è¨­æ¯åˆ†é˜ä¸€ç­†)
        expected_records = int((end_date - start_date).total_seconds() / 60)
        logger.info(f"        ğŸ“… æœŸæœ›è¨˜éŒ„æ•¸: {expected_records} ç­† (æ¯åˆ†é˜1ç­†)")

        # è¨ˆç®—å®Œæ•´æ€§æ¯”ä¾‹
        actual_records = min(result_l1['total_records'], result_l2['total_records'])
        completeness_ratio = actual_records / expected_records if expected_records > 0 else 0.0

        # ç°¡åŒ–çš„ç¼ºå¤±æ™‚æ®µè¨ˆç®— (å¯¦éš›å¯ä»¥æ›´ç²¾ç´°)
        missing_periods = max(0, expected_records - actual_records)

        logger.info(f"        ğŸ“Š å¯¦éš›è¨˜éŒ„æ•¸: {actual_records} ç­†")
        logger.info(f"        ğŸ“ˆ å®Œæ•´æ€§æ¯”ä¾‹: {completeness_ratio:.3f} ({actual_records}/{expected_records})")
        logger.info(f"        âš ï¸  ç¼ºå¤±æ™‚æ®µ: {missing_periods} å€‹")

        return DataQualityMetrics(
            completeness_ratio=completeness_ratio,
            missing_periods=missing_periods,
            consecutive_days=(end_date - start_date).days,
            start_date=start_date,
            end_date=end_date
        )

    async def extract_raw_data(
        self,
        room_info: RoomInfo,
        start_date: datetime,
        end_date: datetime
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        æ­¥é©ŸäºŒï¼šæŠ½å–åŸå§‹æ•¸æ“š (Extract)

        å¾æ•¸æ“šåº«ä¸­æŠ½å–æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„åŸå§‹é›»éŒ¶æ•¸æ“š

        Args:
            room_info: æˆ¿é–“è³‡è¨Š
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            (df_l1, df_l2): L1 å’Œ L2 çš„åŸå§‹æ•¸æ“š DataFrame
        """
        logger.info(f"é–‹å§‹æŠ½å–åŸå§‹æ•¸æ“š: {start_date} åˆ° {end_date}")

        # æŸ¥è©¢ L1 é›»éŒ¶æ•¸æ“š
        query_l1 = """
        SELECT "lastUpdated" as timestamp, power as wattage
        FROM ammeter_log
        WHERE "deviceNumber" = $1
        AND "lastUpdated" BETWEEN $2 AND $3
        ORDER BY "lastUpdated"
        """

        # æŸ¥è©¢ L2 é›»éŒ¶æ•¸æ“š
        query_l2 = """
        SELECT "lastUpdated" as timestamp, power as wattage
        FROM ammeter_log
        WHERE "deviceNumber" = $1
        AND "lastUpdated" BETWEEN $2 AND $3
        ORDER BY "lastUpdated"
        """

        # åŸ·è¡ŒæŸ¥è©¢
        rows_l1 = await self.conn.fetch(query_l1, room_info.meter_id_l1, start_date, end_date)
        rows_l2 = await self.conn.fetch(query_l2, room_info.meter_id_l2, start_date, end_date)

        # è½‰æ›ç‚º DataFrame
        df_l1 = pd.DataFrame(rows_l1, columns=['timestamp', 'wattage'])
        df_l2 = pd.DataFrame(rows_l2, columns=['timestamp', 'wattage'])

        # è¨­ç½®æ™‚é–“æˆ³ç‚ºç´¢å¼•
        df_l1['timestamp'] = pd.to_datetime(df_l1['timestamp'])
        df_l2['timestamp'] = pd.to_datetime(df_l2['timestamp'])
        df_l1.set_index('timestamp', inplace=True)
        df_l2.set_index('timestamp', inplace=True)

        logger.info(f"L1 æ•¸æ“šç­†æ•¸: {len(df_l1)}, L2 æ•¸æ“šç­†æ•¸: {len(df_l2)}")
        return df_l1, df_l2

    def transform_data(
        self,
        df_l1: pd.DataFrame,
        df_l2: pd.DataFrame,
        room_info: RoomInfo
    ) -> pd.DataFrame:
        """
        æ­¥é©Ÿä¸‰ï¼šè½‰æ›æ•¸æ“š (Transform)

        é€™æ˜¯æµç¨‹çš„æ ¸å¿ƒï¼ŒåŸ·è¡Œæ™‚é–“æˆ³å°é½Šã€åŠŸç‡æ‹†è§£è¨ˆç®—ç­‰è™•ç†

        Args:
            df_l1: L1 é›»éŒ¶æ•¸æ“š
            df_l2: L2 é›»éŒ¶æ•¸æ“š
            room_info: æˆ¿é–“è³‡è¨Š

        Returns:
            pd.DataFrame: è½‰æ›å¾Œçš„ä¹¾æ·¨æ•¸æ“š
        """
        logger.info("é–‹å§‹æ•¸æ“šè½‰æ›è™•ç†")

        # 3.1 æ™‚é–“æˆ³å°é½Š (Timestamp Alignment)
        logger.info("åŸ·è¡Œæ™‚é–“æˆ³å°é½Š...")
        logger.info(f"L1 åŸå§‹æ•¸æ“šç¯„åœ: {df_l1.index.min()} åˆ° {df_l1.index.max()}")
        logger.info(f"L2 åŸå§‹æ•¸æ“šç¯„åœ: {df_l2.index.min()} åˆ° {df_l2.index.max()}")

        # æ­¥é©Ÿ 4.1: åˆ†åˆ¥é‡æ¡æ¨£ (Resample Separately)
        # å°‡å¸¶æœ‰ã€Œæ¯›åˆºã€æ™‚é–“æˆ³çš„æ•¸æ“šå°é½Šåˆ°çµ±ä¸€çš„åˆ†é˜æ™‚é–“æ ¼ç·š
        resample_freq = self.config["resample_frequency"]
        df_l1_resampled = df_l1.resample(resample_freq).mean()
        df_l2_resampled = df_l2.resample(resample_freq).mean()

        logger.info(f"ä½¿ç”¨é‡æ¡æ¨£é »ç‡: {resample_freq}")
        logger.info(f"é‡æ¡æ¨£å¾Œ L1 æ•¸æ“šç­†æ•¸: {len(df_l1_resampled.dropna())}")
        logger.info(f"é‡æ¡æ¨£å¾Œ L2 æ•¸æ“šç­†æ•¸: {len(df_l2_resampled.dropna())}")

        # æ­¥é©Ÿ 4.2: åˆä½µå°é½Šå¾Œçš„æ•¸æ“š (Merge the Aligned Data)
        # ä½¿ç”¨ concat é€²è¡Œå¤–éƒ¨åˆä½µï¼Œç¢ºä¿æ‰€æœ‰æ™‚é–“é»éƒ½è¢«ä¿ç•™
        df_merged = pd.concat([df_l1_resampled, df_l2_resampled], axis=1, join='outer')
        df_merged.columns = ['wattage_l1', 'wattage_l2']

        logger.info(f"åˆä½µå¾Œæ•¸æ“šç­†æ•¸: {len(df_merged)}")

        # æ­¥é©Ÿ 4.3: è™•ç†åˆä½µå¾Œçš„ç¼ºå¤±å€¼ (Handle Missing Values Post-Merge)
        # ä½¿ç”¨å‘å‰å¡«å……ï¼Œæœ€å¤šå¡«å……é€£çºŒ N å€‹ç¼ºå¤±å€¼
        ffill_limit = self.config["ffill_limit"]
        df_merged = df_merged.fillna(method='ffill', limit=ffill_limit)

        logger.info(f"ä½¿ç”¨å‘å‰å¡«å……é™åˆ¶: {ffill_limit} å€‹æ™‚é–“é»")

        # åˆªé™¤ä»ç„¶å­˜åœ¨ NaN çš„è¡Œï¼ˆè¡¨ç¤ºè¶…éé™åˆ¶çš„é€£çºŒæ•¸æ“šç¼ºå¤±ï¼‰
        initial_count = len(df_merged)
        df_merged = df_merged.dropna()
        final_count = len(df_merged)

        logger.info(f"å‘å‰å¡«å……å¾Œæœ€çµ‚æ•¸æ“šç­†æ•¸: {final_count}")
        if initial_count > final_count:
            logger.warning(f"å› é€£çºŒç¼ºå¤±è¶…é{ffill_limit}å€‹æ™‚é–“é»ï¼Œåˆªé™¤äº† {initial_count - final_count} ç­†æ•¸æ“š")

        # 3.2 åŠŸç‡æ‹†è§£è¨ˆç®— (Power Decomposition)
        logger.info("åŸ·è¡ŒåŠŸç‡æ‹†è§£è¨ˆç®—...")

        df_transformed = df_merged.copy()
        df_transformed['rawWattageL1'] = df_merged['wattage_l1']
        df_transformed['rawWattageL2'] = df_merged['wattage_l2']

        # æ ¹æ“šå–®ç›¸ä¸‰ç·šé…ç½®è¨ˆç®—æ–°ç‰¹å¾µ
        df_transformed['wattageTotal'] = df_merged['wattage_l1'] + df_merged['wattage_l2']
        df_transformed['wattage220v'] = 2 * np.minimum(df_merged['wattage_l1'], df_merged['wattage_l2'])
        df_transformed['wattage110v'] = np.abs(df_merged['wattage_l1'] - df_merged['wattage_l2'])

        # è¨ˆç®—çµ±è¨ˆè³‡è¨Šç”¨æ–¼è¨ºæ–·
        logger.info(f"åŠŸç‡ç‰¹å¾µè¨ˆç®—å®Œæˆ:")
        logger.info(f"  wattageTotal ç¯„åœ: {df_transformed['wattageTotal'].min():.2f} ~ {df_transformed['wattageTotal'].max():.2f}")
        logger.info(f"  wattage220v ç¯„åœ: {df_transformed['wattage220v'].min():.2f} ~ {df_transformed['wattage220v'].max():.2f}")
        logger.info(f"  wattage110v ç¯„åœ: {df_transformed['wattage110v'].min():.2f} ~ {df_transformed['wattage110v'].max():.2f}")

        # æ·»åŠ æˆ¿é–“è³‡è¨Š
        df_transformed['room'] = room_info.room

        # é‡ç½®ç´¢å¼•ï¼Œè®“æ™‚é–“æˆ³æˆç‚ºæ™®é€šæ¬„ä½
        df_transformed.reset_index(inplace=True)
        df_transformed.rename(columns={'timestamp': 'timestamp'}, inplace=True)

        # åˆªé™¤ä¸­é–“è¨ˆç®—æ¬„ä½
        df_transformed = df_transformed.drop(columns=['wattage_l1', 'wattage_l2'])

        logger.info("åŠŸç‡æ‹†è§£è¨ˆç®—å®Œæˆ")
        return df_transformed

    def _extract_time_window_features(
        self,
        df: pd.DataFrame,
        timestamp: pd.Timestamp,
        window_size: int,
        suffix: str
    ) -> Dict[str, float]:
        """
        å¾æŒ‡å®šæ™‚é–“çª—å£ä¸­æå–çµ±è¨ˆç‰¹å¾µ

        Args:
            df: åŒ…å«åŠŸç‡æ•¸æ“šçš„ DataFrame
            timestamp: çª—å£çµæŸæ™‚é–“é»
            window_size: çª—å£å¤§å°ï¼ˆåˆ†é˜ï¼‰
            suffix: ç‰¹å¾µåç¨±å¾Œç¶´ï¼ˆå¦‚ "60m", "15m"ï¼‰

        Returns:
            Dict: æå–çš„ç‰¹å¾µå­—å…¸
        """
        # è¨ˆç®—çª—å£é–‹å§‹æ™‚é–“
        window_start = timestamp - pd.Timedelta(minutes=window_size-1)

        # å¾ DataFrame ä¸­åˆ‡ç‰‡å‡ºæŒ‡å®šçª—å£çš„æ•¸æ“š
        mask = (df['timestamp'] >= window_start) & (df['timestamp'] <= timestamp)
        window_data = df[mask]

        features = {}

        if len(window_data) == 0:
            # å¦‚æœçª—å£å…§æ²’æœ‰æ•¸æ“šï¼Œè¿”å›ç©ºå€¼
            power_types = ['wattage110v', 'wattage220v', 'wattageTotal']
            stats = ['mean', 'std', 'max', 'min']
            for power_type in power_types:
                for stat in stats:
                    features[f"{power_type}_{stat}_{suffix}"] = np.nan
            return features

        # å°æ¯ç¨®åŠŸç‡é¡å‹è¨ˆç®—çµ±è¨ˆç‰¹å¾µ
        power_types = ['wattage110v', 'wattage220v', 'wattageTotal']

        for power_type in power_types:
            if power_type in window_data.columns:
                values = window_data[power_type].dropna()
                if len(values) > 0:
                    features[f"{power_type}_mean_{suffix}"] = values.mean()
                    features[f"{power_type}_std_{suffix}"] = values.std()
                    features[f"{power_type}_max_{suffix}"] = values.max()
                    features[f"{power_type}_min_{suffix}"] = values.min()
                    features[f"{power_type}_range_{suffix}"] = values.max() - values.min()
                    features[f"{power_type}_var_{suffix}"] = values.var()
                else:
                    features[f"{power_type}_mean_{suffix}"] = np.nan
                    features[f"{power_type}_std_{suffix}"] = np.nan
                    features[f"{power_type}_max_{suffix}"] = np.nan
                    features[f"{power_type}_min_{suffix}"] = np.nan
                    features[f"{power_type}_range_{suffix}"] = np.nan
                    features[f"{power_type}_var_{suffix}"] = np.nan

        return features

    def _extract_time_features(self, timestamp: pd.Timestamp) -> Dict[str, float]:
        """
        æå–æ™‚é–“ç›¸é—œç‰¹å¾µ

        Args:
            timestamp: æ™‚é–“æˆ³

        Returns:
            Dict: æ™‚é–“ç‰¹å¾µå­—å…¸
        """
        return {
            'hour_of_day': timestamp.hour,
            'day_of_week': timestamp.dayofweek,
            'is_weekend': 1.0 if timestamp.dayofweek >= 5 else 0.0,
            'is_business_hours': 1.0 if 8 <= timestamp.hour <= 18 else 0.0,
        }

    def generate_multiscale_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾µ (Multi-scale Feature Engineering)

        å°æ¯å€‹æ™‚é–“é»ï¼ŒåŒæ™‚æå–é•·æœŸï¼ˆ60åˆ†é˜ï¼‰å’ŒçŸ­æœŸï¼ˆ15åˆ†é˜ï¼‰çª—å£çš„ç‰¹å¾µ

        Args:
            df: è½‰æ›å¾Œçš„åŸºç¤æ•¸æ“š

        Returns:
            pd.DataFrame: åŒ…å«å¤šå°ºåº¦ç‰¹å¾µçš„æ¨£æœ¬æ•¸æ“š
        """
        logger.info("é–‹å§‹å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹...")

        # ç²å–é…ç½®åƒæ•¸
        long_window = self.config["long_window_size"]   # 60åˆ†é˜
        short_window = self.config["short_window_size"]  # 15åˆ†é˜
        step_size = self.config["feature_step_size"]     # 1åˆ†é˜

        logger.info(f"é•·æœŸçª—å£: {long_window}åˆ†é˜, çŸ­æœŸçª—å£: {short_window}åˆ†é˜, æ­¥é•·: {step_size}åˆ†é˜")

        # ç¢ºä¿æ•¸æ“šæŒ‰æ™‚é–“æ’åº
        df_sorted = df.sort_values('timestamp').reset_index(drop=True)

        # è¨ˆç®—å¯ä»¥ç”Ÿæˆç‰¹å¾µçš„æœ€å°é–‹å§‹ä½ç½®ï¼ˆéœ€è¦è¶³å¤ çš„æ­·å²æ•¸æ“šï¼‰
        min_start_idx = long_window - 1  # é•·æœŸçª—å£éœ€è¦çš„æœ€å°æ­·å²æ•¸æ“š

        if len(df_sorted) <= min_start_idx:
            logger.warning(f"æ•¸æ“šé‡ä¸è¶³ä»¥ç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾µï¼Œéœ€è¦è‡³å°‘ {min_start_idx + 1} ç­†æ•¸æ“šï¼Œå¯¦éš›åªæœ‰ {len(df_sorted)} ç­†")
            return pd.DataFrame()

        # å­˜å„²æ‰€æœ‰æ¨£æœ¬çš„åˆ—è¡¨
        samples = []

        # å¾èƒ½å¤ ç”Ÿæˆå®Œæ•´é•·æœŸçª—å£ç‰¹å¾µçš„ä½ç½®é–‹å§‹æ»‘å‹•
        for i in range(min_start_idx, len(df_sorted), step_size):
            current_timestamp = df_sorted.iloc[i]['timestamp']

            # æå–ç•¶å‰æ™‚é–“é»çš„åŸºç¤è³‡è¨Š
            current_row = df_sorted.iloc[i]

            sample = {
                'timestamp': current_timestamp,
                'room': current_row['room'],
                'rawWattageL1': current_row['rawWattageL1'],
                'rawWattageL2': current_row['rawWattageL2'],
                'wattage110v_current': current_row['wattage110v'],
                'wattage220v_current': current_row['wattage220v'],
                'wattageTotal_current': current_row['wattageTotal'],
                'isPositiveLabel': current_row.get('isPositiveLabel', False),
                'sourceAnomalyEventId': current_row.get('sourceAnomalyEventId', None),
            }

            # æå–é•·æœŸçª—å£ç‰¹å¾µ (60åˆ†é˜)
            long_features = self._extract_time_window_features(
                df_sorted.iloc[:i+1], current_timestamp, long_window, "60m"
            )
            sample.update(long_features)

            # æå–çŸ­æœŸçª—å£ç‰¹å¾µ (15åˆ†é˜)
            short_features = self._extract_time_window_features(
                df_sorted.iloc[:i+1], current_timestamp, short_window, "15m"
            )
            sample.update(short_features)

            # æå–æ™‚é–“ç‰¹å¾µ
            time_features = self._extract_time_features(current_timestamp)
            sample.update(time_features)

            samples.append(sample)

        # è½‰æ›ç‚º DataFrame
        result_df = pd.DataFrame(samples)

        logger.info(f"æˆåŠŸç”Ÿæˆ {len(result_df)} å€‹å¤šå°ºåº¦ç‰¹å¾µæ¨£æœ¬")
        logger.info(f"ç‰¹å¾µç¸½æ•¸: {len(result_df.columns)} å€‹")

        # é¡¯ç¤ºç‰¹å¾µåˆ†é¡çµ±è¨ˆ
        long_features_count = len([col for col in result_df.columns if '60m' in col])
        short_features_count = len([col for col in result_df.columns if '15m' in col])
        time_features_count = len([col for col in result_df.columns if col in ['hour_of_day', 'day_of_week', 'is_weekend', 'is_business_hours']])

        logger.info(f"  é•·æœŸç‰¹å¾µ (60åˆ†é˜): {long_features_count} å€‹")
        logger.info(f"  çŸ­æœŸç‰¹å¾µ (15åˆ†é˜): {short_features_count} å€‹")
        logger.info(f"  æ™‚é–“ç‰¹å¾µ: {time_features_count} å€‹")
        logger.info(f"  å…¶ä»–åŸºç¤ç‰¹å¾µ: {len(result_df.columns) - long_features_count - short_features_count - time_features_count} å€‹")

        return result_df

    def _diagnose_timestamp_alignment(
        self,
        df_l1: pd.DataFrame,
        df_l2: pd.DataFrame,
        df_merged: pd.DataFrame
    ) -> Dict[str, any]:
        """
        è¨ºæ–·æ™‚é–“æˆ³å°é½Šçš„æ•ˆæœï¼Œæä¾›è©³ç´°çš„çµ±è¨ˆè³‡è¨Š

        Args:
            df_l1: L1 åŸå§‹æ•¸æ“š
            df_l2: L2 åŸå§‹æ•¸æ“š
            df_merged: åˆä½µå¾Œæ•¸æ“š

        Returns:
            Dict: è¨ºæ–·çµ±è¨ˆè³‡è¨Š
        """
        diagnostics = {}

        # è¨ˆç®—æ™‚é–“ç¯„åœ
        l1_time_range = df_l1.index.max() - df_l1.index.min()
        l2_time_range = df_l2.index.max() - df_l2.index.min()
        merged_time_range = df_merged.index.max() - df_merged.index.min()

        # è¨ˆç®—æ™‚é–“æˆ³åˆ†ä½ˆ
        l1_timestamps = df_l1.index.to_series()
        l2_timestamps = df_l2.index.to_series()

        # è¨ˆç®—ç§’ç´šæ™‚é–“æˆ³çš„åˆ†ä½ˆï¼ˆæª¢æŸ¥æ¼‚ç§»ï¼‰
        l1_seconds = l1_timestamps.dt.second.value_counts().sort_index()
        l2_seconds = l2_timestamps.dt.second.value_counts().sort_index()

        diagnostics.update({
            'l1_record_count': len(df_l1),
            'l2_record_count': len(df_l2),
            'merged_record_count': len(df_merged),
            'l1_time_range_hours': l1_time_range.total_seconds() / 3600,
            'l2_time_range_hours': l2_time_range.total_seconds() / 3600,
            'merged_time_range_hours': merged_time_range.total_seconds() / 3600,
            'l1_seconds_distribution': l1_seconds.to_dict(),
            'l2_seconds_distribution': l2_seconds.to_dict(),
            'alignment_efficiency': len(df_merged) / max(len(df_l1), len(df_l2))
        })

        # è¨˜éŒ„è¨ºæ–·è³‡è¨Šåˆ°æ—¥èªŒ
        logger.info("=== æ™‚é–“æˆ³å°é½Šè¨ºæ–· ===")
        logger.info(f"L1 è¨˜éŒ„æ•¸: {diagnostics['l1_record_count']}, æ™‚é–“è·¨åº¦: {diagnostics['l1_time_range_hours']:.2f} å°æ™‚")
        logger.info(f"L2 è¨˜éŒ„æ•¸: {diagnostics['l2_record_count']}, æ™‚é–“è·¨åº¦: {diagnostics['l2_time_range_hours']:.2f} å°æ™‚")
        logger.info(f"åˆä½µå¾Œè¨˜éŒ„æ•¸: {diagnostics['merged_record_count']}, å°é½Šæ•ˆç‡: {diagnostics['alignment_efficiency']:.1%}")

        return diagnostics

    async def enrich_ground_truth(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ­¥é©Ÿä¸‰ï¼šè±å¯ŒçœŸå¯¦æ¨™ç±¤ (Ground Truth Enrichment)

        å°‡æ•¸æ“šèˆ‡å·²æ¨™è¨»çš„ç•°å¸¸äº‹ä»¶é€²è¡Œé—œè¯

        Args:
            df: è½‰æ›å¾Œçš„æ•¸æ“š

        Returns:
            pd.DataFrame: è±å¯Œæ¨™ç±¤å¾Œçš„æ•¸æ“š
        """
        logger.info("é–‹å§‹è±å¯ŒçœŸå¯¦æ¨™ç±¤...")

        # æŸ¥è©¢æ‰€æœ‰å·²æ¨™è¨»ç‚º POSITIVE çš„ç•°å¸¸äº‹ä»¶
        query_positive_events = """
        SELECT id, "eventTimestamp", "meterId"
        FROM anomaly_event
        WHERE status = 'CONFIRMED_POSITIVE'
        """

        positive_events = await self.conn.fetch(query_positive_events)

        # å¦‚æœæ²’æœ‰ç•°å¸¸äº‹ä»¶ï¼Œåˆå§‹åŒ–æ¨™ç±¤æ¬„ä½ä¸¦è¿”å›
        if not positive_events:
            df['isPositiveLabel'] = False
            df['sourceAnomalyEventId'] = None
            logger.info("æ²’æœ‰æ‰¾åˆ°å·²æ¨™è¨»çš„ç•°å¸¸äº‹ä»¶")
            return df

        # è½‰æ›ç‚º DataFrame ä¸¦è™•ç†æ™‚é–“æ ¼å¼
        events_df = pd.DataFrame(positive_events, columns=['id', 'eventTimestamp', 'meterId'])
        events_df['eventTimestamp'] = pd.to_datetime(events_df['eventTimestamp'])

        # ç¢ºä¿å…©å€‹ DataFrame éƒ½æŒ‰æ™‚é–“æ’åº
        df = df.sort_values('timestamp').reset_index(drop=True)
        events_df = events_df.sort_values('eventTimestamp').reset_index(drop=True)

        tolerance_minutes = self.config["anomaly_time_tolerance_minutes"]
        logger.info(f"ä½¿ç”¨ç•°å¸¸äº‹ä»¶æ™‚é–“é—œè¯å®¹å·®: Â±{tolerance_minutes} åˆ†é˜")

        # ä½¿ç”¨ merge_asof é€²è¡Œé«˜æ•ˆç¯„åœæŸ¥æ‰¾
        merged_df = pd.merge_asof(
            left=df,
            right=events_df,
            left_on='timestamp',
            right_on='eventTimestamp',
            direction='nearest',  # æ‰¾åˆ°æœ€è¿‘çš„äº‹ä»¶
            tolerance=pd.Timedelta(minutes=tolerance_minutes)
        )

        # æ ¹æ“šåˆä½µçµæœè¨­ç½®æ¨™ç±¤
        df['isPositiveLabel'] = ~merged_df['id'].isnull()
        df['sourceAnomalyEventId'] = merged_df['id']

        positive_count = df['isPositiveLabel'].sum()
        logger.info(f"é«˜æ•ˆæ¨™è¨˜äº† {positive_count} ç­†æ­£æ¨£æœ¬æ•¸æ“š")

        return df

    async def load_data(
        self,
        df: pd.DataFrame,
        room_info: RoomInfo,
        start_date: datetime,
        end_date: datetime
    ) -> str:
        """
        æ­¥é©Ÿå››ï¼šè¼‰å…¥æ•¸æ“š (Load)

        å°‡è™•ç†å¥½çš„æ•¸æ“šè¼‰å…¥åˆ°åˆ†ææ•¸æ“šåº«è¡¨ä¸­ï¼Œä½¿ç”¨äº¤æ˜“å®‰å…¨å’Œé«˜æ•ˆæ‰¹æ¬¡æ“ä½œ

        Args:
            df: æœ€çµ‚è™•ç†å®Œæˆçš„æ•¸æ“š
            room_info: æˆ¿é–“è³‡è¨Š
            start_date: æ•¸æ“šèµ·å§‹æ—¥æœŸ
            end_date: æ•¸æ“šçµæŸæ—¥æœŸ

        Returns:
            str: å‰µå»ºçš„æ•¸æ“šé›† ID
        """
        logger.info("é–‹å§‹è¼‰å…¥æ•¸æ“šåˆ°æ•¸æ“šåº«...")

        positive_count = df['isPositiveLabel'].sum()
        total_count = len(df)
        dataset_name = f"{room_info.building}-{room_info.floor}-{room_info.room}-Golden-Week-{start_date.strftime('%Y-%m')}"

        # ä½¿ç”¨æ•¸æ“šåº«äº¤æ˜“ç¢ºä¿æ•¸æ“šä¸€è‡´æ€§
        async with self.conn.transaction():
            # 4.1 å‰µå»º AnalysisDataset ç´€éŒ„
            insert_dataset_query = """
            INSERT INTO analysis_datasets (
                name, building, floor, room, start_date, end_date,
                occupant_type, meter_id_l1, meter_id_l2,
                total_records, positive_labels
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
            RETURNING id
            """

            dataset_id = await self.conn.fetchval(
                insert_dataset_query,
                dataset_name, room_info.building, room_info.floor, room_info.room,
                start_date, end_date, room_info.occupant_type.value,
                room_info.meter_id_l1, room_info.meter_id_l2,
                total_count, int(positive_count)
            )

            logger.info(f"å‰µå»ºæ•¸æ“šé›†: {dataset_name} (ID: {dataset_id})")

            # 4.2 ä½¿ç”¨é«˜æ•ˆæ‰¹æ¬¡è¤‡è£½æ’å…¥æ•¸æ“šè¨˜éŒ„
            await self.copy_analysis_data_to_table(df, dataset_id)

        logger.info(f"æˆåŠŸè¼‰å…¥ {total_count} ç­†åˆ†ææ•¸æ“š")
        logger.info(f"å…¶ä¸­åŒ…å« {positive_count} ç­†æ­£æ¨£æœ¬æ¨™ç±¤")

        return dataset_id

    async def copy_analysis_data_to_table(self, df: pd.DataFrame, dataset_id: str):
        """
        ä½¿ç”¨é«˜æ•ˆæ‰¹æ¬¡æ’å…¥åˆ†ææ•¸æ“š

        Args:
            df: è¦æ’å…¥çš„æ•¸æ“š DataFrame
            dataset_id: æ•¸æ“šé›† ID
        """
        if df.empty:
            logger.warning("æ²’æœ‰æ•¸æ“šéœ€è¦æ’å…¥")
            return

        # æº–å‚™æ‰¹æ¬¡æ’å…¥æŸ¥è©¢
        insert_query = """
        INSERT INTO analysis_ready_data (
            dataset_id, timestamp, room, raw_wattage_l1, raw_wattage_l2,
            wattage_110v, wattage_220v, wattage_total,
            is_positive_label, source_anomaly_event_id
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
        """

        # è™•ç† NaN å€¼å’Œæ•¸æ“šæ¸…ç†
        df_clean = df.copy()

        # å°æ–¼æœ‰é‡è¤‡ source_anomaly_event_id çš„è¡Œï¼Œé™¤äº†ç¬¬ä¸€å€‹ä»¥å¤–éƒ½è¨­ç‚º None
        # é€™æ¨£å¯ä»¥é¿å…å”¯ä¸€æ€§ç´„æŸè¡çª
        df_clean['sourceAnomalyEventId'] = df_clean['sourceAnomalyEventId'].where(
            df_clean['sourceAnomalyEventId'].notna(), None
        )

        # è™•ç†é‡è¤‡çš„ sourceAnomalyEventId - åªä¿ç•™ç¬¬ä¸€å€‹
        if 'sourceAnomalyEventId' in df_clean.columns:
            # æ¨™è¨˜é‡è¤‡é …ï¼ˆé™¤äº†ç¬¬ä¸€å€‹ï¼‰
            duplicated_mask = df_clean.duplicated(subset=['sourceAnomalyEventId'], keep='first')
            # å°‡é‡è¤‡çš„è¨­ç‚º None
            df_clean.loc[duplicated_mask & df_clean['sourceAnomalyEventId'].notna(), 'sourceAnomalyEventId'] = None

        df_clean['isPositiveLabel'] = df_clean['isPositiveLabel'].fillna(False)

        # æº–å‚™æ‰¹æ¬¡æ•¸æ“š - ä½¿ç”¨åˆ—è¡¨æ¨å°å¼æé«˜æ•ˆç‡
        batch_data = []
        for _, row in df_clean.iterrows():
            batch_data.append((
                dataset_id,
                row['timestamp'],
                row['room'],
                row['rawWattageL1'],
                row['rawWattageL2'],
                row['wattage110v_current'],  # ä½¿ç”¨ç•¶å‰æ™‚é–“é»çš„åŠŸç‡å€¼
                row['wattage220v_current'],
                row['wattageTotal_current'],
                bool(row['isPositiveLabel']),  # ç¢ºä¿æ˜¯å¸ƒæ—å€¼
                row['sourceAnomalyEventId']    # å¯èƒ½æ˜¯ None
            ))

        # è¨ˆç®—æ­£æ¨£æœ¬æ•¸é‡
        positive_count = sum(1 for item in batch_data if item[8])

        # åŸ·è¡Œæ‰¹æ¬¡æ’å…¥
        await self.conn.executemany(insert_query, batch_data)

        logger.info(f"æˆåŠŸæ‰¹æ¬¡æ’å…¥ {len(batch_data)} ç­†åˆ†ææ•¸æ“š")
        logger.info(f"å…¶ä¸­åŒ…å« {positive_count} ç­†æ­£æ¨£æœ¬æ¨™ç±¤")

        return dataset_id

    async def process_room_data(
        self,
        room_info: RoomInfo,
        search_start: datetime,
        search_end: datetime,
        window_days: int = 7,
        enable_multiscale_features: bool = True
    ) -> str:
        """
        åŸ·è¡Œå®Œæ•´çš„ ETL æµç¨‹è™•ç†å–®å€‹æˆ¿é–“çš„æ•¸æ“š

        Args:
            room_info: æˆ¿é–“è³‡è¨Š
            search_start: æœç´¢é–‹å§‹æ—¥æœŸ
            search_end: æœç´¢çµæŸæ—¥æœŸ
            window_days: æ•¸æ“šçª—å£å¤©æ•¸
            enable_multiscale_features: æ˜¯å¦å•Ÿç”¨å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹

        Returns:
            str: å‰µå»ºçš„æ•¸æ“šé›† ID
        """
        logger.info(f"é–‹å§‹è™•ç†æˆ¿é–“ {room_info.room} çš„æ•¸æ“š")
        logger.info(f"å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹: {'å•Ÿç”¨' if enable_multiscale_features else 'åœç”¨'}")

        try:
            # æ­¥é©Ÿä¸€ï¼šå°‹æ‰¾æœ€ä½³æ•¸æ“šçª—å£
            start_date, end_date = await self.find_golden_window(
                room_info, search_start, search_end, window_days
            )

            # æ­¥é©ŸäºŒï¼šæŠ½å–åŸå§‹æ•¸æ“š
            df_l1, df_l2 = await self.extract_raw_data(room_info, start_date, end_date)

            # æ­¥é©Ÿä¸‰ï¼šè½‰æ›æ•¸æ“š
            df_transformed = self.transform_data(df_l1, df_l2, room_info)

            # æ­¥é©Ÿä¸‰ï¼šè±å¯ŒçœŸå¯¦æ¨™ç±¤
            df_enriched = await self.enrich_ground_truth(df_transformed)

            # æ–°æ­¥é©Ÿï¼šå¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹
            if enable_multiscale_features:
                df_final = self.generate_multiscale_features(df_enriched)
                if len(df_final) == 0:
                    logger.warning("å¤šå°ºåº¦ç‰¹å¾µç”Ÿæˆå¤±æ•—ï¼Œå›é€€åˆ°åŸºç¤æ•¸æ“š")
                    df_final = df_enriched
            else:
                df_final = df_enriched

            # æ­¥é©Ÿå››ï¼šè¼‰å…¥æ•¸æ“š
            dataset_id = await self.load_data(df_final, room_info, start_date, end_date)

            logger.info(f"æˆ¿é–“ {room_info.room} æ•¸æ“šè™•ç†å®Œæˆï¼Œæ•¸æ“šé›† ID: {dataset_id}")
            return dataset_id

        except Exception as e:
            logger.error(f"è™•ç†æˆ¿é–“ {room_info.room} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

    async def process_multiple_rooms_from_csv(
        self,
        csv_path: str = "meter.csv",
        search_start: datetime = None,
        search_end: datetime = None,
        window_days: int = 7,
        enable_multiscale_features: bool = True,
        max_concurrent: int = 3
    ) -> List[str]:
        """
        å¾ CSV æ–‡ä»¶æ‰¹é‡è™•ç†å¤šå€‹æˆ¿é–“çš„æ•¸æ“š

        Args:
            csv_path: é›»è¡¨æ˜ å°„ CSV æ–‡ä»¶è·¯å¾‘
            search_start: æœç´¢é–‹å§‹æ—¥æœŸï¼ˆé»˜èªç‚ºç•¶æœˆç¬¬ä¸€å¤©ï¼‰
            search_end: æœç´¢çµæŸæ—¥æœŸï¼ˆé»˜èªç‚ºç•¶æœˆæœ€å¾Œä¸€å¤©ï¼‰
            window_days: æ•¸æ“šçª—å£å¤©æ•¸
            enable_multiscale_features: æ˜¯å¦å•Ÿç”¨å¤šå°ºåº¦ç‰¹å¾µå·¥ç¨‹
            max_concurrent: æœ€å¤§ä¸¦ç™¼è™•ç†æ•¸é‡

        Returns:
            List[str]: æˆåŠŸå‰µå»ºçš„æ•¸æ“šé›† ID åˆ—è¡¨
        """
        logger.info("é–‹å§‹æ‰¹é‡è™•ç†å¤šå€‹æˆ¿é–“...")

        # è¨­ç½®é»˜èªæ™‚é–“ç¯„åœï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¨­å®šï¼‰
        if search_start is None:
            try:
                current_month = get_time_range("2025_08")
                search_start = current_month["start"]
            except:
                search_start = datetime(2025, 8, 1)

        if search_end is None:
            try:
                current_month = get_time_range("2025_08")
                search_end = current_month["end"]
            except:
                search_end = datetime(2025, 8, 31)

        logger.info(f"è™•ç†æ™‚é–“ç¯„åœ: {search_start} åˆ° {search_end}")

        # è¼‰å…¥é›»è¡¨æ˜ å°„
        meter_mapping = self.load_meter_mapping(csv_path)

        # å‰µå»ºæˆ¿é–“è³‡è¨Šåˆ—è¡¨
        room_infos = self.create_room_info_from_mapping(meter_mapping)

        if not room_infos:
            logger.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½•æˆ¿é–“è³‡è¨Š")
            return []

        logger.info(f"æº–å‚™è™•ç† {len(room_infos)} å€‹æˆ¿é–“")

        # æ‰¹é‡è™•ç†çµæœ
        successful_datasets = []
        failed_rooms = []

        # åˆ†æ‰¹è™•ç†ä»¥é¿å…éè¼‰
        for i in range(0, len(room_infos), max_concurrent):
            batch = room_infos[i:i + max_concurrent]
            batch_size = len(batch)

            logger.info(f"è™•ç†ç¬¬ {i//max_concurrent + 1} æ‰¹æ¬¡ ({batch_size} å€‹æˆ¿é–“)...")

            # ä½µç™¼è™•ç†ç•¶å‰æ‰¹æ¬¡
            tasks = []
            for room_info in batch:
                task = self.process_room_data(
                    room_info=room_info,
                    search_start=search_start,
                    search_end=search_end,
                    window_days=window_days,
                    enable_multiscale_features=enable_multiscale_features
                )
                tasks.append((room_info, task))

            # ç­‰å¾…ç•¶å‰æ‰¹æ¬¡å®Œæˆ
            for room_info, task in tasks:
                try:
                    dataset_id = await task
                    successful_datasets.append(dataset_id)
                    logger.info(f"âœ… æˆ¿é–“ {room_info.building}-{room_info.floor}-{room_info.room} è™•ç†æˆåŠŸ: {dataset_id}")

                except Exception as e:
                    failed_rooms.append(f"{room_info.building}-{room_info.floor}-{room_info.room}")
                    logger.error(f"âŒ æˆ¿é–“ {room_info.building}-{room_info.floor}-{room_info.room} è™•ç†å¤±æ•—: {e}")

        # è™•ç†çµæœæ‘˜è¦
        total_rooms = len(room_infos)
        successful_count = len(successful_datasets)
        failed_count = len(failed_rooms)

        logger.info("=" * 50)
        logger.info("æ‰¹é‡è™•ç†å®Œæˆæ‘˜è¦:")
        logger.info(f"ç¸½æˆ¿é–“æ•¸: {total_rooms}")
        logger.info(f"æˆåŠŸè™•ç†: {successful_count}")
        logger.info(f"è™•ç†å¤±æ•—: {failed_count}")
        logger.info(f"æˆåŠŸç‡: {successful_count/total_rooms:.1%}")

        if failed_rooms:
            logger.warning("è™•ç†å¤±æ•—çš„æˆ¿é–“:")
            for room in failed_rooms:
                logger.warning(f"  - {room}")

        logger.info("æˆåŠŸå‰µå»ºçš„æ•¸æ“šé›†:")
        for i, dataset_id in enumerate(successful_datasets, 1):
            logger.info(f"  {i}. {dataset_id}")

        return successful_datasets

# ä½¿ç”¨ç¯„ä¾‹å’Œå·¥å…·å‡½æ•¸
async def main():
    """
    ä¸»å‡½æ•¸ - è™•ç†é«˜å“è³ªé€±æœŸæ•¸æ“šï¼ˆåŸºæ–¼æ•¸æ“šå“è³ªåˆ†æçµæœï¼‰
    å°ˆé–€è™•ç†è¡¨ç¾æœ€ä½³çš„é€±æœŸï¼š2025-07-21, 2025-08-04, 2025-08-11, 2025-08-18
    ä½¿ç”¨ asyncio ä½µç™¼è™•ç†æå‡æ€§èƒ½
    """
    # å¾é…ç½®æ–‡ä»¶è®€å–è³‡æ–™åº«é€£æ¥å­—ä¸²
    database_url = get_database_url()

    # ä½¿ç”¨é«˜å“è³ªæˆ¿é–“é…ç½®
    from etl_config import get_etl_config, get_high_quality_rooms
    config = get_etl_config()

    # åˆå§‹åŒ– ETL è™•ç†å™¨
    etl = DataPreprocessingETL(database_url, config)

    try:
        await etl.connect_database()
        logger.info("ğŸ”— è³‡æ–™åº«é€£æ¥æˆåŠŸ")

        # ç²å–é«˜å“è³ªæˆ¿é–“åˆ—è¡¨ï¼ˆA2ã€A3ã€A5æ¨“å±¤ï¼‰
        high_quality_rooms = get_high_quality_rooms()
        logger.info(f"ğŸ  è¼‰å…¥é«˜å“è³ªæˆ¿é–“æ•¸é‡: {len(high_quality_rooms)}")

        # è™•ç†é«˜å“è³ªé€±æœŸæ™‚é–“ç¯„åœï¼ˆæ•´é«”è™•ç†ï¼‰
        time_range = get_time_range("high_quality_weeks")
        start_time = time_range["start"]
        end_time = time_range["end"]

        logger.info(f"\nğŸ“… é–‹å§‹è™•ç†é€±æœŸ: {time_range['name']}")
        logger.info(f"   æ™‚é–“ç¯„åœ: {start_time} è‡³ {end_time}")

        # ä½¿ç”¨ asyncio.gather ä½µç™¼è™•ç†å¤šå€‹æˆ¿é–“
        max_concurrent = min(3, len(high_quality_rooms))  # é™åˆ¶ä½µç™¼æ•¸é¿å…è³‡æ–™åº«å£“åŠ›
        logger.info(f"ğŸš€ ä½¿ç”¨ä½µç™¼è™•ç†ï¼Œæœ€å¤§ä½µç™¼æ•¸: {max_concurrent}")

        async def process_single_room(room: RoomInfo) -> tuple[RoomInfo, str | None]:
            """è™•ç†å–®å€‹æˆ¿é–“çš„åŒ…è£å‡½æ•¸"""
            try:
                logger.info(f"\nğŸ¢ é–‹å§‹è™•ç†æˆ¿é–“: {room.building}-{room.floor}-{room.room}")

                # è™•ç†å–®å€‹æˆ¿é–“çš„æ•¸æ“š
                result = await etl.process_room_data(
                    room_info=room,
                    search_start=start_time,
                    search_end=end_time,
                    window_days=7,
                    enable_multiscale_features=True  # å•Ÿç”¨å¤šå°ºåº¦ç‰¹å¾µ
                )

                if result:
                    logger.info(f"âœ… æˆ¿é–“ {room.building}-{room.floor}-{room.room} è™•ç†æˆåŠŸ")
                    logger.info(f"   æ•¸æ“šé›†ID: {result}")
                    return room, result
                else:
                    logger.warning(f"âš ï¸  æˆ¿é–“ {room.building}-{room.floor}-{room.room} è™•ç†å¤±æ•—æˆ–ç„¡ç¬¦åˆæ¢ä»¶çš„æ•¸æ“š")
                    return room, None

            except Exception as e:
                logger.error(f"âŒ æˆ¿é–“ {room.building}-{room.floor}-{room.room} è™•ç†å¤±æ•—: {e}")
                return room, None

        # å°‡æˆ¿é–“åˆ†æ‰¹ä½µç™¼è™•ç†
        success_count = 0
        total_count = len(high_quality_rooms)
        results = []

        # åˆ†æ‰¹è™•ç†æˆ¿é–“ä»¥æ§åˆ¶ä½µç™¼æ•¸
        for i in range(0, len(high_quality_rooms), max_concurrent):
            batch_rooms = high_quality_rooms[i:i + max_concurrent]
            batch_number = i // max_concurrent + 1
            total_batches = (len(high_quality_rooms) + max_concurrent - 1) // max_concurrent

            logger.info(f"\nğŸ“¦ è™•ç†æ‰¹æ¬¡ {batch_number}/{total_batches} ({len(batch_rooms)} å€‹æˆ¿é–“)")

            # ä½µç™¼è™•ç†ç•¶å‰æ‰¹æ¬¡çš„æˆ¿é–“
            batch_tasks = [process_single_room(room) for room in batch_rooms]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # çµ±è¨ˆæ‰¹æ¬¡çµæœ
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.error(f"æ‰¹æ¬¡è™•ç†ä¸­ç™¼ç”Ÿç•°å¸¸: {result}")
                    continue

                room, dataset_id = result
                results.append((room, dataset_id))
                if dataset_id:
                    success_count += 1

        logger.info(f"\nğŸ“Š é«˜å“è³ªé€±æœŸè™•ç†å®Œæˆ:")
        logger.info(f"   æˆåŠŸ: {success_count}/{total_count} å€‹æˆ¿é–“")
        logger.info(f"   æˆåŠŸç‡: {(success_count/total_count)*100:.1f}%")

        # é¡¯ç¤ºè™•ç†çµæœæ‘˜è¦
        successful_rooms = [room for room, dataset_id in results if dataset_id]
        failed_rooms = [room for room, dataset_id in results if not dataset_id]

        if successful_rooms:
            logger.info(f"\nâœ… æˆåŠŸè™•ç†çš„æˆ¿é–“:")
            for room in successful_rooms:
                logger.info(f"   - {room.building}-{room.floor}-{room.room}")

        if failed_rooms:
            logger.info(f"\nâŒ è™•ç†å¤±æ•—çš„æˆ¿é–“:")
            for room in failed_rooms:
                logger.info(f"   - {room.building}-{room.floor}-{room.room}")

        logger.info(f"\nğŸ‰ æ‰€æœ‰é«˜å“è³ªæˆ¿é–“è™•ç†å®Œæˆï¼")

    except Exception as e:
        logger.error(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise
    finally:
        await etl.close_database()
        logger.info("ğŸ”Œ è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

if __name__ == "__main__":
    # é‹è¡Œé è¨“ç·´æ•¸æ“šè™•ç†
    asyncio.run(main())

