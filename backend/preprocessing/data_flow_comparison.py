#!/usr/bin/env python3
"""
æ•¸æ“šæµæ¯”å°åˆ†æè…³æœ¬
æ¯”å°å¾ chunk -> deviceNumber -> æœ€çµ‚ç›®éŒ„ çš„æ•¸æ“šè®ŠåŒ–
åˆ†ææ•¸æ“šä¸Ÿå¤±çš„åŸå› å’Œä½ç½®
"""

import pandas as pd
import os
import logging
from pathlib import Path
import time
from collections import defaultdict

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_comparison.log'),
        logging.StreamHandler()
    ]
)

def analyze_data_flow():
    """åˆ†ææ•¸æ“šå¾ chunk åˆ°æœ€çµ‚ç›®éŒ„çš„æµç¨‹è®ŠåŒ–"""

    # å®šç¾©è·¯å¾‘
    chunk_dir = "/home/infowin/Git-projects/pu-in-practice/backend/preprocessing/backup_20250823_235656/processed_data/chunk"
    device_dir = "/home/infowin/Git-projects/pu-in-practice/backend/preprocessing/backup_20250823_235656/processed_data/deviceNumber"
    final_dir = "/home/infowin/Git-projects/pu-in-practice/backend/preprocessing/backup_2025-07-21_2025-08-23"

    logging.info("=" * 80)
    logging.info("é–‹å§‹æ•¸æ“šæµæ¯”å°åˆ†æ")
    logging.info("=" * 80)

    # éšæ®µ 1: åˆ†æ chunk æª”æ¡ˆ
    logging.info("ğŸ“Š éšæ®µ 1: åˆ†æ chunk æª”æ¡ˆ")
    chunk_stats = analyze_chunk_files(chunk_dir)

    # éšæ®µ 2: åˆ†æ deviceNumber æª”æ¡ˆ
    logging.info("ğŸ“Š éšæ®µ 2: åˆ†æ deviceNumber æª”æ¡ˆ")
    device_stats = analyze_device_files(device_dir)

    # éšæ®µ 3: åˆ†ææœ€çµ‚æª”æ¡ˆ
    logging.info("ğŸ“Š éšæ®µ 3: åˆ†ææœ€çµ‚æª”æ¡ˆ")
    final_stats = analyze_final_files(final_dir)

    # éšæ®µ 4: æ¯”å°åˆ†æ
    logging.info("ğŸ“Š éšæ®µ 4: æ•¸æ“šæµæ¯”å°åˆ†æ")
    compare_data_flow(chunk_stats, device_stats, final_stats)

    # éšæ®µ 5: æ·±åº¦åˆ†æ - æª¢æŸ¥å…·é«”æ•¸æ“šå·®ç•°
    logging.info("ğŸ“Š éšæ®µ 5: æ·±åº¦åˆ†æ - æª¢æŸ¥æ•¸æ“šå·®ç•°åŸå› ")
    deep_analysis(chunk_dir, device_dir, final_dir)

def analyze_chunk_files(chunk_dir):
    """åˆ†æ chunk æª”æ¡ˆ"""
    chunk_path = Path(chunk_dir)
    chunk_files = sorted(list(chunk_path.glob("*.csv")))

    total_records = 0
    device_records = defaultdict(int)
    duplicate_count = 0

    logging.info(f"ç™¼ç¾ {len(chunk_files)} å€‹ chunk æª”æ¡ˆ")

    for i, chunk_file in enumerate(chunk_files):
        try:
            df = pd.read_csv(chunk_file)
            records = len(df)
            total_records += records

            # çµ±è¨ˆæ¯å€‹è¨­å‚™çš„è¨˜éŒ„æ•¸
            if 'deviceNumber' in df.columns:
                device_counts = df['deviceNumber'].value_counts()
                for device, count in device_counts.items():
                    device_records[device] += count

            # æª¢æŸ¥é‡è¤‡è¨˜éŒ„ (å¦‚æœæœ‰ id æ¬„ä½)
            if 'id' in df.columns:
                duplicates = df.duplicated(subset=['id']).sum()
                duplicate_count += duplicates
                if duplicates > 0:
                    logging.warning(f"  {chunk_file.name}: ç™¼ç¾ {duplicates} ç­†é‡è¤‡ ID")

            if (i + 1) % 5 == 0:
                logging.info(f"  å·²è™•ç† {i + 1}/{len(chunk_files)} å€‹æª”æ¡ˆï¼Œç´¯è¨ˆ {total_records:,} ç­†è¨˜éŒ„")

        except Exception as e:
            logging.error(f"è™•ç† {chunk_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    chunk_stats = {
        'total_records': total_records,
        'device_count': len(device_records),
        'device_records': dict(device_records),
        'duplicate_count': duplicate_count,
        'files_count': len(chunk_files)
    }

    logging.info(f"Chunk éšæ®µçµ±è¨ˆ:")
    logging.info(f"  - ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
    logging.info(f"  - è¨­å‚™æ•¸é‡: {len(device_records)}")
    logging.info(f"  - é‡è¤‡è¨˜éŒ„: {duplicate_count:,}")
    logging.info(f"  - æª”æ¡ˆæ•¸é‡: {len(chunk_files)}")

    return chunk_stats

def analyze_device_files(device_dir):
    """åˆ†æ deviceNumber æª”æ¡ˆ"""
    device_path = Path(device_dir)
    device_files = sorted(list(device_path.glob("device_*.csv")))

    total_records = 0
    device_records = {}
    duplicate_count = 0

    logging.info(f"ç™¼ç¾ {len(device_files)} å€‹è¨­å‚™æª”æ¡ˆ")

    for i, device_file in enumerate(device_files):
        try:
            df = pd.read_csv(device_file)
            records = len(df)
            total_records += records

            # å¾æª”åæå–è¨­å‚™è™Ÿ
            device_name = device_file.stem.replace('device_', '')
            device_records[device_name] = records

            # æª¢æŸ¥é‡è¤‡è¨˜éŒ„
            if 'id' in df.columns:
                duplicates = df.duplicated(subset=['id']).sum()
                duplicate_count += duplicates
                if duplicates > 0:
                    logging.warning(f"  {device_file.name}: ç™¼ç¾ {duplicates} ç­†é‡è¤‡ ID")

            if (i + 1) % 20 == 0:
                logging.info(f"  å·²è™•ç† {i + 1}/{len(device_files)} å€‹æª”æ¡ˆï¼Œç´¯è¨ˆ {total_records:,} ç­†è¨˜éŒ„")

        except Exception as e:
            logging.error(f"è™•ç† {device_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    device_stats = {
        'total_records': total_records,
        'device_count': len(device_records),
        'device_records': device_records,
        'duplicate_count': duplicate_count,
        'files_count': len(device_files)
    }

    logging.info(f"DeviceNumber éšæ®µçµ±è¨ˆ:")
    logging.info(f"  - ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
    logging.info(f"  - è¨­å‚™æ•¸é‡: {len(device_records)}")
    logging.info(f"  - é‡è¤‡è¨˜éŒ„: {duplicate_count:,}")
    logging.info(f"  - æª”æ¡ˆæ•¸é‡: {len(device_files)}")

    return device_stats

def analyze_final_files(final_dir):
    """åˆ†ææœ€çµ‚æª”æ¡ˆ"""
    final_path = Path(final_dir)
    if not final_path.exists():
        logging.error(f"æœ€çµ‚ç›®éŒ„ä¸å­˜åœ¨: {final_dir}")
        return {}

    final_files = sorted(list(final_path.glob("device_*.csv")))

    total_records = 0
    device_records = {}

    logging.info(f"ç™¼ç¾ {len(final_files)} å€‹æœ€çµ‚æª”æ¡ˆ")

    for i, final_file in enumerate(final_files):
        try:
            df = pd.read_csv(final_file)
            records = len(df)
            total_records += records

            # å¾æª”åæå–è¨­å‚™è™Ÿ
            device_name = final_file.stem.replace('device_', '')
            device_records[device_name] = records

            if (i + 1) % 20 == 0:
                logging.info(f"  å·²è™•ç† {i + 1}/{len(final_files)} å€‹æª”æ¡ˆï¼Œç´¯è¨ˆ {total_records:,} ç­†è¨˜éŒ„")

        except Exception as e:
            logging.error(f"è™•ç† {final_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    final_stats = {
        'total_records': total_records,
        'device_count': len(device_records),
        'device_records': device_records,
        'files_count': len(final_files)
    }

    logging.info(f"æœ€çµ‚éšæ®µçµ±è¨ˆ:")
    logging.info(f"  - ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
    logging.info(f"  - è¨­å‚™æ•¸é‡: {len(device_records)}")
    logging.info(f"  - æª”æ¡ˆæ•¸é‡: {len(final_files)}")

    return final_stats

def compare_data_flow(chunk_stats, device_stats, final_stats):
    """æ¯”å°æ•¸æ“šæµçš„è®ŠåŒ–"""
    logging.info("=" * 80)
    logging.info("ğŸ” æ•¸æ“šæµæ¯”å°çµæœ")
    logging.info("=" * 80)

    # ç¸½è¨˜éŒ„æ•¸æ¯”è¼ƒ
    chunk_total = chunk_stats.get('total_records', 0)
    device_total = device_stats.get('total_records', 0)
    final_total = final_stats.get('total_records', 0)

    logging.info(f"ğŸ“ˆ ç¸½è¨˜éŒ„æ•¸è®ŠåŒ–:")
    logging.info(f"  Chunk éšæ®µ:        {chunk_total:,} ç­†")
    logging.info(f"  DeviceNumber éšæ®µ: {device_total:,} ç­†")
    logging.info(f"  æœ€çµ‚éšæ®µ:          {final_total:,} ç­†")

    # è¨ˆç®—æå¤±
    chunk_to_device_loss = chunk_total - device_total
    device_to_final_loss = device_total - final_total
    total_loss = chunk_total - final_total

    logging.info(f"ğŸ“‰ æ•¸æ“šæå¤±åˆ†æ:")
    logging.info(f"  Chunk -> DeviceNumber: {chunk_to_device_loss:,} ç­† ({chunk_to_device_loss/chunk_total*100:.2f}%)")
    logging.info(f"  DeviceNumber -> æœ€çµ‚:  {device_to_final_loss:,} ç­† ({device_to_final_loss/device_total*100:.2f}%)")
    logging.info(f"  ç¸½æå¤±:              {total_loss:,} ç­† ({total_loss/chunk_total*100:.2f}%)")

    # è¨­å‚™æ•¸é‡æ¯”è¼ƒ
    chunk_devices = chunk_stats.get('device_count', 0)
    device_devices = device_stats.get('device_count', 0)
    final_devices = final_stats.get('device_count', 0)

    logging.info(f"ğŸ“± è¨­å‚™æ•¸é‡è®ŠåŒ–:")
    logging.info(f"  Chunk éšæ®µ:        {chunk_devices} å€‹è¨­å‚™")
    logging.info(f"  DeviceNumber éšæ®µ: {device_devices} å€‹è¨­å‚™")
    logging.info(f"  æœ€çµ‚éšæ®µ:          {final_devices} å€‹è¨­å‚™")

    # æª”æ¡ˆæ•¸é‡æ¯”è¼ƒ
    chunk_files = chunk_stats.get('files_count', 0)
    device_files = device_stats.get('files_count', 0)
    final_files = final_stats.get('files_count', 0)

    logging.info(f"ğŸ“ æª”æ¡ˆæ•¸é‡è®ŠåŒ–:")
    logging.info(f"  Chunk éšæ®µ:        {chunk_files} å€‹æª”æ¡ˆ")
    logging.info(f"  DeviceNumber éšæ®µ: {device_files} å€‹æª”æ¡ˆ")
    logging.info(f"  æœ€çµ‚éšæ®µ:          {final_files} å€‹æª”æ¡ˆ")

def deep_analysis(chunk_dir, device_dir, final_dir):
    """æ·±åº¦åˆ†ææ•¸æ“šå·®ç•°çš„å…·é«”åŸå› """
    logging.info("=" * 80)
    logging.info("ğŸ”¬ æ·±åº¦åˆ†æ - æ•¸æ“šå·®ç•°åŸå› ")
    logging.info("=" * 80)

    # 1. æª¢æŸ¥é‡è¤‡è¨˜éŒ„å½±éŸ¿
    logging.info("1ï¸âƒ£ åˆ†æé‡è¤‡è¨˜éŒ„çš„å½±éŸ¿")
    analyze_duplicates(chunk_dir, device_dir)

    # 2. æª¢æŸ¥æ¨™é¡Œè¡Œå½±éŸ¿
    logging.info("2ï¸âƒ£ åˆ†ææ¨™é¡Œè¡Œçš„å½±éŸ¿")
    analyze_headers(chunk_dir, device_dir, final_dir)

    # 3. æª¢æŸ¥è¨­å‚™åˆ†å¸ƒå·®ç•°
    logging.info("3ï¸âƒ£ åˆ†æè¨­å‚™åˆ†å¸ƒå·®ç•°")
    analyze_device_distribution(chunk_dir, device_dir)

    # 4. æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
    logging.info("4ï¸âƒ£ æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§")
    analyze_data_integrity(device_dir, final_dir)

def analyze_duplicates(chunk_dir, device_dir):
    """åˆ†æé‡è¤‡è¨˜éŒ„çš„å½±éŸ¿"""
    try:
        # æª¢æŸ¥ç¬¬ä¸€å€‹ chunk æª”æ¡ˆçš„é‡è¤‡æƒ…æ³
        chunk_path = Path(chunk_dir)
        chunk_files = sorted(list(chunk_path.glob("*.csv")))

        if chunk_files:
            sample_chunk = chunk_files[0]
            df_chunk = pd.read_csv(sample_chunk)

            if 'id' in df_chunk.columns:
                total_chunk = len(df_chunk)
                unique_chunk = df_chunk['id'].nunique()
                duplicates_chunk = total_chunk - unique_chunk

                logging.info(f"  ç¯„ä¾‹ chunk æª”æ¡ˆ ({sample_chunk.name}):")
                logging.info(f"    ç¸½è¨˜éŒ„æ•¸: {total_chunk}")
                logging.info(f"    å”¯ä¸€ ID: {unique_chunk}")
                logging.info(f"    é‡è¤‡è¨˜éŒ„: {duplicates_chunk}")

        # æª¢æŸ¥å°æ‡‰çš„è¨­å‚™æª”æ¡ˆ
        device_path = Path(device_dir)
        device_files = list(device_path.glob("device_*.csv"))

        if device_files:
            sample_device = device_files[0]
            df_device = pd.read_csv(sample_device)

            if 'id' in df_device.columns:
                total_device = len(df_device)
                unique_device = df_device['id'].nunique()
                duplicates_device = total_device - unique_device

                logging.info(f"  ç¯„ä¾‹è¨­å‚™æª”æ¡ˆ ({sample_device.name}):")
                logging.info(f"    ç¸½è¨˜éŒ„æ•¸: {total_device}")
                logging.info(f"    å”¯ä¸€ ID: {unique_device}")
                logging.info(f"    é‡è¤‡è¨˜éŒ„: {duplicates_device}")

    except Exception as e:
        logging.error(f"åˆ†æé‡è¤‡è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def analyze_headers(chunk_dir, device_dir, final_dir):
    """åˆ†ææ¨™é¡Œè¡Œçš„å½±éŸ¿"""
    try:
        # è¨ˆç®—å„éšæ®µçš„æ¨™é¡Œè¡Œæ•¸é‡
        chunk_path = Path(chunk_dir)
        chunk_files = list(chunk_path.glob("*.csv"))
        chunk_headers = len(chunk_files)  # æ¯å€‹æª”æ¡ˆä¸€å€‹æ¨™é¡Œè¡Œ

        device_path = Path(device_dir)
        device_files = list(device_path.glob("device_*.csv"))
        device_headers = len(device_files)

        final_path = Path(final_dir)
        final_files = list(final_path.glob("device_*.csv")) if final_path.exists() else []
        final_headers = len(final_files)

        logging.info(f"  æ¨™é¡Œè¡Œæ•¸é‡å½±éŸ¿:")
        logging.info(f"    Chunk æª”æ¡ˆæ¨™é¡Œè¡Œ: {chunk_headers}")
        logging.info(f"    DeviceNumber æª”æ¡ˆæ¨™é¡Œè¡Œ: {device_headers}")
        logging.info(f"    æœ€çµ‚æª”æ¡ˆæ¨™é¡Œè¡Œ: {final_headers}")
        logging.info(f"    æ¨™é¡Œè¡Œå·®ç•°: {chunk_headers - device_headers}")

    except Exception as e:
        logging.error(f"åˆ†ææ¨™é¡Œè¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def analyze_device_distribution(chunk_dir, device_dir):
    """åˆ†æè¨­å‚™åˆ†å¸ƒå·®ç•°"""
    try:
        # å¾ chunk ä¸­çµ±è¨ˆè¨­å‚™åˆ†å¸ƒ
        chunk_path = Path(chunk_dir)
        chunk_files = sorted(list(chunk_path.glob("*.csv")))

        chunk_devices = set()
        chunk_device_records = defaultdict(int)

        # åªæª¢æŸ¥å‰å¹¾å€‹ chunk æª”æ¡ˆä»¥ç¯€çœæ™‚é–“
        for chunk_file in chunk_files[:3]:
            df = pd.read_csv(chunk_file)
            if 'deviceNumber' in df.columns:
                devices = df['deviceNumber'].unique()
                chunk_devices.update(devices)

                device_counts = df['deviceNumber'].value_counts()
                for device, count in device_counts.items():
                    chunk_device_records[device] += count

        # å¾ deviceNumber ç›®éŒ„çµ±è¨ˆè¨­å‚™åˆ†å¸ƒ
        device_path = Path(device_dir)
        device_files = list(device_path.glob("device_*.csv"))
        device_devices = set()

        for device_file in device_files:
            device_name = device_file.stem.replace('device_', '')
            device_devices.add(device_name)

        logging.info(f"  è¨­å‚™åˆ†å¸ƒåˆ†æ:")
        logging.info(f"    Chunk ä¸­ç™¼ç¾çš„è¨­å‚™ (å‰3æª”): {len(chunk_devices)}")
        logging.info(f"    DeviceNumber ç›®éŒ„ä¸­çš„è¨­å‚™: {len(device_devices)}")

        # æª¢æŸ¥æ˜¯å¦æœ‰è¨­å‚™éºå¤±
        missing_devices = chunk_devices - device_devices
        extra_devices = device_devices - chunk_devices

        if missing_devices:
            logging.warning(f"    éºå¤±çš„è¨­å‚™: {len(missing_devices)}")
            for device in list(missing_devices)[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                logging.warning(f"      - {device}")

        if extra_devices:
            logging.info(f"    é¡å¤–çš„è¨­å‚™: {len(extra_devices)}")
            for device in list(extra_devices)[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                logging.info(f"      - {device}")

    except Exception as e:
        logging.error(f"åˆ†æè¨­å‚™åˆ†å¸ƒæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def analyze_data_integrity(device_dir, final_dir):
    """æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"""
    try:
        final_path = Path(final_dir)
        if not final_path.exists():
            logging.warning(f"  æœ€çµ‚ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éå®Œæ•´æ€§æª¢æŸ¥")
            return

        # æ¯”è¼ƒç›¸åŒè¨­å‚™æª”æ¡ˆçš„è¨˜éŒ„æ•¸
        device_path = Path(device_dir)
        device_files = list(device_path.glob("device_*.csv"))
        final_files = list(final_path.glob("device_*.csv"))

        device_dict = {f.stem: f for f in device_files}
        final_dict = {f.stem: f for f in final_files}

        mismatches = 0
        total_device_records = 0
        total_final_records = 0

        for device_name in device_dict.keys():
            if device_name in final_dict:
                device_records = len(pd.read_csv(device_dict[device_name]))
                final_records = len(pd.read_csv(final_dict[device_name]))

                total_device_records += device_records
                total_final_records += final_records

                if device_records != final_records:
                    mismatches += 1
                    logging.warning(f"    {device_name}: {device_records} -> {final_records} ({device_records - final_records})")

        logging.info(f"  æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥:")
        logging.info(f"    è¨˜éŒ„æ•¸ä¸åŒ¹é…çš„æª”æ¡ˆ: {mismatches}")
        logging.info(f"    DeviceNumber ç¸½è¨˜éŒ„: {total_device_records}")
        logging.info(f"    æœ€çµ‚ç¸½è¨˜éŒ„: {total_final_records}")
        logging.info(f"    å·®ç•°: {total_device_records - total_final_records}")

    except Exception as e:
        logging.error(f"æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    try:
        start_time = time.time()
        analyze_data_flow()
        end_time = time.time()
        logging.info(f"åˆ†æå®Œæˆï¼Œç¸½è€—æ™‚: {end_time - start_time:.2f} ç§’")
    except KeyboardInterrupt:
        logging.info("åˆ†æè¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        logging.error(f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise
