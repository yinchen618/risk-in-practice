"""
PU å­¸ç¿’å¼•æ“æ ¸å¿ƒæ¨¡çµ„
å¯¦ç¾ uPU å’Œ nnPU æ¼”ç®—æ³•ï¼Œå°æ‡‰ MATLAB PU_SL.m çš„åŠŸèƒ½
"""
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from scipy.spatial.distance import cdist
from sklearn.model_selection import KFold
from typing import Tuple, Dict, List, Optional
import warnings

# å¿½ç•¥ä¸€äº›ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings('ignore')


class MLPClassifier(nn.Module):
    """å¤šå±¤æ„ŸçŸ¥å™¨åˆ†é¡å™¨ï¼Œç”¨æ–¼ nnPU"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 100, activation: str = 'relu'):
        super(MLPClassifier, self).__init__()
        
        # æ ¹æ“šæ¿€æ´»å‡½æ•¸é¡å‹é¸æ“‡
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'softsign':
            self.activation = nn.Softsign()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        else:
            self.activation = nn.ReLU()
        
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            self.activation,
            nn.Linear(hidden_dim, hidden_dim),
            self.activation,
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, x):
        return self.network(x)


def calc_dist2(x: np.ndarray, xc: np.ndarray) -> np.ndarray:
    """
    è¨ˆç®—å¹³æ–¹è·é›¢çŸ©é™£
    å°æ‡‰ MATLAB ä¸­çš„ calc_dist2 å‡½æ•¸
    
    Args:
        x: è¼¸å…¥æ•¸æ“š (n_samples, n_features)
        xc: ä¸­å¿ƒé» (n_centers, n_features)
    
    Returns:
        å¹³æ–¹è·é›¢çŸ©é™£ (n_samples, n_centers)
    """
    return cdist(x, xc, 'sqeuclidean')


def calc_ker(dp: np.ndarray, du: np.ndarray, sigma: float, use_bias: bool = True, 
             model_type: str = 'gauss') -> Tuple[np.ndarray, np.ndarray]:
    """
    è¨ˆç®—æ ¸çŸ©é™£
    å°æ‡‰ MATLAB ä¸­çš„ calc_ker å‡½æ•¸
    
    Args:
        dp: æ­£æ¨£æœ¬è·é›¢çŸ©é™£
        du: æœªæ¨™è¨˜æ¨£æœ¬è·é›¢çŸ©é™£
        sigma: é«˜æ–¯æ ¸çš„å¸¶å¯¬åƒæ•¸
        use_bias: æ˜¯å¦ä½¿ç”¨åç½®é …
        model_type: æ¨¡å‹é¡å‹ ('gauss' æˆ– 'lm')
    
    Returns:
        Kp, Ku: æ ¸çŸ©é™£
    """
    np_samples, n_centers = dp.shape
    nu_samples = du.shape[0]
    
    if model_type == 'gauss':
        # é«˜æ–¯æ ¸
        Kp = np.exp(-dp / (2 * sigma**2))
        Ku = np.exp(-du / (2 * sigma**2))
    elif model_type == 'lm':
        # ç·šæ€§æ¨¡å‹
        Kp = dp
        Ku = du
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    if use_bias:
        # æ·»åŠ åç½®é …
        Kp = np.column_stack([Kp, np.ones(np_samples)])
        Ku = np.column_stack([Ku, np.ones(nu_samples)])
    
    return Kp, Ku


def solve_cholesky(H: np.ndarray, hpu: np.ndarray, use_bias: bool = True) -> np.ndarray:
    """
    ä½¿ç”¨ Cholesky åˆ†è§£æ±‚è§£ç·šæ€§æ–¹ç¨‹ Hx = hpu
    å°æ‡‰ MATLAB ä¸­çš„æ±‚è§£é‚è¼¯
    
    Args:
        H: ä¿‚æ•¸çŸ©é™£
        hpu: å³æ‰‹é‚Šå‘é‡
        use_bias: æ˜¯å¦ä½¿ç”¨åç½®é …
    
    Returns:
        theta: æ±‚è§£çš„åƒæ•¸å‘é‡
    """
    try:
        # å˜—è©¦ Cholesky åˆ†è§£
        R = np.linalg.cholesky(H)
        y = np.linalg.solve(R, hpu)
        theta = np.linalg.solve(R.T, y)
        return theta
    except np.linalg.LinAlgError:
        # å¦‚æœçŸ©é™£ä¸æ˜¯æ­£å®šï¼Œä½¿ç”¨å½é€†
        print("Warning: Matrix is not positive definite, using pseudo-inverse")
        return np.linalg.pinv(H) @ hpu


def estimate_prior_penL1CP(xp: np.ndarray, xu: np.ndarray, method: str = 'median') -> float:
    """
    ä½¿ç”¨ PenL1CP æ–¹æ³•ä¼°è¨ˆé¡åˆ¥å…ˆé©—
    ç°¡åŒ–ç‰ˆæœ¬çš„ MATLAB PenL1CP.m å¯¦ç¾
    
    Args:
        xp: æ­£æ¨£æœ¬
        xu: æœªæ¨™è¨˜æ¨£æœ¬
        method: ä¼°è¨ˆæ–¹æ³• ('mean' æˆ– 'median')
    
    Returns:
        ä¼°è¨ˆçš„é¡åˆ¥å…ˆé©—
    """
    # é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„å…ˆé©—ä¼°è¨ˆ
    # å¯¦éš›çš„ PenL1CP æ–¹æ³•è¼ƒç‚ºè¤‡é›œï¼Œé€™è£¡ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•
    
    # è¨ˆç®—æ­£æ¨£æœ¬å’Œæœªæ¨™è¨˜æ¨£æœ¬çš„å¯†åº¦æ¯”
    from sklearn.neighbors import KernelDensity
    
    try:
        # ä½¿ç”¨æ ¸å¯†åº¦ä¼°è¨ˆ
        kde_p = KernelDensity(bandwidth=0.5, kernel='gaussian')
        kde_u = KernelDensity(bandwidth=0.5, kernel='gaussian')
        
        kde_p.fit(xp)
        kde_u.fit(xu)
        
        # åœ¨æœªæ¨™è¨˜æ¨£æœ¬ä¸Šè©•ä¼°å¯†åº¦
        log_dens_p = kde_p.score_samples(xu)
        log_dens_u = kde_u.score_samples(xu)
        
        # è¨ˆç®—å¯†åº¦æ¯”
        density_ratio = np.exp(log_dens_p - log_dens_u)
        
        # ğŸ’¡ æ ¹æ“šæ–¹æ³•é¸æ“‡ä½¿ç”¨å‡å€¼æˆ–ä¸­ä½æ•¸
        if method == 'mean':
            estimated_prior = np.clip(np.mean(density_ratio), 0.1, 0.9)
        else:  # median
            estimated_prior = np.clip(np.median(density_ratio), 0.1, 0.9)
        
        print(f"[DEBUG] å…ˆé©—ä¼°è¨ˆçµ±è¨ˆ (æ–¹æ³•: {method}):")
        print(f"   â€¢ å¯†åº¦æ¯”å‡å€¼: {np.mean(density_ratio):.4f}")
        print(f"   â€¢ å¯†åº¦æ¯”ä¸­ä½æ•¸: {np.median(density_ratio):.4f}")
        print(f"   â€¢ ä¿®æ­£å¾Œä¼°è¨ˆ: {estimated_prior:.4f}")
        
    except Exception:
        # å¦‚æœå‡ºéŒ¯ï¼Œè¿”å›ä¸€å€‹åˆç†çš„é è¨­å€¼
        estimated_prior = 0.3
    
    return estimated_prior


def print_common_training_info(algorithm: str, xp: np.ndarray, xu: np.ndarray, prior: float, options: Dict):
    """æ‰“å°è¨“ç·´çš„å…±åŒè³‡è¨Š"""
    print("="*60)
    print(f"ğŸ”§ [DEBUG] Training with {algorithm} algorithm...")
    print("="*60)
    
    # 1. æ¨¡å‹èˆ‡é…ç½®è³‡è¨Š
    print("\nğŸ“‹ [DEBUG] æ¨¡å‹èˆ‡é…ç½®è³‡è¨Š:")
    if algorithm == 'uPU':
        print(f"   â€¢ æ¼”ç®—æ³•é¡å‹: uPU (Unbiased PU Learning) with Non-negative Risk Estimator")
        print(f"   â€¢ æ¨¡å‹æ¶æ§‹: æ ¸æ–¹æ³• + ç·šæ€§ä»£æ•¸ç›´æ¥è§£ (éç¥ç¶“ç¶²è·¯)")
        print(f"   â€¢ æå¤±å‡½æ•¸: Squared Loss")
        print(f"   â€¢ æ±‚è§£æ–¹å¼: ç›´æ¥è§£æè§£ (Cholesky åˆ†è§£)")
        print(f"   â€¢ æ˜¯å¦è¿­ä»£è¨“ç·´: å¦ (ç›´æ¥è¨ˆç®—)")
        print(f"   â€¢ è² é¢¨éšªè™•ç†: ä½¿ç”¨ Non-negative Risk Estimator (max(0, R_neg))")
    else:  # nnPU
        print(f"   â€¢ æ¼”ç®—æ³•é¡å‹: nnPU (Non-negative PU Learning)")
        print(f"   â€¢ æ¨¡å‹æ¶æ§‹: å¤šå±¤æ„ŸçŸ¥å™¨ (MLP) ç¥ç¶“ç¶²è·¯")
        print(f"   â€¢ ç¶²è·¯å±¤æ•¸: 3å±¤ (è¼¸å…¥å±¤ â†’ éš±è—å±¤1 â†’ éš±è—å±¤2 â†’ è¼¸å‡ºå±¤)")
        print(f"   â€¢ éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡: {options.get('hidden_dim', 100)}")
        print(f"   â€¢ æ¿€æ´»å‡½æ•¸: {options.get('activation', 'relu')}")
        print(f"   â€¢ æå¤±å‡½æ•¸: Sigmoid Loss")
        print(f"   â€¢ æ±‚è§£æ–¹å¼: è¿­ä»£è¨“ç·´ (æ¢¯åº¦ä¸‹é™)")
        print(f"   â€¢ æ˜¯å¦è¿­ä»£è¨“ç·´: æ˜¯")
    
    # 2. è³‡æ–™ç”Ÿæˆè³‡è¨Š
    print("\nğŸ“Š [DEBUG] è³‡æ–™ç”Ÿæˆè³‡è¨Š:")
    np_samples, d = xp.shape
    nu_samples = xu.shape[0]
    print(f"   â€¢ æ­£æ¨£æœ¬ (P) å½¢ç‹€: {xp.shape}")
    print(f"   â€¢ æœªæ¨™è¨˜æ¨£æœ¬ (U) å½¢ç‹€: {xu.shape}")
    print(f"   â€¢ æ•¸æ“šç¶­åº¦: {d}")
    print(f"   â€¢ è¨­å®šçš„é¡åˆ¥å…ˆé©— (prior): {prior}")
    
    # é¡¯ç¤ºå‰5ç­†æ•¸æ“š
    print("\n   ğŸ“‹ å‰5ç­†æ­£æ¨£æœ¬ (P):")
    for i in range(min(5, np_samples)):
        print(f"      P[{i}]: {xp[i]}")
    
    print("\n   ğŸ“‹ å‰5ç­†æœªæ¨™è¨˜æ¨£æœ¬ (U):")
    for i in range(min(5, nu_samples)):
        print(f"      U[{i}]: {xu[i]}")


def print_upu_parameters(options: Dict):
    """æ‰“å° uPU ç‰¹å®šçš„åƒæ•¸é…ç½®"""
    n_fold = options.get('n_fold', 5)
    model_type = options.get('model_type', 'gauss')
    lambda_list = options.get('lambda_list', np.logspace(-3, 1, 10))
    n_basis = options.get('n_basis', 200)
    use_bias = options.get('use_bias', True)
    
    # æ‰“å°è¶…åƒæ•¸
    print(f"   â€¢ äº¤å‰é©—è­‰æŠ˜æ•¸ (n_fold): {n_fold}")
    print(f"   â€¢ æ¨¡å‹é¡å‹ (model_type): {model_type}")
    print(f"   â€¢ åŸºå‡½æ•¸æ•¸é‡ (n_basis): {n_basis}")
    print(f"   â€¢ ä½¿ç”¨åç½®é … (use_bias): {use_bias}")
    print(f"   â€¢ æ­£å‰‡åŒ–åƒæ•¸å€™é¸ (lambda_list): {lambda_list}")


def print_nnpu_parameters(options: Dict):
    """æ‰“å° nnPU ç‰¹å®šçš„åƒæ•¸é…ç½®"""
    n_epochs = options.get('n_epochs', 50)
    learning_rate = options.get('learning_rate', 0.001)
    hidden_dim = options.get('hidden_dim', 100)
    activation = options.get('activation', 'relu')
    weight_decay = options.get('weight_decay', 0.0)
    
    print(f"\nğŸ“Š [DEBUG] è¶…åƒæ•¸è¨­å®š:")
    print(f"   â€¢ å­¸ç¿’ç‡ (learning_rate): {learning_rate}")
    print(f"   â€¢ Epoch æ•¸é‡ (n_epochs): {n_epochs}")
    print(f"   â€¢ æ¬Šé‡è¡°æ¸› (weight_decay): {weight_decay}")
    print(f"   â€¢ æ‰¹æ¬¡å¤§å° (batch size): å…¨æ‰¹æ¬¡ (full batch)")


def print_device_info():
    """æ‰“å°è¨­å‚™è³‡è¨Š"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  [DEBUG] è¨ˆç®—è¨­å‚™è³‡è¨Š:")
    print(f"   â€¢ ä½¿ç”¨è¨­å‚™: {device}")
    print(f"   â€¢ CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   â€¢ GPU åç¨±: {torch.cuda.get_device_name(0)}")
        print(f"   â€¢ GPU è¨˜æ†¶é«”å·²åˆ†é…: {torch.cuda.memory_allocated(0)}")
        print(f"   â€¢ GPU è¨˜æ†¶é«”å·²ä¿ç•™: {torch.cuda.memory_reserved(0)}")
    return device


def analyze_model_complexity(algorithm: str, params, input_dim: int, n_p: int, n_u: int):
    """åˆ†ææ¨¡å‹è¤‡é›œåº¦"""
    if algorithm == 'uPU':
        total_params = params['n_basis'] + (1 if params['use_bias'] else 0)
        threshold_high = 0.5
        threshold_low = 0.05
    else:  # nnPU
        hidden_dim = params.hidden_dim
        total_params = input_dim * hidden_dim + hidden_dim + hidden_dim * 1 + 1
        threshold_high = 0.1
        threshold_low = 0.01
    
    data_size = n_p + n_u
    complexity_ratio = total_params / data_size
    
    print(f"\n   ğŸ“Š {algorithm} æ¨¡å‹è¤‡é›œåº¦åˆ†æ:")
    print(f"      â€¢ ç¸½åƒæ•¸æ•¸é‡: {total_params}")
    print(f"      â€¢ è¨“ç·´æ•¸æ“šé‡: {data_size}")
    print(f"      â€¢ åƒæ•¸/æ•¸æ“šæ¯”ä¾‹: {complexity_ratio:.3f}")
    
    if complexity_ratio > threshold_high:
        print(f"      âš ï¸  è­¦å‘Šï¼š{algorithm} æ¨¡å‹åƒæ•¸éå¤šï¼Œå¯èƒ½éæ“¬åˆ (æ¯”ä¾‹: {complexity_ratio:.3f})")
    elif complexity_ratio < threshold_low:
        print(f"      âš ï¸  è­¦å‘Šï¼š{algorithm} æ¨¡å‹åƒæ•¸éå°‘ï¼Œå¯èƒ½æ¬ æ“¬åˆ (æ¯”ä¾‹: {complexity_ratio:.3f})")
    else:
        print(f"      âœ… {algorithm} æ¨¡å‹è¤‡é›œåº¦é©ä¸­ (æ¯”ä¾‹: {complexity_ratio:.3f})")


def print_test_data_info(xt_p, xt_n, data_params):
    """æ‰“å°æ¸¬è©¦æ•¸æ“šè³‡è¨Š"""
    print(f"\nğŸ“Š [DEBUG] æ•¸æ“šç”Ÿæˆå®Œæˆ:")
    print(f"   â€¢ æ­£æ¨£æœ¬ (P): {xt_p.shape}")
    print(f"   â€¢ æœªæ¨™è¨˜æ¨£æœ¬ (U): {xt_n.shape}")
    print(f"   â€¢ æ¸¬è©¦æ­£æ¨£æœ¬ (Test_P): {xt_p.shape}")
    print(f"   â€¢ æ¸¬è©¦è² æ¨£æœ¬ (Test_N): {xt_n.shape}")
    
    # **é‡æ–°è¨ˆç®—çœŸå¯¦çš„ True Prior åœ¨æ¸¬è©¦é›†ä¸Š**
    true_test_prior = len(xt_p) / (len(xt_p) + len(xt_n))
    print(f"\nğŸ¯ [DEBUG] æ¸¬è©¦é›†ä¸Šçš„çœŸå¯¦ Prior é©—è­‰:")
    print(f"   â€¢ è¨­å®šçš„ Prior: {data_params.prior:.3f}")
    print(f"   â€¢ æ¸¬è©¦é›†è¨ˆç®—çš„ True Prior: {true_test_prior:.3f}")
    print(f"   â€¢ å·®ç•°: {abs(data_params.prior - true_test_prior):.3f}")
    if abs(data_params.prior - true_test_prior) > 0.05:
        print("   âš ï¸  **Warning: æ¸¬è©¦é›† Prior èˆ‡è¨­å®šå€¼å·®ç•°è¼ƒå¤§ï¼**")


def print_prediction_analysis(pred_p, pred_n, algorithm: str):
    """æ‰“å°é æ¸¬åˆ†æ"""
    print(f"\n   ğŸ¯ {algorithm} æ¨¡å‹é æ¸¬åˆ†æ:")
    if algorithm == 'nnPU':
        pred_p_flat = pred_p.flatten()
        pred_n_flat = pred_n.flatten()
    else:
        pred_p_flat = pred_p
        pred_n_flat = pred_n
    
    print(f"      â€¢ æ­£æ¨£æœ¬é æ¸¬å€¼ç¯„åœ: [{pred_p_flat.min():.4f}, {pred_p_flat.max():.4f}]")
    print(f"      â€¢ è² æ¨£æœ¬é æ¸¬å€¼ç¯„åœ: [{pred_n_flat.min():.4f}, {pred_n_flat.max():.4f}]")
    print(f"      â€¢ æ­£æ¨£æœ¬é æ¸¬å‡å€¼: {pred_p_flat.mean():.4f} Â± {pred_p_flat.std():.4f}")
    print(f"      â€¢ è² æ¨£æœ¬é æ¸¬å‡å€¼: {pred_n_flat.mean():.4f} Â± {pred_n_flat.std():.4f}")
    
    # æª¢æŸ¥é æ¸¬åˆ†ä½ˆ
    pos_positive_pred = np.sum(pred_p_flat > 0)
    pos_negative_pred = np.sum(pred_p_flat <= 0)
    neg_positive_pred = np.sum(pred_n_flat > 0)
    neg_negative_pred = np.sum(pred_n_flat <= 0)
    
    print(f"      â€¢ æ­£æ¨£æœ¬ä¸­é æ¸¬ç‚ºæ­£çš„: {pos_positive_pred}/{len(pred_p_flat)} ({pos_positive_pred/len(pred_p_flat)*100:.1f}%)")
    print(f"      â€¢ æ­£æ¨£æœ¬ä¸­é æ¸¬ç‚ºè² çš„: {pos_negative_pred}/{len(pred_p_flat)} ({pos_negative_pred/len(pred_p_flat)*100:.1f}%)")
    print(f"      â€¢ è² æ¨£æœ¬ä¸­é æ¸¬ç‚ºæ­£çš„: {neg_positive_pred}/{len(pred_n_flat)} ({neg_positive_pred/len(pred_n_flat)*100:.1f}%)")
    print(f"      â€¢ è² æ¨£æœ¬ä¸­é æ¸¬ç‚ºè² çš„: {neg_negative_pred}/{len(pred_n_flat)} ({neg_negative_pred/len(pred_n_flat)*100:.1f}%)")
    
    # åˆ†ææ˜¯å¦å­˜åœ¨æ˜é¡¯çš„é æ¸¬åå·®
    if pos_positive_pred / len(pred_p_flat) > 0.95:
        print(f"      âš ï¸  è­¦å‘Šï¼š{algorithm} æ¨¡å‹å°æ­£æ¨£æœ¬çš„é æ¸¬éæ–¼è‡ªä¿¡ ({pos_positive_pred/len(pred_p_flat)*100:.1f}%)")
    if neg_negative_pred / len(pred_n_flat) > 0.95:
        print(f"      âš ï¸  è­¦å‘Šï¼š{algorithm} æ¨¡å‹å°è² æ¨£æœ¬çš„é æ¸¬éæ–¼è‡ªä¿¡ ({neg_negative_pred/len(pred_n_flat)*100:.1f}%)")


def calculate_error_rates(pred_p, pred_n, xt_p, xt_n, prior: float, algorithm: str):
    """è¨ˆç®—éŒ¯èª¤ç‡"""
    correct_p = np.sum(pred_p > 0)  # æ­£ç¢ºé æ¸¬ç‚ºæ­£çš„æ•¸é‡
    incorrect_p = np.sum(pred_p <= 0)  # éŒ¯èª¤é æ¸¬ç‚ºè² çš„æ•¸é‡ (False Negative)
    correct_n = np.sum(pred_n <= 0)  # æ­£ç¢ºé æ¸¬ç‚ºè² çš„æ•¸é‡
    incorrect_n = np.sum(pred_n > 0)  # éŒ¯èª¤é æ¸¬ç‚ºæ­£çš„æ•¸é‡ (False Positive)
    
    print(f"   ğŸ“Š æ¸¬è©¦é›†è©³ç´°çµ±è¨ˆ:")
    print(f"      â€¢ æ¸¬è©¦æ­£æ¨£æœ¬ç¸½æ•¸: {len(xt_p)}")
    print(f"      â€¢ æ¸¬è©¦è² æ¨£æœ¬ç¸½æ•¸: {len(xt_n)}")
    print(f"      â€¢ æ­£ç¢ºé æ¸¬ç‚ºæ­£çš„æ•¸é‡: {correct_p}")
    print(f"      â€¢ éŒ¯èª¤é æ¸¬ç‚ºè² çš„æ•¸é‡ (FN): {incorrect_p}")
    print(f"      â€¢ æ­£ç¢ºé æ¸¬ç‚ºè² çš„æ•¸é‡: {correct_n}")
    print(f"      â€¢ éŒ¯èª¤é æ¸¬ç‚ºæ­£çš„æ•¸é‡ (FP): {incorrect_n}")
    
    fn_rate = incorrect_p / len(xt_p)
    fp_rate = incorrect_n / len(xt_n)
    error_rate = prior * fn_rate + (1 - prior) * fp_rate
    
    print(f"   ğŸ“ˆ éŒ¯èª¤ç‡è¨ˆç®—:")
    print(f"      â€¢ False Negative Rate (FNR): {fn_rate:.4f}")
    print(f"      â€¢ False Positive Rate (FPR): {fp_rate:.4f}")
    print(f"      â€¢ åŠ æ¬ŠéŒ¯èª¤ç‡å…¬å¼: Ï€*FNR + (1-Ï€)*FPR")
    print(f"      â€¢ æœ€çµ‚éŒ¯èª¤ç‡: {prior:.3f}*{fn_rate:.4f} + {(1-prior):.3f}*{fp_rate:.4f} = {error_rate:.4f}")
    
    return error_rate, fn_rate, fp_rate


def analyze_error_rate_reasonableness(error_rate: float, pred_p, pred_n, algorithm: str):
    """åˆ†æéŒ¯èª¤ç‡çš„åˆç†æ€§"""
    print(f"\n   ğŸ” {algorithm} éŒ¯èª¤ç‡åˆç†æ€§åˆ†æ:")
    overall_accuracy = (np.sum(pred_p > 0) + np.sum(pred_n <= 0)) / (len(pred_p) + len(pred_n))
    print(f"      â€¢ ç¸½é«”æº–ç¢ºç‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
    print(f"      â€¢ å°æ‡‰éŒ¯èª¤ç‡: {1-overall_accuracy:.4f} ({(1-overall_accuracy)*100:.2f}%)")
    
    # æª¢æŸ¥æ˜¯å¦ç•°å¸¸
    if error_rate < 0.05:  # éŒ¯èª¤ç‡ä½æ–¼5%
        print(f"      âš ï¸  ç•°å¸¸è­¦å‘Šï¼š{algorithm} éŒ¯èª¤ç‡ {error_rate*100:.2f}% éä½ï¼")
        print(f"      âš ï¸  é€™å°æ–¼ two_moons æ•¸æ“šé›†æ˜¯ä¸ç¾å¯¦çš„")
        print(f"      âš ï¸  å¯èƒ½åŸå› ï¼šæ¨¡å‹éåº¦è‡ªä¿¡ã€æ¸¬è©¦é›†æ¨™ç±¤éŒ¯èª¤ã€æˆ–æ•¸æ“šç”Ÿæˆå•é¡Œ")
        
        # æª¢æŸ¥æ¨¡å‹é æ¸¬çš„åˆ†ä½ˆ
        if algorithm == 'nnPU':
            pred_p_flat = pred_p.flatten()
            pred_n_flat = pred_n.flatten()
        else:
            pred_p_flat = pred_p
            pred_n_flat = pred_n
            
        pred_separation = abs(pred_p_flat.mean() - pred_n_flat.mean())
        print(f"      â€¢ æ­£è² æ¨£æœ¬é æ¸¬å‡å€¼å·®è·: {pred_separation:.4f}")
        
        if pred_separation > 2.0:
            print(f"      âš ï¸  é æ¸¬å€¼åˆ†é›¢åº¦éé«˜ ({pred_separation:.2f})ï¼Œæ¨¡å‹å¯èƒ½éåº¦è‡ªä¿¡")
            
        if algorithm == 'nnPU':
            # æª¢æŸ¥ sigmoid æ¦‚ç‡åˆ†ä½ˆ
            pred_p_sigmoid = 1 / (1 + np.exp(-pred_p_flat))
            pred_n_sigmoid = 1 / (1 + np.exp(-pred_n_flat))
            print(f"      â€¢ æ­£æ¨£æœ¬ sigmoid æ¦‚ç‡å‡å€¼: {pred_p_sigmoid.mean():.4f}")
            print(f"      â€¢ è² æ¨£æœ¬ sigmoid æ¦‚ç‡å‡å€¼: {pred_n_sigmoid.mean():.4f}")
            
            if pred_p_sigmoid.mean() > 0.95:
                print(f"      âš ï¸  æ­£æ¨£æœ¬æ¦‚ç‡éé«˜ ({pred_p_sigmoid.mean():.3f})ï¼Œæ¨¡å‹éåº¦è‡ªä¿¡")
            if pred_n_sigmoid.mean() < 0.05:
                print(f"      âš ï¸  è² æ¨£æœ¬æ¦‚ç‡éä½ ({pred_n_sigmoid.mean():.3f})ï¼Œæ¨¡å‹éåº¦è‡ªä¿¡")
    
    elif error_rate > 0.40:  # éŒ¯èª¤ç‡é«˜æ–¼40%
        print(f"      âš ï¸  ç•°å¸¸è­¦å‘Šï¼š{algorithm} éŒ¯èª¤ç‡ {error_rate*100:.2f}% éé«˜ï¼")
        print(f"      âš ï¸  æ¨¡å‹æ€§èƒ½å¯èƒ½æœ‰å•é¡Œ")
    else:
        print(f"      âœ… {algorithm} éŒ¯èª¤ç‡ {error_rate*100:.2f}% åœ¨åˆç†ç¯„åœå…§ (5%-40%)")


def analyze_prior_estimation(estimated_prior: float, true_prior: float, algorithm: str):
    """åˆ†æå…ˆé©—ä¼°è¨ˆçš„åˆç†æ€§"""
    prior_error = abs(estimated_prior - true_prior)
    print(f"\n   ğŸ“Š {algorithm} å…ˆé©—ä¼°è¨ˆåˆ†æ:")
    print(f"      â€¢ çœŸå¯¦å…ˆé©—: {true_prior:.3f}")
    print(f"      â€¢ ä¼°è¨ˆå…ˆé©—: {estimated_prior:.3f}")
    print(f"      â€¢ çµ•å°èª¤å·®: {prior_error:.3f}")
    
    if prior_error > 0.3:
        print(f"      âš ï¸  {algorithm} å…ˆé©—ä¼°è¨ˆèª¤å·®éå¤§ ({prior_error:.3f})ï¼")
        print(f"      âš ï¸  é€™æœƒåš´é‡å½±éŸ¿ {algorithm} Learning çš„æ€§èƒ½")
        
        # åˆ†æå¯èƒ½çš„åŸå› 
        if estimated_prior > true_prior + 0.3:
            print(f"      âš ï¸  ä¼°è¨ˆå…ˆé©—éé«˜ï¼Œå¯èƒ½åŸå› ï¼šunlabeled data ä¸­æ­£æ¨£æœ¬æ¯”ä¾‹è¢«é«˜ä¼°")
        elif estimated_prior < true_prior - 0.3:
            print(f"      âš ï¸  ä¼°è¨ˆå…ˆé©—éä½ï¼Œå¯èƒ½åŸå› ï¼šunlabeled data ä¸­æ­£æ¨£æœ¬æ¯”ä¾‹è¢«ä½ä¼°")
            
    elif prior_error > 0.1:
        print(f"      âš ï¸  {algorithm} å…ˆé©—ä¼°è¨ˆæœ‰è¼ƒå¤§åå·® ({prior_error:.3f})")
    else:
        print(f"      âœ… {algorithm} å…ˆé©—ä¼°è¨ˆèª¤å·®å¯æ¥å— ({prior_error:.3f})")


def train_upu(xp: np.ndarray, xu: np.ndarray, prior: float, options: Dict, seed: int = None) -> Tuple[callable, Dict]:
    """
    uPU æ¼”ç®—æ³•çš„ Python å¯¦ç¾
    å°æ‡‰ MATLAB PU_SL.m çš„ä¸»è¦é‚è¼¯
    
    Args:
        xp: æ­£æ¨£æœ¬ (n_p, d)
        xu: æœªæ¨™è¨˜æ¨£æœ¬ (n_u, d)
        prior: é¡åˆ¥å…ˆé©—
        options: é¸é …å­—å…¸
    
    Returns:
        func_dec: æ±ºç­–å‡½æ•¸
        outputs: åŒ…å«æ¨¡å‹åƒæ•¸å’Œå…¶ä»–è¼¸å‡ºçš„å­—å…¸
    """
    # ä½¿ç”¨å…±ç”¨çš„è¨“ç·´è³‡è¨Šæ‰“å°å‡½æ•¸
    print_common_training_info('uPU', xp, xu, prior, options)
    print_upu_parameters(options)
    
    # åƒæ•¸è¨­ç½®
    n_fold = options.get('n_fold', 5)
    model_type = options.get('model_type', 'gauss')
    lambda_list = options.get('lambda_list', np.logspace(-3, 1, 10))
    n_basis = options.get('n_basis', min(200, xu.shape[0]))
    use_bias = options.get('use_bias', True)
    
    np_samples, d = xp.shape
    nu_samples = xu.shape[0]
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ï¼ˆå¦‚æœæä¾›ï¼‰
    if seed is not None:
        np.random.seed(seed)
        print(f"   ğŸ”§ [DEBUG] uPU è¨“ç·´è¨­ç½®éš¨æ©Ÿç¨®å­: {seed}")
    
    # é¸æ“‡åŸºå‡½æ•¸ä¸­å¿ƒé»
    center_indices = np.random.choice(nu_samples, size=min(n_basis, nu_samples), replace=False)
    xc = xu[center_indices]
    
    # è¨ˆç®—è·é›¢çŸ©é™£
    if model_type == 'gauss':
        dp = calc_dist2(xp, xc)
        du = calc_dist2(xu, xc)
        # è¨­ç½® sigma å€™é¸å€¼
        all_distances = np.concatenate([dp.flatten(), du.flatten()])
        sigma_list = np.sqrt(np.median(all_distances)) * np.logspace(-2, 1, 10)
    else:  # ç·šæ€§æ¨¡å‹
        dp = xp
        du = xu
        sigma_list = [1.0]  # å°ç·šæ€§æ¨¡å‹ï¼Œsigma å€¼ä¸é‡è¦
    
    n_sigma = len(sigma_list)
    n_lambda = len(lambda_list)
    
    # äº¤å‰é©—è­‰
    cv_seed = seed if seed is not None else 42
    kf_p = KFold(n_splits=n_fold, shuffle=True, random_state=cv_seed)
    kf_u = KFold(n_splits=n_fold, shuffle=True, random_state=cv_seed)
    
    cv_indices_p = list(kf_p.split(range(np_samples)))
    cv_indices_u = list(kf_u.split(range(nu_samples)))
    
    score_table = np.zeros((n_sigma, n_lambda))
    
    for i_sigma, sigma in enumerate(sigma_list):
        print(f"  Processing sigma {i_sigma+1}/{n_sigma}: {sigma:.4f}")
        
        # è¨ˆç®—æ ¸çŸ©é™£
        Kp, Ku = calc_ker(dp, du, sigma, use_bias, model_type)
        
        cv_scores = []
        
        for fold in range(n_fold):
            train_idx_p, test_idx_p = cv_indices_p[fold]
            train_idx_u, test_idx_u = cv_indices_u[fold]
            
            # è¨“ç·´é›†æ ¸çŸ©é™£
            Kp_train = Kp[train_idx_p]
            Ku_train = Ku[train_idx_u]
            
            # æ¸¬è©¦é›†æ ¸çŸ©é™£
            Kp_test = Kp[test_idx_p]
            Ku_test = Ku[test_idx_u]
            
            # è¨ˆç®—è¨“ç·´ç”¨çš„çŸ©é™£
            Hp_tr = prior * (Kp_train.T @ Kp_train) / len(train_idx_p)
            Hu_tr = (Ku_train.T @ Ku_train) / len(train_idx_u)
            hp_tr = prior * np.mean(Kp_train, axis=0)
            hu_tr = np.mean(Ku_train, axis=0)
            
            fold_scores = []
            for i_lambda, lambda_reg in enumerate(lambda_list):
                # æ­£å‰‡åŒ–çŸ©é™£
                b = Hu_tr.shape[0]
                Reg = lambda_reg * np.eye(b)
                if use_bias:
                    Reg[-1, -1] = 0  # åç½®é …ä¸æ­£å‰‡åŒ–
                
                # uPU é¢¨éšª: hpu = 2*hp - hu
                hpu = 2 * hp_tr - hu_tr
                
                # æ±‚è§£
                theta = solve_cholesky(Hu_tr + Reg, hpu, use_bias)
                
                # è¨ˆç®—é©—è­‰æå¤±
                gp_test = Kp_test @ theta
                gu_test = Ku_test @ theta
                
                # uPU æå¤±è¨ˆç®—ï¼Œå¯¦ç¾ non-negative risk estimator
                fn = np.mean(gp_test <= 0) if len(gp_test) > 0 else 0
                fp_u = np.mean(gu_test >= 0) if len(gu_test) > 0 else 0
                
                # uPU é¢¨éšªçš„åŸå§‹è¨ˆç®— (å¯èƒ½ç‚ºè² )
                raw_negative_risk = fp_u + prior * fn - prior  # é€™å¯èƒ½ç‚ºè² 
                upu_risk_raw = prior * fn + raw_negative_risk
                
                # **ä»»å‹™äºŒä¿®æ­£ï¼šä½¿ç”¨ non-negative risk estimator**
                # é¡ä¼¼ nnPU çš„åšæ³•ï¼šmax(0, negative_risk)
                non_negative_risk = max(raw_negative_risk, 0)
                loss = prior * fn + non_negative_risk
                
                # åœ¨éƒ¨åˆ†æŠ˜ä¸­è¨˜éŒ„è² é¢¨éšªæƒ…æ³
                if fold == 0 and i_lambda == 0:  # åªåœ¨ç¬¬ä¸€æŠ˜ç¬¬ä¸€å€‹lambdaè¨˜éŒ„
                    print(f"      ğŸ“Š CV Fold {fold+1}, Î»={lambda_reg:.4f}:")
                    print(f"         â€¢ æ­£æ¨£æœ¬é¢¨éšª (prior * FN): {prior * fn:.4f}")
                    print(f"         â€¢ åŸå§‹è² æ¨£æœ¬é¢¨éšª: {raw_negative_risk:.4f}", end="")
                    if raw_negative_risk < 0:
                        print(" âš ï¸  **è®Šæˆè² æ•¸ï¼ä½¿ç”¨éè² ç´„æŸ**")
                    else:
                        print("")
                    print(f"         â€¢ uPU é¢¨éšª (åŸå§‹): {upu_risk_raw:.4f}")
                    print(f"         â€¢ uPU é¢¨éšª (éè² ç´„æŸå¾Œ): {loss:.4f}")
                
                fold_scores.append(loss)
            
            cv_scores.append(fold_scores)
        
        # å¹³å‡äº¤å‰é©—è­‰åˆ†æ•¸
        score_table[i_sigma, :] = np.mean(cv_scores, axis=0)
    
    # é¸æ“‡æœ€ä½³åƒæ•¸
    best_idx = np.unravel_index(np.argmin(score_table), score_table.shape)
    best_sigma = sigma_list[best_idx[0]]
    best_lambda = lambda_list[best_idx[1]]
    
    print(f"  Best sigma: {best_sigma:.4f}, Best lambda: {best_lambda:.4f}")
    
    # ä½¿ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æœ€çµ‚æ¨¡å‹
    Kp, Ku = calc_ker(dp, du, best_sigma, use_bias, model_type)
    
    Hu = (Ku.T @ Ku) / nu_samples
    hp = prior * np.mean(Kp, axis=0)
    hu = np.mean(Ku, axis=0)
    
    b = Hu.shape[0]
    Reg = best_lambda * np.eye(b)
    if use_bias:
        Reg[-1, -1] = 0
    
    hpu = 2 * hp - hu
    theta = solve_cholesky(Hu + Reg, hpu, use_bias)
    
    # æ§‹å»ºæ±ºç­–å‡½æ•¸
    def func_dec(x_test):
        if model_type == 'gauss':
            dist_test = calc_dist2(x_test, xc)
            K_test = np.exp(-dist_test / (2 * best_sigma**2))
        else:
            K_test = x_test
        
        if use_bias:
            K_test = np.column_stack([K_test, np.ones(x_test.shape[0])])
        
        return K_test @ theta
    
    outputs = {
        'theta': theta,
        'sigma': best_sigma,
        'lambda': best_lambda,
        'score_table': score_table,
        'xc': xc,
        'model_type': model_type,
        'use_bias': use_bias
    }
    
    return func_dec, outputs


def train_nnpu(xp: np.ndarray, xu: np.ndarray, prior: float, options: Dict, seed: int = None) -> Tuple[nn.Module, Dict]:
    """
    nnPU æ¼”ç®—æ³•çš„ Python å¯¦ç¾
    ä½¿ç”¨ç¥ç¶“ç¶²è·¯å’Œ Sigmoid Loss
    
    Args:
        xp: æ­£æ¨£æœ¬
        xu: æœªæ¨™è¨˜æ¨£æœ¬
        prior: é¡åˆ¥å…ˆé©—
        options: é¸é …å­—å…¸
    
    Returns:
        model: è¨“ç·´å¥½çš„ç¥ç¶“ç¶²è·¯æ¨¡å‹
        outputs: åŒ…å«è¨“ç·´æ­·å²å’ŒæŒ‡æ¨™çš„å­—å…¸
    """
    # ä½¿ç”¨å…±ç”¨çš„è¨“ç·´è³‡è¨Šæ‰“å°å‡½æ•¸
    print_common_training_info('nnPU', xp, xu, prior, options)
    print_nnpu_parameters(options)
    
    # åƒæ•¸è¨­ç½®
    n_epochs = options.get('n_epochs', 50)
    learning_rate = options.get('learning_rate', 0.001)
    hidden_dim = options.get('hidden_dim', 100)
    activation = options.get('activation', 'relu')
    weight_decay = options.get('weight_decay', 0.0)  # æ–°å¢ weight_decay æ”¯æ´
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ï¼ˆå¦‚æœæä¾›ï¼‰
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)
        print(f"   ğŸ”§ [DEBUG] nnPU è¨“ç·´è¨­ç½®éš¨æ©Ÿç¨®å­: {seed}")
    
    # ä½¿ç”¨å…±ç”¨çš„è¨­å‚™è³‡è¨Šæ‰“å°å‡½æ•¸
    device = print_device_info()
    
    # æ•¸æ“šæº–å‚™
    xp_tensor = torch.FloatTensor(xp).to(device)
    xu_tensor = torch.FloatTensor(xu).to(device)
    
    # æ¨¡å‹åˆå§‹åŒ–
    input_dim = xp.shape[1]
    model = MLPClassifier(input_dim, hidden_dim, activation).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    
    print(f"\nğŸ¤– [DEBUG] æ¨¡å‹åˆå§‹åŒ–å®Œæˆ:")
    print(f"   â€¢ è¼¸å…¥ç¶­åº¦: {input_dim}")
    print(f"   â€¢ éš±è—å±¤ç¶­åº¦: {hidden_dim}")
    print(f"   â€¢ æ¨¡å‹åƒæ•¸ç¸½æ•¸: {sum(p.numel() for p in model.parameters())}")
    print(f"   â€¢ å„ªåŒ–å™¨: Adam (lr={learning_rate}, weight_decay={weight_decay})")
    
    # è¨“ç·´æ­·å²
    risk_curve = []
    
    print(f"\nğŸƒ [DEBUG] é–‹å§‹è¨“ç·´éç¨‹...")
    print("="*50)
    
    for epoch in range(n_epochs):
        model.train()
        optimizer.zero_grad()
        
        # å‰å‘å‚³æ’­
        gp = model(xp_tensor).squeeze()
        gu = model(xu_tensor).squeeze()
        
        # Sigmoid æå¤±
        sigmoid = torch.nn.Sigmoid()
        
        # R_p^+ = mean(sigmoid(-g(x_p)))
        R_p_plus = torch.mean(sigmoid(-gp))
        
        # R_u^- = mean(sigmoid(g(x_u)))
        R_u_minus = torch.mean(sigmoid(gu))
        
        # R_p^- = mean(sigmoid(g(x_p)))
        R_p_minus = torch.mean(sigmoid(gp))
        
        # 3. è¨“ç·´éç¨‹æ—¥èªŒ - ç›£æ§ uPU risk çµ„æˆéƒ¨åˆ†
        upu_risk_raw = R_u_minus - prior * R_p_minus  # å¯èƒ½ç‚ºè² 
        
        # nnPU é¢¨éšª: prior * R_p^+ + max(0, R_u^- - prior * R_p^-)
        nnpu_risk = prior * R_p_plus + torch.clamp(R_u_minus - prior * R_p_minus, min=0)
        
        # è©³ç´°çš„è¨“ç·´æ—¥èªŒ
        if epoch % 5 == 0 or epoch < 5:  # å‰5å€‹epochå’Œæ¯5å€‹epochè¨˜éŒ„ä¸€æ¬¡
            print(f"\nğŸ“Š [DEBUG] Epoch {epoch+1}/{n_epochs}:")
            print(f"   â€¢ Positive Risk (R_p^+): {R_p_plus.item():.6f}")
            print(f"   â€¢ Unlabeled Risk (R_u^-): {R_u_minus.item():.6f}")
            print(f"   â€¢ Negative Risk Estimate (R_p^-): {R_p_minus.item():.6f}")
            print(f"   â€¢ uPU Risk Raw (R_u^- - Ï€*R_p^-): {upu_risk_raw.item():.6f}", end="")
            if upu_risk_raw.item() < 0:
                print(" âš ï¸  **è®Šæˆè² æ•¸ï¼**")
            else:
                print("")
            print(f"   â€¢ nnPU Risk (éè² ç´„æŸå¾Œ): {nnpu_risk.item():.6f}")
            print(f"   â€¢ é¡åˆ¥å…ˆé©— (Ï€): {prior}")
        
        # åå‘å‚³æ’­
        nnpu_risk.backward()
        optimizer.step()
        
        # è¨˜éŒ„é¢¨éšªå€¼
        risk_value = nnpu_risk.item()
        risk_curve.append({'epoch': epoch + 1, 'risk': risk_value})
        
        if (epoch + 1) % 10 == 0:
            print(f"   âœ… Epoch {epoch + 1}/{n_epochs}, Final Risk: {risk_value:.6f}")
    
    print("="*50)
    print("ğŸ¯ [DEBUG] è¨“ç·´å®Œæˆï¼")
    model.eval()
    
    outputs = {
        'risk_curve': risk_curve,
        'final_risk': risk_curve[-1]['risk'],
        'device': device
    }
    
    return model, outputs


def run_pu_simulation(request) -> Dict:
    """
    å”èª¿æ•¸æ“šç”Ÿæˆå’Œæ¨¡å‹è¨“ç·´çš„ä¸»å‡½æ•¸
    
    Args:
        request: SimulationRequest å°è±¡
    
    Returns:
        åŒ…å«å¯è¦–åŒ–æ•¸æ“šå’ŒæŒ‡æ¨™çš„å­—å…¸
    """
    from data_generator import generate_synthetic_data, reduce_to_2d_for_visualization
    
    print(f"\nğŸš€ [DEBUG] Running PU simulation with {request.algorithm} algorithm...")
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡ç¾æ€§
    seed = getattr(request, 'seed', 42)  # å¾å‰ç«¯æ¥æ”¶ç¨®å­ï¼Œé è¨­ç‚º 42
    print(f"ğŸ”§ [DEBUG] ä¸»å¼•æ“è¨­ç½®éš¨æ©Ÿç¨®å­: {seed}")
    # æ³¨æ„ï¼šå…·é«”çš„ç¨®å­è¨­ç½®å°‡åœ¨æ•¸æ“šç”Ÿæˆå™¨å’Œæ¨¡å‹è¨“ç·´ä¸­é€²è¡Œ
    
    # 1. ç”Ÿæˆæ•¸æ“š
    data_params = request.data_params
    xp, xu, xt_p, xt_n = generate_synthetic_data(
        distribution=data_params.distribution,
        dims=data_params.dims,
        n_p=data_params.n_p,
        n_u=data_params.n_u,
        prior=data_params.prior,
        seed=seed  # å‚³éç¨®å­çµ¦æ•¸æ“šç”Ÿæˆå™¨
    )
    
    # ä½¿ç”¨å…±ç”¨çš„æ¸¬è©¦æ•¸æ“šè³‡è¨Šæ‰“å°å‡½æ•¸
    print_test_data_info(xt_p, xt_n, data_params)
    
    # 2. è¨“ç·´æ¨¡å‹ï¼ˆå…ˆé©—ä¼°è¨ˆå°‡åœ¨è¨“ç·´å®Œæˆå¾Œé€²è¡Œï¼‰
    model_params = request.model_params
    
    # åˆå§‹åŒ–è®Šé‡
    training_error_rate = 0.0  # åˆå§‹åŒ–è¨“ç·´éŒ¯èª¤ç‡
    estimated_prior = 0.0  # åˆå§‹åŒ–å…ˆé©—ä¼°è¨ˆ
    
    if request.algorithm == 'uPU':
        # ä½¿ç”¨å‰ç«¯å‚³ä¾†çš„ uPU åƒæ•¸ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é è¨­å€¼
        options = {
            'model_type': getattr(model_params, 'model_type', 'gauss'),
            'use_bias': getattr(model_params, 'use_bias', True),
            'n_basis': getattr(model_params, 'n_basis', min(200, xu.shape[0]))
        }
        
        # ä½¿ç”¨å…±ç”¨çš„æ¨¡å‹è¤‡é›œåº¦åˆ†æå‡½æ•¸
        analyze_model_complexity('uPU', options, data_params.dims, len(xp), len(xu))
        
        model, outputs = train_upu(xp, xu, data_params.prior, options, seed)
        
        # 3. åœ¨æ¨¡å‹è¨“ç·´å®Œæˆå¾Œé€²è¡Œå…ˆé©—ä¼°è¨ˆ
        print(f"\nğŸ” [DEBUG] è¨“ç·´å¾Œå…ˆé©—ä¼°è¨ˆ (uPU):")
        prior_method = getattr(request, 'prior_estimation_method', 'median')
        estimated_prior = estimate_prior_penL1CP(xp, xu, prior_method)
        print(f"   â€¢ ä¼°è¨ˆçš„ Prior: {estimated_prior:.3f}")
        print(f"   â€¢ çœŸå¯¦çš„ Prior: {data_params.prior:.3f}")
        print(f"   â€¢ ä¼°è¨ˆèª¤å·®: {abs(estimated_prior - data_params.prior):.3f}")
        
        # 4. è©•ä¼°éç¨‹æ—¥èªŒ - è©³ç´°è¨ˆç®—åˆ†é¡éŒ¯èª¤ç‡
        print(f"\nğŸ“ˆ [DEBUG] æ¨¡å‹è©•ä¼° (uPU):")
        pred_p = model(xt_p)
        pred_n = model(xt_n)
        
        # ä½¿ç”¨å…±ç”¨çš„é æ¸¬åˆ†æå‡½æ•¸
        print_prediction_analysis(pred_p, pred_n, 'uPU')
        
        # ä½¿ç”¨å…±ç”¨çš„éŒ¯èª¤ç‡è¨ˆç®—å‡½æ•¸
        error_rate, fn_rate, fp_rate = calculate_error_rates(pred_p, pred_n, xt_p, xt_n, data_params.prior, 'uPU')
        
        # è¨ˆç®— uPU è¨“ç·´éŒ¯èª¤ç‡ - ä¿®æ­£ç‰ˆæœ¬ï¼šåªè¨ˆç®—æ­£æ¨£æœ¬çš„éŒ¯èª¤ç‡
        print(f"\n   ğŸ“Š [DEBUG] è¨ˆç®— uPU è¨“ç·´éŒ¯èª¤ç‡ (ä¿®æ­£ç‰ˆæœ¬)...")
        
        # ç¬¬ä¸€æ­¥ï¼šå–å¾—å° P æ¨£æœ¬çš„é æ¸¬
        train_pred_p = model(xp)  # è¨“ç·´é›†ä¸­çš„æ­£æ¨£æœ¬é æ¸¬åˆ†æ•¸
        
        # ç¬¬äºŒæ­¥ï¼šè¨ˆç®—éŒ¯èª¤æ•¸é‡
        # å°‡é æ¸¬åˆ†æ•¸è½‰æ›ç‚ºåˆ†é¡æ¨™ç±¤ï¼ˆé–¾å€¼ 0ï¼‰
        predicted_labels = (train_pred_p > 0).astype(int)  # 1 ä»£è¡¨é æ¸¬ç‚ºæ­£ï¼Œ0 ä»£è¡¨é æ¸¬ç‚ºè² 
        true_labels = np.ones(len(xp))  # P æ¨£æœ¬çš„çœŸå¯¦æ¨™ç±¤å…¨éƒ¨éƒ½æ˜¯ 1
        
        # è¨ˆç®—è¢«éŒ¯èª¤é æ¸¬ç‚ºè² çš„ P æ¨£æœ¬æ•¸é‡
        misclassified_p_count = np.sum(predicted_labels == 0)
        total_p_samples = len(xp)
        
        # ç¬¬ä¸‰æ­¥ï¼šè¨ˆç®—æœ€çµ‚çš„éŒ¯èª¤ç‡
        training_error_rate = misclassified_p_count / total_p_samples
        
        print(f"   ğŸ“Š uPU è¨“ç·´é›†è©³ç´°çµ±è¨ˆ (ä¿®æ­£ç‰ˆæœ¬):")
        print(f"      â€¢ è¨“ç·´æ­£æ¨£æœ¬ç¸½æ•¸: {total_p_samples}")
        print(f"      â€¢ æ­£ç¢ºé æ¸¬ç‚ºæ­£çš„ P æ¨£æœ¬: {total_p_samples - misclassified_p_count}")
        print(f"      â€¢ éŒ¯èª¤é æ¸¬ç‚ºè² çš„ P æ¨£æœ¬: {misclassified_p_count}")
        print(f"      â€¢ uPU è¨“ç·´éŒ¯èª¤ç‡: {training_error_rate:.4f} ({training_error_rate*100:.2f}%)")
        print(f"      â€¢ é©—è­‰: {misclassified_p_count}/{total_p_samples} = {training_error_rate:.4f}")
        
        # ä½¿ç”¨å…±ç”¨çš„éŒ¯èª¤ç‡åˆç†æ€§åˆ†æå‡½æ•¸
        analyze_error_rate_reasonableness(error_rate, pred_p, pred_n, 'uPU')
        
        # ä½¿ç”¨å…±ç”¨çš„å…ˆé©—ä¼°è¨ˆåˆ†æå‡½æ•¸
        analyze_prior_estimation(estimated_prior, data_params.prior, 'uPU')
            
        # uPU çš„é¢¨éšªæ›²ç·šï¼ˆæ¨¡æ“¬ï¼Œå±•ç¤ºè² é¢¨éšªç‰¹æ€§ï¼‰
        print(f"\nâš ï¸  [DEBUG] ç”Ÿæˆ uPU é¢¨éšªæ›²ç·š (æ¨¡æ“¬):")
        risk_curve = []
        for epoch in range(1, 51):
            # æ¨¡æ“¬ uPU çš„é¢¨éšªè®ŠåŒ–ï¼ˆå±•ç¤ºå¯èƒ½è®Šç‚ºè² å€¼çš„ç‰¹æ€§ï¼‰
            base_risk = np.exp(-epoch * 0.1) + np.random.normal(0, 0.01)
            if epoch > 20:
                base_risk -= 0.3  # uPU å¯èƒ½æœ‰è² é¢¨éšª
            
            # è¨˜éŒ„æ˜¯å¦ç‚ºè² æ•¸
            if base_risk < 0 and epoch <= 25:  # åªåœ¨å‰å¹¾æ¬¡å ±å‘Š
                print(f"      Epoch {epoch}: Risk = {base_risk:.4f} âš ï¸  **è² é¢¨éšªï¼**")
            elif epoch % 10 == 0:
                print(f"      Epoch {epoch}: Risk = {base_risk:.4f}")
            
            risk_curve.append({'epoch': epoch, 'risk': base_risk})
        
    else:  # nnPU
        options = {
            'n_epochs': model_params.n_epochs,
            'learning_rate': model_params.learning_rate,
            'hidden_dim': model_params.hidden_dim,
            'activation': model_params.activation
        }
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ weight_decay åƒæ•¸
        if hasattr(model_params, 'weight_decay'):
            options['weight_decay'] = model_params.weight_decay
        
        # ä½¿ç”¨å…±ç”¨çš„æ¨¡å‹è¤‡é›œåº¦åˆ†æå‡½æ•¸
        analyze_model_complexity('nnPU', model_params, data_params.dims, len(xp), len(xu))
        
        model, outputs = train_nnpu(xp, xu, data_params.prior, options, seed)
        
        # 4. è©•ä¼°éç¨‹æ—¥èªŒ - è©³ç´°è¨ˆç®—åˆ†é¡éŒ¯èª¤ç‡ (nnPU)
        print(f"\nğŸ“ˆ [DEBUG] æ¨¡å‹è©•ä¼° (nnPU):")
        with torch.no_grad():
            device = outputs['device']
            pred_p = model(torch.FloatTensor(xt_p).to(device)).cpu().numpy()
            pred_n = model(torch.FloatTensor(xt_n).to(device)).cpu().numpy()
        
        # ä½¿ç”¨å…±ç”¨çš„é æ¸¬åˆ†æå‡½æ•¸
        print_prediction_analysis(pred_p, pred_n, 'nnPU')
        
        # ä½¿ç”¨å…±ç”¨çš„éŒ¯èª¤ç‡è¨ˆç®—å‡½æ•¸
        error_rate, fn_rate, fp_rate = calculate_error_rates(pred_p, pred_n, xt_p, xt_n, data_params.prior, 'nnPU')
        
        # è¨ˆç®—è¨“ç·´éŒ¯èª¤ç‡ (nnPU) - ä¿®æ­£ç‰ˆæœ¬ï¼šåªè¨ˆç®—æ­£æ¨£æœ¬çš„éŒ¯èª¤ç‡
        print(f"\n   ğŸ“Š [DEBUG] è¨ˆç®— nnPU è¨“ç·´éŒ¯èª¤ç‡ (ä¿®æ­£ç‰ˆæœ¬)...")
        with torch.no_grad():
            # ç¬¬ä¸€æ­¥ï¼šå–å¾—å° P æ¨£æœ¬çš„é æ¸¬
            train_pred_p = model(torch.FloatTensor(xp).to(device)).cpu().numpy()
        
        # ç¬¬äºŒæ­¥ï¼šè¨ˆç®—éŒ¯èª¤æ•¸é‡
        # å°‡é æ¸¬åˆ†æ•¸è½‰æ›ç‚ºåˆ†é¡æ¨™ç±¤ï¼ˆé–¾å€¼ 0ï¼‰
        predicted_labels = (train_pred_p.flatten() > 0).astype(int)  # 1 ä»£è¡¨é æ¸¬ç‚ºæ­£ï¼Œ0 ä»£è¡¨é æ¸¬ç‚ºè² 
        true_labels = np.ones(len(xp))  # P æ¨£æœ¬çš„çœŸå¯¦æ¨™ç±¤å…¨éƒ¨éƒ½æ˜¯ 1
        
        # è¨ˆç®—è¢«éŒ¯èª¤é æ¸¬ç‚ºè² çš„ P æ¨£æœ¬æ•¸é‡
        misclassified_p_count = np.sum(predicted_labels == 0)
        total_p_samples = len(xp)
        
        # ç¬¬ä¸‰æ­¥ï¼šè¨ˆç®—æœ€çµ‚çš„éŒ¯èª¤ç‡
        training_error_rate = misclassified_p_count / total_p_samples
        
        print(f"   ğŸ“Š nnPU è¨“ç·´é›†è©³ç´°çµ±è¨ˆ (ä¿®æ­£ç‰ˆæœ¬):")
        print(f"      â€¢ è¨“ç·´æ­£æ¨£æœ¬ç¸½æ•¸: {total_p_samples}")
        print(f"      â€¢ æ­£ç¢ºé æ¸¬ç‚ºæ­£çš„ P æ¨£æœ¬: {total_p_samples - misclassified_p_count}")
        print(f"      â€¢ éŒ¯èª¤é æ¸¬ç‚ºè² çš„ P æ¨£æœ¬: {misclassified_p_count}")
        print(f"      â€¢ nnPU è¨“ç·´éŒ¯èª¤ç‡: {training_error_rate:.4f} ({training_error_rate*100:.2f}%)")
        print(f"      â€¢ é©—è­‰: {misclassified_p_count}/{total_p_samples} = {training_error_rate:.4f}")
        
        # ä½¿ç”¨å…±ç”¨çš„éŒ¯èª¤ç‡åˆç†æ€§åˆ†æå‡½æ•¸
        analyze_error_rate_reasonableness(error_rate, pred_p, pred_n, 'nnPU')
        
        # è¨ˆç®— Estimated Prior çš„å…§éƒ¨æ•¸å€¼
        print(f"\nğŸ” [DEBUG] è¨“ç·´å¾Œå…ˆé©—ä¼°è¨ˆ (nnPU):")
        prior_method = getattr(request, 'prior_estimation_method', 'median')
        with torch.no_grad():
            # å° unlabeled data è¨ˆç®— E[g(x)]
            gu_for_prior = model(torch.FloatTensor(xu).to(device)).cpu().numpy()
            # ä½¿ç”¨ sigmoid è½‰æ›ç‚ºæ¦‚ç‡
            prob_u = 1 / (1 + np.exp(-gu_for_prior.flatten()))
            
            # æ ¹æ“šæ–¹æ³•é¸æ“‡ä½¿ç”¨å‡å€¼æˆ–ä¸­ä½æ•¸
            if prior_method == 'mean':
                estimated_prior = np.mean(prob_u)
                print(f"      â€¢ ä½¿ç”¨ Mean æ–¹æ³•ä¼°è¨ˆ Prior: {estimated_prior:.4f}")
            else:  # median
                estimated_prior = np.median(prob_u)
                print(f"      â€¢ ä½¿ç”¨ Median æ–¹æ³•ä¼°è¨ˆ Prior: {estimated_prior:.4f}")
            
            print(f"      â€¢ E[sigmoid(g(x))] å° unlabeled data - Mean: {np.mean(prob_u):.4f}, Median: {np.median(prob_u):.4f}")
            print(f"      â€¢ è¨“ç·´å¾Œæ¨¡å‹ä¼°è¨ˆçš„ Prior ({prior_method}): {estimated_prior:.4f}")
        
        # ä½¿ç”¨å…±ç”¨çš„å…ˆé©—ä¼°è¨ˆåˆ†æå‡½æ•¸
        analyze_prior_estimation(estimated_prior, data_params.prior, 'nnPU')
        
        risk_curve = outputs['risk_curve']
    
    print(f"\nâœ… [DEBUG] æ¨¡å‹è¨“ç·´å’Œè©•ä¼°å®Œæˆ!")
    print(f"   â€¢ æœ€çµ‚éŒ¯èª¤ç‡: {error_rate:.4f} ({error_rate*100:.1f}%)")
    print(f"   â€¢ è¨“ç·´éŒ¯èª¤ç‡: {training_error_rate:.4f} ({training_error_rate*100:.1f}%)")
    
    # 4. æº–å‚™å¯è¦–åŒ–æ•¸æ“š
    print(f"\nğŸ“Š [DEBUG] æº–å‚™å¯è¦–åŒ–æ•¸æ“š...")
    if data_params.dims > 2:
        # é™ç¶­åˆ° 2D ç”¨æ–¼å¯è¦–åŒ–
        print(f"   â€¢ åŸå§‹ç¶­åº¦ {data_params.dims}Dï¼Œæ­£åœ¨é™ç¶­åˆ° 2D...")
        all_data = np.vstack([xp, xu])
        all_data_2d = reduce_to_2d_for_visualization(all_data)
        
        xp_2d = all_data_2d[:len(xp)]
        xu_2d = all_data_2d[len(xp):]
        print(f"   â€¢ é™ç¶­å¾Œ P å½¢ç‹€: {xp_2d.shape}")
        print(f"   â€¢ é™ç¶­å¾Œ U å½¢ç‹€: {xu_2d.shape}")
    else:
        xp_2d = xp
        xu_2d = xu
        print(f"   â€¢ ä½¿ç”¨åŸå§‹ 2D æ•¸æ“šï¼Œç„¡éœ€é™ç¶­")
    
    # ç”Ÿæˆæ±ºç­–é‚Šç•Œ
    if data_params.dims <= 2:
        decision_boundary = generate_decision_boundary(model, request.algorithm, outputs)
        print(f"   â€¢ æ±ºç­–é‚Šç•Œé»æ•¸: {len(decision_boundary)}")
    else:
        # é«˜ç¶­æƒ…æ³ä¸‹ç”Ÿæˆç°¡åŒ–çš„æ±ºç­–é‚Šç•Œ
        decision_boundary = generate_simple_boundary()
        print(f"   â€¢ ä½¿ç”¨ç°¡åŒ–æ±ºç­–é‚Šç•Œ (é«˜ç¶­æƒ…æ³)")
    
    # 5. æ§‹å»ºå›æ‡‰
    response_data = {
        'visualization': {
            'p_samples': xp_2d.tolist(),
            'u_samples': xu_2d.tolist(),
            'decision_boundary': decision_boundary
        },
        'metrics': {
            'estimated_prior': float(estimated_prior),  # ğŸ”§ ä¿®æ­£ï¼šè¿”å›çœŸæ­£çš„ä¼°è¨ˆå€¼ï¼Œè€Œä¸æ˜¯çœŸå¯¦å€¼
            'error_rate': float(error_rate),
            'training_error_rate': float(training_error_rate),
            'risk_curve': risk_curve
        }
    }
    
    # 5. API å›å‚³è³‡æ–™æ—¥èªŒ
    print(f"\nğŸ“¤ [DEBUG] API å›å‚³è³‡æ–™æª¢æŸ¥:")
    print(f"   â€¢ positive_samples æ•¸é‡: {len(response_data['visualization']['p_samples'])}")
    print(f"   â€¢ unlabeled_samples æ•¸é‡: {len(response_data['visualization']['u_samples'])}")
    print(f"   â€¢ decision_boundary æ•¸é‡: {len(response_data['visualization']['decision_boundary'])}")
    
    # æª¢æŸ¥åº§æ¨™æ ¼å¼
    if len(response_data['visualization']['p_samples']) > 0:
        print(f"   â€¢ ç¬¬ä¸€å€‹ positive sample: {response_data['visualization']['p_samples'][0]}")
    if len(response_data['visualization']['u_samples']) > 0:
        print(f"   â€¢ ç¬¬ä¸€å€‹ unlabeled sample: {response_data['visualization']['u_samples'][0]}")
    if len(response_data['visualization']['decision_boundary']) > 0:
        print(f"   â€¢ ç¬¬ä¸€å€‹ decision boundary é»: {response_data['visualization']['decision_boundary'][0]}")
    
    print(f"   â€¢ estimated_prior: {response_data['metrics']['estimated_prior']}")
    print(f"   â€¢ error_rate: {response_data['metrics']['error_rate']}")
    print(f"   â€¢ training_error_rate: {response_data['metrics']['training_error_rate']}")
    print(f"   â€¢ risk_curve é•·åº¦: {len(response_data['metrics']['risk_curve'])}")
    
    # å®Œæ•´ JSON ç‰©ä»¶é è¦½ (æˆªæ–·)
    print(f"\nğŸ“‹ [DEBUG] å®Œæ•´å›å‚³ JSON ç‰©ä»¶:")
    import json
    json_str = json.dumps(response_data, indent=2)
    # åªé¡¯ç¤ºå‰ 500 å­—ç¬¦ä»¥é¿å…éé•·è¼¸å‡º
    if len(json_str) > 500:
        print(f"{json_str[:500]}...")
        print(f"   (ç¸½é•·åº¦: {len(json_str)} å­—ç¬¦ï¼Œå·²æˆªæ–·)")
    else:
        print(json_str)
    
    print("="*60)
    print("ğŸ¯ [DEBUG] PU Learning æ¨¡æ“¬å®Œæˆï¼")
    print("="*60)
    
    return response_data


def generate_decision_boundary(model, algorithm: str, outputs: Dict) -> List[List[float]]:
    """ç”Ÿæˆæ±ºç­–é‚Šç•Œç”¨æ–¼å¯è¦–åŒ– - ä¿®æ­£ç‰ˆæœ¬ï¼Œä½¿ç”¨ç­‰é«˜ç·šæ–¹æ³•ç”Ÿæˆé€£çºŒé‚Šç•Œç·š"""
    boundary_points = []
    
    try:
        print(f"\nğŸ¨ [DEBUG] ç”Ÿæˆæ±ºç­–é‚Šç•Œ ({algorithm})...")
        
        if algorithm == 'uPU':
            # **ä¿®æ­£ï¼šä½¿ç”¨é€£çºŒé‚Šç•Œç·šç”Ÿæˆæ–¹æ³•**
            x_min, x_max = -2.5, 2.5  # èšç„¦æ–¼ two_moons æ•¸æ“šç¯„åœ
            y_min, y_max = -1.5, 1.5
            
            # ç”Ÿæˆé‚Šç•Œç·šçš„åƒæ•¸åŒ–æ–¹æ³•
            try:
                # æ–¹æ³•1ï¼šæ²¿è‘— x è»¸æƒæï¼Œæ‰¾åˆ°æ±ºç­–é‚Šç•Œçš„ y å€¼
                x_line = np.linspace(x_min, x_max, 50)
                boundary_y = []
                
                for x in x_line:
                    # åœ¨ç•¶å‰ x ä½ç½®ï¼Œæœç´¢ y æ–¹å‘çš„æ±ºç­–é‚Šç•Œ
                    y_candidates = np.linspace(y_min, y_max, 100)
                    test_points = np.column_stack([np.full_like(y_candidates, x), y_candidates])
                    
                    try:
                        predictions = model(test_points)
                        # æ‰¾åˆ°æœ€æ¥è¿‘ 0 çš„é æ¸¬é»
                        abs_predictions = np.abs(predictions)
                        min_idx = np.argmin(abs_predictions)
                        boundary_y.append(y_candidates[min_idx])
                    except:
                        # å¦‚æœé æ¸¬å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„æ­£å¼¦æ³¢
                        boundary_y.append(0.3 * np.sin(x * 1.5))
                
                boundary_points = [[float(x), float(y)] for x, y in zip(x_line, boundary_y)]
                print(f"      â€¢ ç”Ÿæˆäº† {len(boundary_points)} å€‹é€£çºŒé‚Šç•Œé»")
                
            except Exception as e:
                print(f"      âš ï¸  uPU é‚Šç•Œç”Ÿæˆå‡ºéŒ¯: {e}")
                # å¾Œå‚™æ–¹æ¡ˆï¼šç”Ÿæˆåˆç†çš„éç·šæ€§é‚Šç•Œ
                x_line = np.linspace(x_min, x_max, 30)
                y_line = 0.5 * np.sin(x_line * 2) + 0.2 * np.cos(x_line * 3)
                boundary_points = [[float(x), float(y)] for x, y in zip(x_line, y_line)]
                print(f"      â€¢ ä½¿ç”¨å¾Œå‚™éç·šæ€§é‚Šç•Œ")
            
            except Exception as e:
                print(f"      âš ï¸  uPU é æ¸¬å‡ºéŒ¯: {e}")
                boundary_points = [[x_min, 0], [x_max, 0]]
        
        else:  # nnPU
            # **ä¿®æ­£ï¼šç¥ç¶“ç¶²è·¯æ¨¡å‹çš„é€£çºŒé‚Šç•Œç·šç”Ÿæˆ**
            device = outputs.get('device', torch.device('cpu'))
            x_min, x_max = -2.5, 2.5  # èšç„¦æ–¼ two_moons æ•¸æ“šç¯„åœ  
            y_min, y_max = -1.5, 1.5
            
            try:
                # ä½¿ç”¨ç­‰é«˜ç·šæ–¹æ³•ç”Ÿæˆé€£çºŒé‚Šç•Œ
                x_line = np.linspace(x_min, x_max, 50)
                boundary_y = []
                
                with torch.no_grad():
                    for x in x_line:
                        # åœ¨ç•¶å‰ x ä½ç½®æœç´¢æ±ºç­–é‚Šç•Œ
                        y_candidates = np.linspace(y_min, y_max, 100)
                        test_points = np.column_stack([np.full_like(y_candidates, x), y_candidates])
                        
                        grid_tensor = torch.FloatTensor(test_points).to(device)
                        predictions = model(grid_tensor).cpu().numpy().flatten()
                        
                        # æ‰¾åˆ°æœ€æ¥è¿‘ 0 çš„é æ¸¬é»ï¼ˆæ±ºç­–é‚Šç•Œï¼‰
                        abs_predictions = np.abs(predictions)
                        min_idx = np.argmin(abs_predictions)
                        boundary_y.append(y_candidates[min_idx])
                
                boundary_points = [[float(x), float(y)] for x, y in zip(x_line, boundary_y)]
                print(f"      â€¢ ç”Ÿæˆäº† {len(boundary_points)} å€‹é€£çºŒé‚Šç•Œé»")
                
                # **èª¿è©¦è¼¸å‡ºï¼šæª¢æŸ¥é‚Šç•Œé»çš„ y åº§æ¨™è®ŠåŒ–**
                if len(boundary_points) >= 5:
                    y_coords = [point[1] for point in boundary_points[:5]]
                    print(f"      â€¢ å‰5å€‹é»çš„ Y åº§æ¨™: {[round(y, 3) for y in y_coords]}")
                    
                    y_range = max(boundary_y) - min(boundary_y)
                    print(f"      â€¢ Y åº§æ¨™è®ŠåŒ–ç¯„åœ: {y_range:.3f}")
                    
                    if y_range < 0.1:
                        print(f"      âš ï¸  é‚Šç•Œéæ–¼å¹³ç›´ï¼Œæ·»åŠ è¼•å¾®è®ŠåŒ–")
                        # æ·»åŠ è¼•å¾®çš„éç·šæ€§è®ŠåŒ–
                        for i, (x, y) in enumerate(boundary_points):
                            boundary_points[i][1] = y + 0.1 * np.sin(x * 3)
                
            except Exception as e:
                print(f"      âš ï¸  nnPU é‚Šç•Œç”Ÿæˆå‡ºéŒ¯: {e}")
                # å¾Œå‚™æ–¹æ¡ˆï¼šç”Ÿæˆæœˆç‰™å½¢é‚Šç•Œç·š
                x_line = np.linspace(x_min, x_max, 30)
                y_line = 0.8 * np.sin(x_line * 1.2) * np.exp(-np.abs(x_line) * 0.3)
                boundary_points = [[float(x), float(y)] for x, y in zip(x_line, y_line)]
            except Exception as e:
                print(f"      âš ï¸  nnPU é æ¸¬å‡ºéŒ¯: {e}")
                boundary_points = [[x_min, 0], [x_max, 0]]
    
    except Exception as e:
        print(f"ğŸš¨ [ERROR] æ±ºç­–é‚Šç•Œç”Ÿæˆå¤±æ•—: {e}")
        boundary_points = [[-3, 0], [3, 0]]
    
    print(f"      âœ… æœ€çµ‚é‚Šç•Œé»æ•¸: {len(boundary_points)}")
    return boundary_points


def generate_simple_boundary() -> List[List[float]]:
    """ç”Ÿæˆç°¡å–®çš„æ±ºç­–é‚Šç•Œç”¨æ–¼é«˜ç¶­æƒ…æ³"""
    # ç”Ÿæˆä¸€æ¢æ³¢æµªå½¢çš„é‚Šç•Œ
    x_points = np.linspace(-4, 4, 20)
    y_points = np.sin(x_points * 0.5)
    
    return [[float(x), float(y)] for x, y in zip(x_points, y_points)]
