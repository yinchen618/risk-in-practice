import os
import json
import logging
from datetime import datetime
from typing import Optional, List, Dict, Any
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import declarative_base
from sqlalchemy import Column, String, Float, Integer, DateTime, Boolean, Text, ForeignKey, JSON, Index, text
from sqlalchemy.future import select
import uuid

# Import base models and connection from core
from core.database import Ammeter, AmmeterLog, Base, engine, async_session, init_database as core_init_database

# è¨­ç½® logger
logger = logging.getLogger(__name__)

# Extend engine configuration for PU learning specific needs
# (Keep the existing engine configuration if needed)

# Import core DatabaseManager for inheritance
from core.database import DatabaseManager as CoreDatabaseManager

# Initialize database with all models (core + extended)
async def init_database():
    """Initialize database with both core and PU learning models"""
    await core_init_database()  # Initialize core models
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)  # Initialize extended models
    print("PostgreSQL è³‡æ–™è¡¨å·²å»ºç«‹ (Core + PU Learning)")

# CRUD ç¯„ä¾‹
class DatabaseManager(CoreDatabaseManager):
    def __init__(self):
        super().__init__()

    # PU Learning è¨“ç·´æ¨¡å‹ç›¸é—œæ–¹æ³•
    async def save_trained_model(self, model_data: Dict[str, Any]) -> Optional[str]:
        """ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹è³‡è¨Šåˆ°æ•¸æ“šåº«"""
        async with self.session_factory() as session:
            trained_model = TrainedModel(**model_data)
            session.add(trained_model)
            await session.commit()
            return trained_model.id

    async def create_evaluation_run(
        self,
        name: str,
        scenario_type: str,
        trained_model_id: str,
        test_set_source: dict,
        evaluation_metrics: dict = None
    ) -> dict:
        """å‰µå»ºæ–°çš„è©•ä¼°é‹è¡Œè¨˜éŒ„"""
        from datetime import datetime
        import uuid

        evaluation_run_id = str(uuid.uuid4())

        max_retries = 3
        for attempt in range(max_retries):
            try:
                async with self.session_factory() as session:
                    # ä½¿ç”¨åŸç”Ÿ SQL æ’å…¥ EvaluationRun
                    insert_sql = """
                        INSERT INTO evaluation_runs (
                            id, name, scenario_type, status, trained_model_id,
                            test_set_source, evaluation_metrics, created_at
                        ) VALUES (
                            :id, :name, :scenario_type, :status, :trained_model_id,
                            :test_set_source, :evaluation_metrics, :created_at
                        )
                    """

                    await session.execute(
                        text(insert_sql),
                        {
                            "id": evaluation_run_id,
                            "name": name,
                            "scenario_type": scenario_type,
                            "status": "RUNNING",
                            "trained_model_id": trained_model_id,
                            "test_set_source": json.dumps(test_set_source),
                            "evaluation_metrics": json.dumps(evaluation_metrics or {}),
                            "created_at": datetime.utcnow()
                        }
                    )
                    await session.commit()

                    return {
                        "id": evaluation_run_id,
                        "name": name,
                        "scenario_type": scenario_type,
                        "status": "RUNNING",
                        "trained_model_id": trained_model_id,
                        "test_set_source": test_set_source,
                        "evaluation_metrics": evaluation_metrics or {},
                        "created_at": datetime.utcnow()
                    }
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                import asyncio
                await asyncio.sleep(1)

    async def update_evaluation_run(
        self,
        evaluation_run_id: str,
        status: str = None,
        evaluation_metrics: dict = None
    ) -> bool:
        """æ›´æ–°è©•ä¼°é‹è¡Œçµæœ"""
        from datetime import datetime

        max_retries = 3
        for attempt in range(max_retries):
            try:
                async with self.session_factory() as session:
                    update_data = {}
                    if status:
                        update_data["status"] = status
                    if evaluation_metrics:
                        update_data["evaluation_metrics"] = json.dumps(evaluation_metrics)
                    if status == "COMPLETED":
                        update_data["completed_at"] = datetime.utcnow()

                    if update_data:
                        # æ§‹å»º SET å­å¥
                        set_clauses = []
                        params = {"id": evaluation_run_id}

                        for key, value in update_data.items():
                            set_clauses.append(f"{key} = :{key}")
                            params[key] = value

                        update_sql = f"""
                            UPDATE evaluation_runs
                            SET {', '.join(set_clauses)}
                            WHERE id = :id
                        """

                        result = await session.execute(text(update_sql), params)
                        await session.commit()
                        return result.rowcount > 0
                    return True
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                import asyncio
                await asyncio.sleep(1)

    async def get_evaluation_runs_by_model(
        self,
        trained_model_id: str
    ) -> list:
        """ç²å–ç‰¹å®šæ¨¡å‹çš„æ‰€æœ‰è©•ä¼°é‹è¡Œ"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                async with self.session_factory() as session:
                    select_sql = """
                        SELECT id, name, scenario_type, status, trained_model_id,
                               test_set_source, evaluation_metrics, created_at, completed_at
                        FROM evaluation_runs
                        WHERE trained_model_id = :trained_model_id
                        ORDER BY created_at DESC
                    """

                    result = await session.execute(
                        text(select_sql),
                        {"trained_model_id": trained_model_id}
                    )

                    evaluations = []
                    for row in result:
                        # å®‰å…¨è™•ç† JSON æ¬„ä½ï¼Œæª¢æŸ¥è³‡æ–™é¡å‹
                        test_set_source = row.test_set_source
                        if isinstance(test_set_source, str):
                            try:
                                test_set_source = json.loads(test_set_source)
                            except (json.JSONDecodeError, TypeError):
                                test_set_source = {}
                        elif not isinstance(test_set_source, dict):
                            test_set_source = {}

                        evaluation_metrics = row.evaluation_metrics
                        if isinstance(evaluation_metrics, str):
                            try:
                                evaluation_metrics = json.loads(evaluation_metrics)
                            except (json.JSONDecodeError, TypeError):
                                evaluation_metrics = {}
                        elif not isinstance(evaluation_metrics, dict):
                            evaluation_metrics = {}

                        evaluation = {
                            "id": row.id,
                            "name": row.name,
                            "scenario_type": row.scenario_type,
                            "status": row.status,
                            "trained_model_id": row.trained_model_id,
                            "test_set_source": test_set_source,
                            "evaluation_metrics": evaluation_metrics,
                            "created_at": row.created_at,
                            "completed_at": row.completed_at
                        }
                        evaluations.append(evaluation)

                    return evaluations
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                import asyncio
                await asyncio.sleep(1)

    async def get_trained_model_by_id(self, model_id: str) -> dict:
        """ç²å–è¨“ç·´æ¨¡å‹è©³ç´°ä¿¡æ¯"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                async with self.session_factory() as session:
                    result = await session.execute(
                        select(TrainedModel).where(TrainedModel.id == model_id)
                    )
                    model = result.scalar_one_or_none()

                    if model:
                        # å®‰å…¨è™•ç† JSON æ¬„ä½
                        model_config = model.model_config
                        if isinstance(model_config, str):
                            try:
                                model_config = json.loads(model_config)
                            except (json.JSONDecodeError, TypeError):
                                model_config = {}
                        elif not isinstance(model_config, dict):
                            model_config = {}

                        data_source_config = model.data_source_config
                        if isinstance(data_source_config, str):
                            try:
                                data_source_config = json.loads(data_source_config)
                            except (json.JSONDecodeError, TypeError):
                                data_source_config = {}
                        elif not isinstance(data_source_config, dict):
                            data_source_config = {}

                        training_metrics = model.training_metrics
                        if isinstance(training_metrics, str):
                            try:
                                training_metrics = json.loads(training_metrics)
                            except (json.JSONDecodeError, TypeError):
                                training_metrics = {}
                        elif not isinstance(training_metrics, dict):
                            training_metrics = {}

                        # å¾ model_config ä¸­æå– model_type
                        model_type = model_config.get("model_type", "unknown") if isinstance(model_config, dict) else "unknown"

                        return {
                            "id": model.id,
                            "name": model.name,
                            "scenario_type": model.scenario_type,
                            "status": model.status,
                            "experiment_run_id": model.experiment_run_id,
                            "model_config": model_config,
                            "data_source_config": data_source_config,
                            "model_path": model.model_path,
                            "training_metrics": training_metrics,
                            "created_at": model.created_at,
                            "completed_at": model.completed_at,
                            "model_type": model_type
                        }
                    return None
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                import asyncio
                await asyncio.sleep(1)

    async def get_anomaly_events_for_evaluation(
        self,
        selected_floors_by_building: Dict[str, List[str]],
        start_date: str,
        end_date: str,
        start_time: str = "00:00",
        end_time: str = "23:59"
    ) -> Optional['pd.DataFrame']:
        """
        ç²å–æŒ‡å®šç¯„åœçš„æ¸¬è©¦æ•¸æ“šï¼Œå¾ ammeter_log çœŸå¯¦æ•¸æ“šç”Ÿæˆæ¨£æœ¬ç”¨æ–¼æ¨¡å‹è©•ä¼°

        Args:
            selected_floors_by_building: é¸æ“‡çš„å»ºç¯‰æ¨“å±¤ï¼ˆæœƒè¢«å¿½ç•¥ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨è¨­å‚™ï¼‰
            start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
            start_time: é–‹å§‹æ™‚é–“ (HH:MM)
            end_time: çµæŸæ™‚é–“ (HH:MM)

        Returns:
            DataFrame: åŒ…å«ç‰¹å¾µçš„æ¸¬è©¦æ•¸æ“šï¼ˆç”¨æ–¼é æ¸¬ï¼‰
        """
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        from sqlalchemy import text

        try:
            async with self.session_factory() as session:
                # æ§‹å»ºæ™‚é–“ç¯„åœæŸ¥è©¢æ¢ä»¶
                start_datetime = datetime.strptime(f"{start_date} {start_time}", "%Y-%m-%d %H:%M")
                end_datetime = datetime.strptime(f"{end_date} {end_time}", "%Y-%m-%d %H:%M")

                logger.info("ğŸ”„ ä½¿ç”¨ ammeter_log çœŸå¯¦æ•¸æ“šç”Ÿæˆè©•ä¼°æ¨£æœ¬")

                # å¾ ammeter_log ç²å–æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„çœŸå¯¦æ•¸æ“š
                query = text("""
                    SELECT
                        "deviceNumber" as meter_id,
                        "lastUpdated" as timestamp,
                        voltage,
                        currents as current,
                        power,
                        battery
                    FROM ammeter_log
                    WHERE "lastUpdated" >= :start_time
                        AND "lastUpdated" <= :end_time
                        AND voltage IS NOT NULL
                        AND currents IS NOT NULL
                        AND power IS NOT NULL
                    ORDER BY "lastUpdated" DESC
                    LIMIT 1000
                """)

                result = await session.execute(query, {
                    'start_time': start_datetime,
                    'end_time': end_datetime
                })

                raw_data = result.fetchall()

                if not raw_data:
                    logger.warning(f"âš ï¸ åœ¨æ™‚é–“ç¯„åœ {start_datetime} åˆ° {end_datetime} å…§æ²’æœ‰æ‰¾åˆ°é›»è¡¨æ•¸æ“š")
                    # æ“´å±•æ™‚é–“ç¯„åœå†è©¦ä¸€æ¬¡
                    extended_start = start_datetime - timedelta(days=7)
                    extended_end = end_datetime + timedelta(days=1)
                    logger.info(f"ğŸ”„ æ“´å±•æ™‚é–“ç¯„åœè‡³ {extended_start} åˆ° {extended_end}")

                    result = await session.execute(query, {
                        'start_time': extended_start,
                        'end_time': extended_end
                    })
                    raw_data = result.fetchall()

                if raw_data:
                    # è½‰æ›ç‚º DataFrame
                    df = pd.DataFrame(raw_data, columns=['meter_id', 'timestamp', 'voltage', 'current', 'power', 'battery'])

                    # ç”ŸæˆçœŸå¯¦æ•¸æ“šç‰¹å¾µ
                    samples = self._generate_real_test_features(df, selected_floors_by_building)

                    if samples:
                        test_df = pd.DataFrame(samples)
                        logger.info(f"ğŸ¯ å¾çœŸå¯¦æ•¸æ“šç”Ÿæˆæ¸¬è©¦æ¨£æœ¬: {len(test_df)} æ¢è¨˜éŒ„")
                        logger.info(f"ğŸ“Š æ¶µè“‹é›»è¡¨: {test_df['meter_id'].nunique()} å€‹")
                        logger.info(f"â° æ™‚é–“ç¯„åœ: {test_df['timestamp'].min()} åˆ° {test_df['timestamp'].max()}")
                        return test_df
                    else:
                        logger.warning("âš ï¸ ç„¡æ³•å¾çœŸå¯¦æ•¸æ“šç”Ÿæˆç‰¹å¾µ")
                        return None
                else:
                    logger.warning("âš ï¸ åœ¨æ“´å±•æ™‚é–“ç¯„åœå…§ä»æœªæ‰¾åˆ°é›»è¡¨æ•¸æ“š")
                    return None

        except Exception as e:
            logger.error(f"å¾ ammeter_log ç²å–çœŸå¯¦æ¸¬è©¦æ•¸æ“šå¤±æ•—: {e}")
            return None

    def _generate_real_test_features(self, df: 'pd.DataFrame', selected_floors_by_building: Dict[str, List[str]]) -> List[Dict]:
        """å¾çœŸå¯¦é›»è¡¨æ•¸æ“šç”Ÿæˆæ¸¬è©¦ç‰¹å¾µ"""
        import numpy as np
        import pandas as pd
        from datetime import timedelta

        samples = []

        # æŒ‰é›»è¡¨åˆ†çµ„è™•ç†æ•¸æ“š
        for meter_id, meter_data in df.groupby('meter_id'):
            if len(meter_data) < 5:  # éœ€è¦è¶³å¤ çš„æ•¸æ“šé»ä¾†è¨ˆç®—çµ±è¨ˆç‰¹å¾µ
                continue

            # æŒ‰æ™‚é–“æ’åº
            meter_data = meter_data.sort_values('timestamp')

            # ç‚ºæ¯å€‹æ™‚é–“çª—å£ç”Ÿæˆç‰¹å¾µ
            for i in range(len(meter_data)):
                row = meter_data.iloc[i]

                # ç²å–ç•¶å‰æ™‚é–“çª—å£çš„æ•¸æ“šï¼ˆåŒ…æ‹¬å‰é¢çš„å¹¾å€‹é»ï¼‰
                window_size = min(10, i + 1)  # æœ€å¤šä½¿ç”¨å‰10å€‹æ•¸æ“šé»
                window_data = meter_data.iloc[max(0, i - window_size + 1):i + 1]

                if len(window_data) < 3:  # è‡³å°‘éœ€è¦3å€‹æ•¸æ“šé»
                    continue

                # è¨ˆç®—çµ±è¨ˆç‰¹å¾µ
                power_values = window_data['power'].values
                voltage_values = window_data['voltage'].values
                current_values = window_data['current'].values

                # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ
                power_mean = np.mean(power_values)
                power_std = np.std(power_values) if len(power_values) > 1 else 0
                power_max = np.max(power_values)
                power_min = np.min(power_values)
                power_range = power_max - power_min
                power_variance = np.var(power_values) if len(power_values) > 1 else 0

                # é›»å£“å’Œé›»æµç‰¹å¾µ
                voltage_mean = np.mean(voltage_values)
                current_mean = np.mean(current_values)

                # è¶¨å‹¢ç‰¹å¾µï¼ˆå¦‚æœæœ‰è¶³å¤ æ•¸æ“šé»ï¼‰
                if len(power_values) >= 3:
                    # è¨ˆç®—ç°¡å–®ç·šæ€§è¶¨å‹¢
                    x = np.arange(len(power_values))
                    try:
                        slope = np.polyfit(x, power_values, 1)[0]
                        power_trend = slope
                    except:
                        power_trend = 0
                else:
                    power_trend = 0

                # æ™‚é–“ç‰¹å¾µ
                timestamp = row['timestamp']
                hour_of_day = timestamp.hour
                day_of_week = timestamp.weekday()
                is_weekend = 1 if day_of_week >= 5 else 0

                # å¾è¨­å‚™IDæ˜ å°„å»ºç¯‰å’Œæ¨“å±¤ï¼ˆç°¡åŒ–æ˜ å°„ï¼‰
                building, floor = self._map_device_to_building_floor(meter_id)

                features = {
                    'meter_id': meter_id,
                    'timestamp': timestamp,
                    'building': building,
                    'floor': floor,
                    'power_consumption': power_mean,
                    'power_variance': power_variance,
                    'power_trend': power_trend,
                    'power_max': power_max,
                    'power_min': power_min,
                    'power_std': power_std,
                    'power_range': power_range,
                    'voltage_mean': voltage_mean,
                    'current_mean': current_mean,
                    'hour_of_day': hour_of_day,
                    'day_of_week': day_of_week,
                    'is_weekend': is_weekend,
                }

                samples.append(features)

                # é™åˆ¶æ¨£æœ¬æ•¸é‡
                if len(samples) >= 1000:
                    break

            if len(samples) >= 1000:
                break

        logger.info(f"ğŸ”§ å¾çœŸå¯¦æ•¸æ“šç”Ÿæˆç‰¹å¾µ: {len(samples)} æ¢è¨˜éŒ„")
        return samples

    def _map_device_to_building_floor(self, device_id: str) -> tuple:
        """å°‡è¨­å‚™IDæ˜ å°„åˆ°å»ºç¯‰å’Œæ¨“å±¤ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        # æ ¹æ“šè¨­å‚™IDå‰ç¶´æˆ–å…¶ä»–è¦å‰‡æ˜ å°„åˆ°å»ºç¯‰æ¨“å±¤
        # é€™è£¡ä½¿ç”¨ç°¡åŒ–çš„æ˜ å°„é‚è¼¯
        if device_id.startswith('402A8FB0'):
            return 'Building A', '1'
        elif device_id.startswith('E8FDF8B4'):
            return 'Building A', '2'
        elif device_id.startswith('402A8FB1'):
            return 'Building B', '1'
        else:
            # é»˜èªæ˜ å°„
            return 'Building A', '2'

# ç•°å¸¸äº‹ä»¶ç›¸é—œè¡¨
class AnomalyEvent(Base):
    __tablename__ = "anomaly_events"
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    event_id = Column(String, nullable=False, unique=True)
    meter_id = Column(String, nullable=False)
    event_timestamp = Column(DateTime, nullable=False)
    detection_rule = Column(String, nullable=False)
    score = Column(Float, nullable=False)
    data_window = Column(JSON)  # å­˜å„²æ™‚é–“åºåˆ—è³‡æ–™
    status = Column(String, nullable=False, default="UNREVIEWED")  # UNREVIEWED, CONFIRMED_POSITIVE, REJECTED_NORMAL
    reviewer_id = Column(String)
    review_timestamp = Column(DateTime)
    justification_notes = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # ç‚ºæ•ˆèƒ½å»ºç«‹ç´¢å¼•
    __table_args__ = (
        Index('idx_anomaly_event_meter_timestamp', 'meter_id', 'event_timestamp'),
        Index('idx_anomaly_event_status', 'status'),
        Index('idx_anomaly_event_timestamp', 'event_timestamp'),
    )

class AnomalyLabel(Base):
    __tablename__ = "anomaly_labels"
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    name = Column(String, nullable=False)
    description = Column(Text)
    color = Column(String, default="#6B7280")  # æ¨™ç±¤é¡è‰²
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class EventLabelLink(Base):
    __tablename__ = "event_label_links"
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    event_id = Column(String, ForeignKey('anomaly_events.id'), nullable=False)
    label_id = Column(String, ForeignKey('anomaly_labels.id'), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

    # ç¢ºä¿åŒä¸€äº‹ä»¶å’Œæ¨™ç±¤çš„çµ„åˆå”¯ä¸€
    __table_args__ = (
        Index('idx_event_label_unique', 'event_id', 'label_id', unique=True),
    )

# æ™‚é–“åºåˆ—è³‡æ–™è¡¨ï¼ˆç”¨æ–¼å­˜å„²è©³ç´°çš„æ„Ÿæ¸¬å™¨è³‡æ–™ï¼‰
class TimeSeriesData(Base):
    __tablename__ = "timeseries_data"
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    device_id = Column(String, nullable=False)
    timestamp = Column(DateTime, nullable=False)
    metric_name = Column(String, nullable=False)  # voltage, current, power, etc.
    value = Column(Float, nullable=False)
    unit = Column(String)  # V, A, W, etc.
    created_at = Column(DateTime, default=datetime.utcnow)

    # ç‚ºæŸ¥è©¢æ•ˆèƒ½å»ºç«‹é—œéµç´¢å¼•
    __table_args__ = (
        Index('idx_timeseries_device_timestamp', 'device_id', 'timestamp'),
        Index('idx_timeseries_timestamp', 'timestamp'),
        Index('idx_timeseries_device_metric', 'device_id', 'metric_name'),
    )

# PU Learning è¨“ç·´æ¨¡å‹è¡¨
class TrainedModel(Base):
    __tablename__ = "trained_models"
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    name = Column(String, nullable=False)
    experiment_run_id = Column(String, nullable=False)

    # è¨“ç·´æƒ…å¢ƒç›¸é—œï¼ˆä½¿ç”¨å¯¦éš›è³‡æ–™åº«æ¬„ä½åç¨±ï¼‰
    scenario_type = Column('scenario_type', String, nullable=False) # 'ERM_BASELINE' or 'DOMAIN_ADAPTATION'
    data_source_config = Column('data_source_config', JSON) # Source data and split configuration
    model_config = Column('model_config', JSON)  # Model hyperparameters

    # è¨“ç·´çµæœ
    model_path = Column('model_path', String, nullable=True) # Path to the stored model file, available on completion
    training_metrics = Column('training_metrics', JSON, nullable=True)  # Training and validation metrics

    # ç‹€æ…‹èˆ‡æ™‚é–“æˆ³
    status = Column(String, nullable=False, default="RUNNING")  # RUNNING, COMPLETED, FAILED
    created_at = Column('created_at', DateTime, default=datetime.utcnow)
    completed_at = Column('completed_at', DateTime, nullable=True)

    # ç´¢å¼•
    __table_args__ = (
        Index('idx_trained_model_experiment', 'experiment_run_id'),
        Index('idx_trained_model_status', 'status'),
    )

    async def create_model_predictions(
        self,
        evaluation_run_id: str,
        predictions: List[Dict[str, Any]]
    ) -> List[str]:
        """æ‰¹é‡å‰µå»ºæ¨¡å‹é æ¸¬è¨˜éŒ„"""
        from datetime import datetime
        import uuid

        prediction_ids = []
        max_retries = 3

        for attempt in range(max_retries):
            try:
                async with self.session_factory() as session:
                    for pred in predictions:
                        prediction_id = str(uuid.uuid4())
                        prediction_ids.append(prediction_id)

                        # ä½¿ç”¨åŸç”Ÿ SQL æ’å…¥ ModelPrediction
                        insert_sql = """
                            INSERT INTO model_predictions (
                                id, evaluation_run_id, anomaly_event_id, timestamp,
                                prediction_score, ground_truth
                            ) VALUES (
                                :id, :evaluation_run_id, :anomaly_event_id, :timestamp,
                                :prediction_score, :ground_truth
                            )
                        """

                        await session.execute(
                            text(insert_sql),
                            {
                                "id": prediction_id,
                                "evaluation_run_id": evaluation_run_id,
                                "anomaly_event_id": pred.get("anomaly_event_id"),
                                "timestamp": pred.get("timestamp", datetime.utcnow()),
                                "prediction_score": float(pred["prediction_score"]),
                                "ground_truth": int(pred["ground_truth"]) if pred.get("ground_truth") is not None else None
                            }
                        )

                    await session.commit()
                    return prediction_ids

            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                import asyncio
                await asyncio.sleep(1)

        return prediction_ids


db_manager = DatabaseManager()
