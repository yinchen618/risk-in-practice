#!/usr/bin/env python3
"""
é©—è­‰å’Œä¿®æ­£ AnalysisDataset çš„ startDate å’Œ endDate
ç¢ºä¿å®ƒå€‘æ­£ç¢ºæ¶µè“‹ç›¸é—œ AnalysisReadyData çš„æ™‚é–“ç¯„åœ
"""

import sqlite3
import sys
import os
from datetime import datetime
from typing import List, Tuple, Dict, Any

# æ·»åŠ å¾Œç«¯è·¯å¾‘ä»¥ä¾¿å°å…¥æ¨¡çµ„
sys.path.append('/home/infowin/Git-projects/pu-in-practice/backend')

def connect_to_database():
    """é€£æ¥åˆ° SQLite è³‡æ–™åº«"""
    db_path = '/home/infowin/Git-projects/pu-in-practice/backend/database/prisma/pu_practice.db'
    if not os.path.exists(db_path):
        print(f"âŒ è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨: {db_path}")
        return None

    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row  # ä½¿çµæœå¯ä»¥åƒå­—å…¸ä¸€æ¨£å­˜å–
        return conn
    except Exception as e:
        print(f"âŒ é€£æ¥è³‡æ–™åº«å¤±æ•—: {e}")
        return None

def get_analysis_datasets(conn) -> List[Dict[str, Any]]:
    """å–å¾—æ‰€æœ‰ AnalysisDataset"""
    cursor = conn.cursor()
    cursor.execute("""
        SELECT id, name, start_date, end_date, building, floor, room, occupant_type
        FROM analysis_datasets
        ORDER BY created_at
    """)

    datasets = []
    for row in cursor.fetchall():
        datasets.append({
            'id': row['id'],
            'name': row['name'],
            'start_date': row['start_date'],
            'end_date': row['end_date'],
            'building': row['building'],
            'floor': row['floor'],
            'room': row['room'],
            'occupant_type': row['occupant_type']
        })

    return datasets

def get_data_range_for_dataset(conn, dataset_id: str) -> Tuple[str, str, int, int]:
    """å–å¾—ç‰¹å®š dataset å°æ‡‰çš„ AnalysisReadyData çš„æ™‚é–“ç¯„åœå’Œçµ±è¨ˆè³‡è¨Š"""
    cursor = conn.cursor()
    cursor.execute("""
        SELECT
            MIN(timestamp) as min_timestamp,
            MAX(timestamp) as max_timestamp,
            COUNT(*) as record_count,
            SUM(CASE WHEN is_positive_label = 1 THEN 1 ELSE 0 END) as positive_count
        FROM analysis_ready_data
        WHERE dataset_id = ?
    """, (dataset_id,))

    result = cursor.fetchone()
    if result and result['min_timestamp'] and result['max_timestamp']:
        return result['min_timestamp'], result['max_timestamp'], result['record_count'], result['positive_count']
    else:
        return None, None, 0, 0

def verify_dataset_date_ranges(conn):
    """é©—è­‰æ‰€æœ‰ AnalysisDataset çš„æ—¥æœŸç¯„åœ"""
    print("ğŸ” é©—è­‰ AnalysisDataset çš„æ—¥æœŸç¯„åœ...")
    print("=" * 80)

    datasets = get_analysis_datasets(conn)
    issues_found = []

    for dataset in datasets:
        dataset_id = dataset['id']
        dataset_name = dataset['name']
        dataset_start = dataset['start_date']
        dataset_end = dataset['end_date']

        print(f"\nğŸ“Š æª¢æŸ¥è³‡æ–™é›†: {dataset_name}")
        print(f"   è³‡æ–™é›† ID: {dataset_id}")
        def format_timestamp(ts):
            if isinstance(ts, int):
                return datetime.fromtimestamp(ts / 1000).strftime('%Y-%m-%d %H:%M:%S')
            elif isinstance(ts, str):
                return datetime.fromisoformat(ts.replace('Z', '+00:00')).strftime('%Y-%m-%d %H:%M:%S')
            else:
                return str(ts)

        print(f"   è³‡æ–™é›†è¨˜éŒ„çš„æ™‚é–“ç¯„åœ: {format_timestamp(dataset_start)} ~ {format_timestamp(dataset_end)}")

        # å–å¾—å¯¦éš›çš„è³‡æ–™æ™‚é–“ç¯„åœ
        actual_start, actual_end, record_count = get_data_range_for_dataset(conn, dataset_id)

        if actual_start is None or actual_end is None:
            print(f"   âš ï¸  æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„ AnalysisReadyData (è¨˜éŒ„æ•¸: {record_count})")
            continue

        print(f"   å¯¦éš›è³‡æ–™çš„æ™‚é–“ç¯„åœ: {format_timestamp(actual_start)} ~ {format_timestamp(actual_end)}")
        print(f"   å¯¦éš›è³‡æ–™è¨˜éŒ„æ•¸: {record_count}")

        # æ¯”è¼ƒæ—¥æœŸç¯„åœ (è™•ç† Unix æ™‚é–“æˆ³æ ¼å¼)
        def parse_timestamp(ts):
            if isinstance(ts, int):
                # Unix æ™‚é–“æˆ³ (æ¯«ç§’)
                return datetime.fromtimestamp(ts / 1000)
            elif isinstance(ts, str):
                # ISO æ ¼å¼
                return datetime.fromisoformat(ts.replace('Z', '+00:00'))
            else:
                return ts

        dataset_start_dt = parse_timestamp(dataset_start)
        dataset_end_dt = parse_timestamp(dataset_end)
        actual_start_dt = parse_timestamp(actual_start)
        actual_end_dt = parse_timestamp(actual_end)

        # æª¢æŸ¥æ˜¯å¦æœ‰å•é¡Œ
        start_mismatch = abs((dataset_start_dt - actual_start_dt).total_seconds()) > 60  # å…è¨±1åˆ†é˜èª¤å·®
        end_mismatch = abs((dataset_end_dt - actual_end_dt).total_seconds()) > 60

        if start_mismatch or end_mismatch:
            print(f"   âŒ ç™¼ç¾æ—¥æœŸç¯„åœä¸åŒ¹é…!")
            if start_mismatch:
                diff_hours = (dataset_start_dt - actual_start_dt).total_seconds() / 3600
                print(f"      é–‹å§‹æ™‚é–“å·®ç•°: {diff_hours:.2f} å°æ™‚")
            if end_mismatch:
                diff_hours = (dataset_end_dt - actual_end_dt).total_seconds() / 3600
                print(f"      çµæŸæ™‚é–“å·®ç•°: {diff_hours:.2f} å°æ™‚")

            issues_found.append({
                'dataset_id': dataset_id,
                'dataset_name': dataset_name,
                'current_start': dataset_start,
                'current_end': dataset_end,
                'correct_start': actual_start,
                'correct_end': actual_end,
                'record_count': record_count
            })
        else:
            print(f"   âœ… æ—¥æœŸç¯„åœæ­£ç¢º")

    return issues_found

def fix_dataset_date_ranges(conn, issues: List[Dict[str, Any]]):
    """ä¿®æ­£ AnalysisDataset çš„æ—¥æœŸç¯„åœ"""
    if not issues:
        print("\nâœ… æ²’æœ‰ç™¼ç¾éœ€è¦ä¿®æ­£çš„æ—¥æœŸç¯„åœå•é¡Œ")
        return

    print(f"\nğŸ”§ ç™¼ç¾ {len(issues)} å€‹éœ€è¦ä¿®æ­£çš„è³‡æ–™é›†")
    print("=" * 80)

    cursor = conn.cursor()

    for issue in issues:
        dataset_id = issue['dataset_id']
        dataset_name = issue['dataset_name']
        correct_start = issue['correct_start']
        correct_end = issue['correct_end']

        print(f"\nä¿®æ­£è³‡æ–™é›†: {dataset_name}")
        print(f"  èˆŠçš„æ™‚é–“ç¯„åœ: {issue['current_start']} ~ {issue['current_end']}")
        print(f"  æ–°çš„æ™‚é–“ç¯„åœ: {correct_start} ~ {correct_end}")

        try:
            # å°‡å¯¦éš›æ™‚é–“ç¯„åœè½‰æ›ç‚ºæ¯«ç§’æ™‚é–“æˆ³ (SQLite ä¸­çš„æ ¼å¼)
            def timestamp_to_ms(ts):
                if isinstance(ts, int):
                    return ts  # å·²ç¶“æ˜¯æ¯«ç§’æ™‚é–“æˆ³
                elif isinstance(ts, str):
                    dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                    return int(dt.timestamp() * 1000)
                else:
                    return ts

            correct_start_ms = timestamp_to_ms(correct_start)
            correct_end_ms = timestamp_to_ms(correct_end)

            cursor.execute("""
                UPDATE analysis_datasets
                SET start_date = ?, end_date = ?
                WHERE id = ?
            """, (correct_start_ms, correct_end_ms, dataset_id))

            print(f"  âœ… ä¿®æ­£æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ ä¿®æ­£å¤±æ•—: {e}")

    # æäº¤è®Šæ›´
    try:
        conn.commit()
        print(f"\nğŸ’¾ æ‰€æœ‰è®Šæ›´å·²ä¿å­˜åˆ°è³‡æ–™åº«")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜è®Šæ›´å¤±æ•—: {e}")
        conn.rollback()

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” AnalysisDataset æ—¥æœŸç¯„åœé©—è­‰å·¥å…·")
    print("=" * 80)

    # é€£æ¥è³‡æ–™åº«
    conn = connect_to_database()
    if not conn:
        return

    try:
        # é©—è­‰æ—¥æœŸç¯„åœ
        issues = verify_dataset_date_ranges(conn)

        # ä¿®æ­£å•é¡Œ
        if issues:
            print(f"\nâš ï¸  ç™¼ç¾ {len(issues)} å€‹æ—¥æœŸç¯„åœå•é¡Œ:")
            for i, issue in enumerate(issues, 1):
                print(f"  {i}. {issue['dataset_name']} (è¨˜éŒ„æ•¸: {issue['record_count']})")

            response = input("\nğŸ¤” æ˜¯å¦è¦ä¿®æ­£é€™äº›å•é¡Œ? (y/n): ").lower().strip()
            if response == 'y' or response == 'yes':
                fix_dataset_date_ranges(conn, issues)
            else:
                print("ğŸ“ è·³éä¿®æ­£ï¼Œåƒ…é€²è¡Œé©—è­‰")
        else:
            print("\nğŸ‰ æ‰€æœ‰ AnalysisDataset çš„æ—¥æœŸç¯„åœéƒ½æ˜¯æ­£ç¢ºçš„!")

    except Exception as e:
        print(f"âŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    finally:
        conn.close()
        print("\nğŸ”š é©—è­‰å®Œæˆ")

if __name__ == "__main__":
    main()
