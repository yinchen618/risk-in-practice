#!/usr/bin/env python3
"""
ğŸ¯ é‡æ–°æ¨™è¨˜ç•°å¸¸æ¨£æœ¬å·¥å…·

æ ¹æ“šæŒ‡å®šçš„ç•°å¸¸æª¢æ¸¬æ¢ä»¶é‡æ–°æ¨™è¨˜ AnalysisReadyData ä¸­çš„æ¨£æœ¬ï¼Œ
ä¸¦æ›´æ–°æ¯å€‹ AnalysisDataset çš„ positiveLabels è¨ˆæ•¸ã€‚

æª¢æ¸¬æ¢ä»¶ï¼š
- Z-Score Threshold: 3.5Ïƒ
- Spike Threshold: 400%
- Min Duration: 90min
- Max Time Gap: 120min

Author: AI Assistant
Date: 2025-08-27
"""

import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import logging
from pathlib import Path

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('relabel_positive_samples.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç•°å¸¸æª¢æ¸¬æ¢ä»¶
DETECTION_CONFIG = {
    'z_score_threshold': 3.5,      # Z-Score é–¾å€¼
    'spike_threshold': 4.0,        # 400% = 4.0 å€
    'min_duration_minutes': 90,    # æœ€å°æŒçºŒæ™‚é–“
    'max_time_gap_minutes': 120    # æœ€å¤§æ™‚é–“é–“éš”
}

def get_database_path():
    """ç²å–æ•¸æ“šåº«è·¯å¾‘"""
    # å˜—è©¦å¤šå€‹å¯èƒ½çš„è·¯å¾‘
    possible_paths = [
        Path("backend/database/prisma/pu_practice.db"),
        Path(__file__).parent / "backend" / "database" / "prisma" / "pu_practice.db",
        Path("backend/database/pu_practice.db"),
        Path(__file__).parent / "backend" / "database" / "pu_practice.db",
    ]

    for db_path in possible_paths:
        if db_path.exists():
            logger.info(f"æ‰¾åˆ°æ•¸æ“šåº«: {db_path}")
            return str(db_path)

    logger.error("æœªæ‰¾åˆ°æ•¸æ“šåº«æ–‡ä»¶ï¼Œå˜—è©¦äº†ä»¥ä¸‹è·¯å¾‘:")
    for path in possible_paths:
        logger.error(f"  - {path}")
    raise FileNotFoundError("æ•¸æ“šåº«æ–‡ä»¶ä¸å­˜åœ¨")

def calculate_z_score_anomalies(df, threshold=3.5):
    """
    è¨ˆç®— Z-Score ç•°å¸¸

    Args:
        df: DataFrame with wattage_total column
        threshold: Z-Score é–¾å€¼

    Returns:
        Boolean Series indicating anomalies
    """
    if len(df) < 3:
        return pd.Series([False] * len(df), index=df.index)

    # è¨ˆç®— Z-Score
    mean_power = df['wattage_total'].mean()
    std_power = df['wattage_total'].std()

    if std_power == 0:
        return pd.Series([False] * len(df), index=df.index)

    z_scores = np.abs((df['wattage_total'] - mean_power) / std_power)
    return z_scores > threshold

def calculate_spike_anomalies(df, threshold=4.0):
    """
    è¨ˆç®—çªå¢ç•°å¸¸ï¼ˆç›¸å°æ–¼å‰ä¸€å€‹å€¼çš„å€æ•¸ï¼‰

    Args:
        df: DataFrame with wattage_total column
        threshold: å€æ•¸é–¾å€¼ (4.0 = 400%)

    Returns:
        Boolean Series indicating spike anomalies
    """
    if len(df) < 2:
        return pd.Series([False] * len(df), index=df.index)

    # è¨ˆç®—ç›¸å°æ–¼å‰ä¸€å€‹å€¼çš„æ¯”ç‡
    prev_values = df['wattage_total'].shift(1)

    # é¿å…é™¤ä»¥é›¶
    valid_prev = prev_values > 0

    # è¨ˆç®—æ¯”ç‡
    ratios = pd.Series([False] * len(df), index=df.index, dtype=bool)
    ratios[valid_prev] = (df['wattage_total'][valid_prev] / prev_values[valid_prev]) >= threshold

    return ratios

def group_consecutive_anomalies(anomaly_series, timestamps, min_duration_minutes=90, max_gap_minutes=120):
    """
    å°‡é€£çºŒçš„ç•°å¸¸é»åˆ†çµ„ï¼Œè€ƒæ…®æœ€å°æŒçºŒæ™‚é–“å’Œæœ€å¤§é–“éš”

    Args:
        anomaly_series: Boolean series of anomalies
        timestamps: Series of timestamps
        min_duration_minutes: æœ€å°æŒçºŒæ™‚é–“
        max_gap_minutes: æœ€å¤§å…è¨±é–“éš”

    Returns:
        Boolean Series with grouped anomalies
    """
    if not anomaly_series.any():
        return pd.Series([False] * len(anomaly_series), index=anomaly_series.index)

    result = pd.Series([False] * len(anomaly_series), index=anomaly_series.index)

    # æ‰¾åˆ°ç•°å¸¸é»çš„ç´¢å¼•
    anomaly_indices = anomaly_series[anomaly_series].index.tolist()

    if not anomaly_indices:
        return result

    # å°‡æ™‚é–“æˆ³è½‰æ›ç‚º datetime
    if isinstance(timestamps.iloc[0], str):
        timestamps = pd.to_datetime(timestamps)

    # åˆ†çµ„é€£çºŒçš„ç•°å¸¸
    groups = []
    current_group = [anomaly_indices[0]]

    for i in range(1, len(anomaly_indices)):
        curr_idx = anomaly_indices[i]
        prev_idx = anomaly_indices[i-1]

        # è¨ˆç®—æ™‚é–“é–“éš”
        time_gap = timestamps.loc[curr_idx] - timestamps.loc[prev_idx]

        if time_gap <= timedelta(minutes=max_gap_minutes):
            current_group.append(curr_idx)
        else:
            groups.append(current_group)
            current_group = [curr_idx]

    groups.append(current_group)

    # æª¢æŸ¥æ¯çµ„çš„æŒçºŒæ™‚é–“
    for group in groups:
        if len(group) >= 2:  # è‡³å°‘éœ€è¦å…©å€‹é»ä¾†è¨ˆç®—æŒçºŒæ™‚é–“
            start_time = timestamps.loc[group[0]]
            end_time = timestamps.loc[group[-1]]
            duration = end_time - start_time

            if duration >= timedelta(minutes=min_duration_minutes):
                # æ¨™è¨˜æ•´å€‹çµ„ç‚ºç•°å¸¸
                for idx in group:
                    result.loc[idx] = True
        elif len(group) == 1:
            # å–®é»ç•°å¸¸ä¹Ÿè€ƒæ…®å‰å¾Œçš„æƒ…æ³
            idx = group[0]
            # é€™è£¡å¯ä»¥æ·»åŠ æ›´è¤‡é›œçš„é‚è¼¯ï¼Œç›®å‰å…ˆæ¨™è¨˜ç‚ºç•°å¸¸
            result.loc[idx] = True

    return result

def detect_anomalies_for_dataset(df, config):
    """
    ç‚ºå–®å€‹æ•¸æ“šé›†æª¢æ¸¬ç•°å¸¸

    Args:
        df: DataFrame with analysis ready data
        config: Detection configuration

    Returns:
        Boolean Series indicating final anomalies
    """
    logger.info(f"é–‹å§‹æª¢æ¸¬ç•°å¸¸ï¼Œå…± {len(df)} æ¢è¨˜éŒ„")

    # æŒ‰æ™‚é–“æ’åº
    df = df.sort_values('timestamp').copy()

    # 1. Z-Score ç•°å¸¸æª¢æ¸¬
    z_score_anomalies = calculate_z_score_anomalies(df, config['z_score_threshold'])
    logger.info(f"Z-Score ç•°å¸¸æª¢æ¸¬: {z_score_anomalies.sum()} å€‹ç•°å¸¸é»")

    # 2. çªå¢ç•°å¸¸æª¢æ¸¬
    spike_anomalies = calculate_spike_anomalies(df, config['spike_threshold'])
    logger.info(f"çªå¢ç•°å¸¸æª¢æ¸¬: {spike_anomalies.sum()} å€‹ç•°å¸¸é»")

    # 3. åˆä½µç•°å¸¸ï¼ˆOR é‚è¼¯ï¼‰
    combined_anomalies = z_score_anomalies | spike_anomalies
    logger.info(f"åˆä½µç•°å¸¸: {combined_anomalies.sum()} å€‹ç•°å¸¸é»")

    # 4. æ‡‰ç”¨æŒçºŒæ™‚é–“å’Œé–“éš”æ¢ä»¶
    final_anomalies = group_consecutive_anomalies(
        combined_anomalies,
        df['timestamp'],
        config['min_duration_minutes'],
        config['max_time_gap_minutes']
    )
    logger.info(f"æœ€çµ‚ç•°å¸¸: {final_anomalies.sum()} å€‹ç•°å¸¸é»")

    return final_anomalies

def relabel_positive_samples():
    """é‡æ–°æ¨™è¨˜æ­£æ¨£æœ¬"""
    db_path = get_database_path()

    try:
        conn = sqlite3.connect(db_path)

        logger.info("ğŸš€ é–‹å§‹é‡æ–°æ¨™è¨˜æ­£æ¨£æœ¬...")
        logger.info(f"æª¢æ¸¬æ¢ä»¶: {DETECTION_CONFIG}")

        # å…ˆé‡ç½®æ‰€æœ‰æ¨£æœ¬çš„æ¨™ç±¤
        logger.info("é‡ç½®æ‰€æœ‰ AnalysisReadyData çš„ isPositiveLabel ç‚º False...")
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE analysis_ready_data
            SET is_positive_label = FALSE
        """)
        reset_count = cursor.rowcount
        logger.info(f"å·²é‡ç½® {reset_count} æ¢è¨˜éŒ„")

        # ç²å–æ‰€æœ‰æ•¸æ“šé›†
        cursor.execute("""
            SELECT id, name
            FROM analysis_datasets
            ORDER BY name
        """)
        datasets = cursor.fetchall()
        logger.info(f"æ‰¾åˆ° {len(datasets)} å€‹æ•¸æ“šé›†")

        total_positive_labels = 0

        for dataset_id, dataset_name in datasets:
            logger.info(f"\nğŸ” è™•ç†æ•¸æ“šé›†: {dataset_name} (ID: {dataset_id})")

            # ç²å–è©²æ•¸æ“šé›†çš„æ‰€æœ‰æ•¸æ“š
            query = """
                SELECT id, timestamp, wattage_total
                FROM analysis_ready_data
                WHERE dataset_id = ?
                ORDER BY timestamp
            """
            df = pd.read_sql_query(query, conn, params=[dataset_id])

            if len(df) == 0:
                logger.warning(f"æ•¸æ“šé›† {dataset_name} æ²’æœ‰æ•¸æ“šï¼Œè·³é")
                continue

            logger.info(f"è¼‰å…¥ {len(df)} æ¢è¨˜éŒ„")

            # æª¢æ¸¬ç•°å¸¸
            anomalies = detect_anomalies_for_dataset(df, DETECTION_CONFIG)

            # æ›´æ–°æ•¸æ“šåº«ä¸­çš„æ¨™ç±¤
            positive_indices = df[anomalies].index.tolist()
            positive_ids = df.loc[positive_indices, 'id'].tolist()

            if positive_ids:
                # æ‰¹é‡æ›´æ–°
                placeholders = ','.join(['?' for _ in positive_ids])
                update_query = f"""
                    UPDATE analysis_ready_data
                    SET is_positive_label = TRUE
                    WHERE id IN ({placeholders})
                """
                cursor.execute(update_query, positive_ids)
                updated_count = cursor.rowcount

                logger.info(f"âœ… æ›´æ–° {updated_count} æ¢è¨˜éŒ„ç‚ºæ­£æ¨™ç±¤")
                total_positive_labels += updated_count
            else:
                logger.info("æœªç™¼ç¾ç•°å¸¸ï¼Œç„¡éœ€æ›´æ–°")

        # æäº¤æ›´æ”¹
        conn.commit()
        logger.info(f"å·²æäº¤æ•¸æ“šåº«æ›´æ”¹ï¼Œç¸½å…±æ¨™è¨˜ {total_positive_labels} å€‹æ­£æ¨£æœ¬")

        return True

    except Exception as e:
        logger.error(f"âŒ é‡æ–°æ¨™è¨˜éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()

def update_dataset_positive_counts():
    """æ›´æ–°æ¯å€‹æ•¸æ“šé›†çš„ positiveLabels è¨ˆæ•¸"""
    db_path = get_database_path()

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        logger.info("\nğŸ“Š æ›´æ–° AnalysisDataset çš„ positiveLabels è¨ˆæ•¸...")

        # ç²å–æ¯å€‹æ•¸æ“šé›†çš„æ­£æ¨™ç±¤è¨ˆæ•¸
        cursor.execute("""
            SELECT
                ad.id,
                ad.name,
                ad.positive_labels as current_count,
                COUNT(CASE WHEN ard.is_positive_label = TRUE THEN 1 END) as actual_count
            FROM analysis_datasets ad
            LEFT JOIN analysis_ready_data ard ON ad.id = ard.dataset_id
            GROUP BY ad.id, ad.name, ad.positive_labels
            ORDER BY ad.name
        """)

        results = cursor.fetchall()
        updated_count = 0

        logger.info(f"æª¢æŸ¥ {len(results)} å€‹æ•¸æ“šé›†çš„æ­£æ¨™ç±¤è¨ˆæ•¸...")

        for dataset_id, dataset_name, current_count, actual_count in results:
            logger.info(f"ğŸ“¦ {dataset_name}: ç•¶å‰={current_count}, å¯¦éš›={actual_count}")

            if current_count != actual_count:
                # æ›´æ–°è¨ˆæ•¸
                cursor.execute("""
                    UPDATE analysis_datasets
                    SET positive_labels = ?
                    WHERE id = ?
                """, (actual_count, dataset_id))

                logger.info(f"   âœ… å·²æ›´æ–°: {current_count} â†’ {actual_count}")
                updated_count += 1
            else:
                logger.info(f"   â„¹ï¸  ç„¡éœ€æ›´æ–°")

        # æäº¤æ›´æ”¹
        conn.commit()

        logger.info(f"\nğŸ‰ æ›´æ–°å®Œæˆï¼å…±æ›´æ–° {updated_count} å€‹æ•¸æ“šé›†çš„è¨ˆæ•¸")

        # é¡¯ç¤ºç¸½çµ
        cursor.execute("""
            SELECT
                COUNT(*) as total_datasets,
                SUM(positive_labels) as total_positive_labels,
                SUM(total_records) as total_records
            FROM analysis_datasets
        """)

        total_datasets, total_positive, total_records = cursor.fetchone()
        positive_ratio = (total_positive / total_records * 100) if total_records > 0 else 0

        logger.info(f"ğŸ“ˆ ç¸½çµçµ±è¨ˆ:")
        logger.info(f"   ç¸½æ•¸æ“šé›†: {total_datasets}")
        logger.info(f"   ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
        logger.info(f"   ç¸½æ­£æ¨™ç±¤: {total_positive:,}")
        logger.info(f"   æ­£æ¨™ç±¤æ¯”ä¾‹: {positive_ratio:.3f}%")

        return True

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°è¨ˆæ•¸éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ¯ é–‹å§‹é‡æ–°æ¨™è¨˜ç•°å¸¸æ¨£æœ¬ç¨‹åº...")
    logger.info("=" * 60)

    start_time = datetime.now()

    try:
        # æ­¥é©Ÿ 1: é‡æ–°æ¨™è¨˜æ­£æ¨£æœ¬
        logger.info("æ­¥é©Ÿ 1: é‡æ–°æª¢æ¸¬ä¸¦æ¨™è¨˜ç•°å¸¸æ¨£æœ¬")
        if not relabel_positive_samples():
            logger.error("âŒ é‡æ–°æ¨™è¨˜å¤±æ•—")
            return False

        # æ­¥é©Ÿ 2: æ›´æ–°æ•¸æ“šé›†è¨ˆæ•¸
        logger.info("\næ­¥é©Ÿ 2: æ›´æ–°æ•¸æ“šé›†æ­£æ¨™ç±¤è¨ˆæ•¸")
        if not update_dataset_positive_counts():
            logger.error("âŒ æ›´æ–°è¨ˆæ•¸å¤±æ•—")
            return False

        # å®Œæˆ
        end_time = datetime.now()
        duration = end_time - start_time

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ é‡æ–°æ¨™è¨˜ç¨‹åºå®Œæˆï¼")
        logger.info(f"â±ï¸  ç¸½è€—æ™‚: {duration}")
        logger.info("=" * 60)

        return True

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
